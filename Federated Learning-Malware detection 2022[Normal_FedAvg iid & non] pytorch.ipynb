{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0c136",
   "metadata": {},
   "source": [
    "## fedavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.198% | global_loss: 0.685093343257904 | global_f1: 0.02638522427440633 | global_precision: 0.5357142857142857 | global_recall: 0.013525698827772768 | global_auc: 0.5258139754633329| flobal_FPR: 0.9864743011722272 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6732624173164368 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6430834699673457| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6622616648674011 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.723156936568105| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6511623859405518 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7792288760968114| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6390740275382996 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8187233468709031| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.624911904335022 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.847562264036266| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.607934832572937 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8727257144023883| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.5874287486076355 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8920740876860347| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5634119510650635 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9097294337915023| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.5365208983421326 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9247556138653964| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.5081815719604492 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9398743869275795| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.480081707239151 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9531033133569898| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 66.223% | global_loss: 0.45316585898399353 | global_f1: 0.1547420965058236 | global_precision: 1.0 | global_recall: 0.08385933273219116 | global_auc: 0.9629181701156369| flobal_FPR: 0.9161406672678089 \n",
      "comm_round: 13 | global_acc: 79.255% | global_loss: 0.4269455373287201 | global_f1: 0.6085319949811794 | global_precision: 1.0 | global_recall: 0.43733092876465285 | global_auc: 0.969766252562333| flobal_FPR: 0.5626690712353472 \n",
      "comm_round: 14 | global_acc: 87.400% | global_loss: 0.40125441551208496 | global_f1: 0.7948023822414726 | global_precision: 0.994579945799458 | global_recall: 0.6618575293056808 | global_auc: 0.9758194598172547| flobal_FPR: 0.3381424706943192 \n",
      "comm_round: 15 | global_acc: 90.392% | global_loss: 0.3760514259338379 | global_f1: 0.8515665125834618 | global_precision: 0.9892601431980907 | global_recall: 0.7475202885482417 | global_auc: 0.9799600283192093| flobal_FPR: 0.25247971145175835 \n",
      "comm_round: 16 | global_acc: 91.955% | global_loss: 0.35092559456825256 | global_f1: 0.879120879120879 | global_precision: 0.9854423292273237 | global_recall: 0.7935076645626691 | global_auc: 0.9830193956194495| flobal_FPR: 0.20649233543733092 \n",
      "comm_round: 17 | global_acc: 93.285% | global_loss: 0.32568806409835815 | global_f1: 0.9013671875 | global_precision: 0.9829605963791267 | global_recall: 0.8322813345356177 | global_auc: 0.9849804676278295| flobal_FPR: 0.16771866546438233 \n",
      "comm_round: 18 | global_acc: 94.149% | global_loss: 0.3000812828540802 | global_f1: 0.9155470249520153 | global_precision: 0.9784615384615385 | global_recall: 0.8602344454463481 | global_auc: 0.9862478044777969| flobal_FPR: 0.13976555455365194 \n",
      "comm_round: 19 | global_acc: 94.681% | global_loss: 0.27432355284690857 | global_f1: 0.9239543726235742 | global_precision: 0.9768844221105528 | global_recall: 0.8764652840396754 | global_auc: 0.9870939619400083| flobal_FPR: 0.12353471596032461 \n",
      "comm_round: 20 | global_acc: 95.080% | global_loss: 0.24869796633720398 | global_f1: 0.9305164319248826 | global_precision: 0.970617042115573 | global_recall: 0.8935978358881875 | global_auc: 0.9878527496081417| flobal_FPR: 0.10640216411181244 \n",
      "comm_round: 21 | global_acc: 95.312% | global_loss: 0.22404369711875916 | global_f1: 0.9342043863742417 | global_precision: 0.9680851063829787 | global_recall: 0.9026149684400361 | global_auc: 0.9884671871817116| flobal_FPR: 0.09738503155996393 \n",
      "comm_round: 22 | global_acc: 95.379% | global_loss: 0.20157083868980408 | global_f1: 0.9354988399071924 | global_precision: 0.9636711281070746 | global_recall: 0.90892696122633 | global_auc: 0.9890858982778179| flobal_FPR: 0.09107303877366997 \n",
      "comm_round: 23 | global_acc: 95.578% | global_loss: 0.18193823099136353 | global_f1: 0.9385113268608415 | global_precision: 0.9629981024667932 | global_recall: 0.915238954012624 | global_auc: 0.9898361389008785| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 24 | global_acc: 95.745% | global_loss: 0.1653178632259369 | global_f1: 0.9410681399631676 | global_precision: 0.9614299153339605 | global_recall: 0.9215509467989179 | global_auc: 0.9905645370754196| flobal_FPR: 0.07844905320108206 \n",
      "comm_round: 25 | global_acc: 95.778% | global_loss: 0.15167759358882904 | global_f1: 0.9416091954022987 | global_precision: 0.9606003752345216 | global_recall: 0.9233543733092876 | global_auc: 0.9910498193012222| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 26 | global_acc: 96.243% | global_loss: 0.14064478874206543 | global_f1: 0.9483782549109183 | global_precision: 0.9611111111111111 | global_recall: 0.9359783588818755 | global_auc: 0.9914439330462476| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 27 | global_acc: 96.410% | global_loss: 0.13169722259044647 | global_f1: 0.9507748404740202 | global_precision: 0.9612903225806452 | global_recall: 0.9404869251577999 | global_auc: 0.9918442196571591| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 28 | global_acc: 96.443% | global_loss: 0.12435995042324066 | global_f1: 0.9512528473804102 | global_precision: 0.9613259668508287 | global_recall: 0.9413886384129847 | global_auc: 0.9922041452218932| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 29 | global_acc: 96.443% | global_loss: 0.11829054355621338 | global_f1: 0.9512528473804102 | global_precision: 0.9613259668508287 | global_recall: 0.9413886384129847 | global_auc: 0.9925047163069547| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 30 | global_acc: 96.543% | global_loss: 0.11322712153196335 | global_f1: 0.9526842584167425 | global_precision: 0.9614325068870524 | global_recall: 0.9440937781785392 | global_auc: 0.9927544799574166| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 31 | global_acc: 96.642% | global_loss: 0.10894999653100967 | global_f1: 0.9540700318326513 | global_precision: 0.9623853211009175 | global_recall: 0.9458972046889089 | global_auc: 0.9929866746818956| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 32 | global_acc: 96.775% | global_loss: 0.10530620068311691 | global_f1: 0.9558488848429677 | global_precision: 0.9650735294117647 | global_recall: 0.9467989179440938 | global_auc: 0.9932079481821148| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 33 | global_acc: 96.908% | global_loss: 0.10219859331846237 | global_f1: 0.9577848388561052 | global_precision: 0.9643510054844607 | global_recall: 0.951307484220018 | global_auc: 0.9933651188442876| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 34 | global_acc: 96.941% | global_loss: 0.09952634572982788 | global_f1: 0.9582577132486388 | global_precision: 0.9643835616438357 | global_recall: 0.9522091974752029 | global_auc: 0.9935322610590454| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 35 | global_acc: 96.908% | global_loss: 0.0971563383936882 | global_f1: 0.9578231292517009 | global_precision: 0.9635036496350365 | global_recall: 0.9522091974752029 | global_auc: 0.9936965542587789| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 36 | global_acc: 96.908% | global_loss: 0.09508752077817917 | global_f1: 0.9577848388561052 | global_precision: 0.9643510054844607 | global_recall: 0.951307484220018 | global_auc: 0.9938171625614735| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 37 | global_acc: 96.941% | global_loss: 0.09324853122234344 | global_f1: 0.9582198001816531 | global_precision: 0.9652333028362305 | global_recall: 0.951307484220018 | global_auc: 0.9939458430734034| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 38 | global_acc: 97.041% | global_loss: 0.09163245558738708 | global_f1: 0.959563834620627 | global_precision: 0.967032967032967 | global_recall: 0.9522091974752029 | global_auc: 0.9940773726003578| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 39 | global_acc: 97.141% | global_loss: 0.09014030545949936 | global_f1: 0.96094459582198 | global_precision: 0.9679780420860018 | global_recall: 0.9540126239855726 | global_auc: 0.9941557205135254| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 40 | global_acc: 97.274% | global_loss: 0.08878695964813232 | global_f1: 0.9627611262488647 | global_precision: 0.969807868252516 | global_recall: 0.9558160504959423 | global_auc: 0.9942910487271788| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 41 | global_acc: 97.274% | global_loss: 0.08755961805582047 | global_f1: 0.9627949183303085 | global_precision: 0.9689497716894977 | global_recall: 0.9567177637511272 | global_auc: 0.9943969371189145| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 42 | global_acc: 97.340% | global_loss: 0.08642145246267319 | global_f1: 0.9637352674524025 | global_precision: 0.9690063810391978 | global_recall: 0.9585211902614968 | global_auc: 0.9944995016597887| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 43 | global_acc: 97.407% | global_loss: 0.08537676185369492 | global_f1: 0.9646098003629765 | global_precision: 0.9707762557077626 | global_recall: 0.9585211902614968 | global_auc: 0.9946039655440122| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 44 | global_acc: 97.440% | global_loss: 0.08442012965679169 | global_f1: 0.9650476622787109 | global_precision: 0.9716636197440585 | global_recall: 0.9585211902614968 | global_auc: 0.9946799392779931| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 45 | global_acc: 97.440% | global_loss: 0.0835305005311966 | global_f1: 0.9650476622787109 | global_precision: 0.9716636197440585 | global_recall: 0.9585211902614968 | global_auc: 0.9947592368628356| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 46 | global_acc: 97.440% | global_loss: 0.0826885774731636 | global_f1: 0.9650793650793651 | global_precision: 0.9708029197080292 | global_recall: 0.9594229035166817 | global_auc: 0.9948418582985398| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 47 | global_acc: 97.473% | global_loss: 0.08190590888261795 | global_f1: 0.9655797101449276 | global_precision: 0.9699727024567789 | global_recall: 0.9612263300270514 | global_auc: 0.9949396744810401| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 48 | global_acc: 97.473% | global_loss: 0.08118896186351776 | global_f1: 0.9654859218891917 | global_precision: 0.9725526075022873 | global_recall: 0.9585211902614968 | global_auc: 0.9949752871688435| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 49 | global_acc: 97.540% | global_loss: 0.08047881722450256 | global_f1: 0.9664246823956443 | global_precision: 0.9726027397260274 | global_recall: 0.9603246167718665 | global_auc: 0.9950488867236374| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 50 | global_acc: 97.540% | global_loss: 0.07980658859014511 | global_f1: 0.9664246823956443 | global_precision: 0.9726027397260274 | global_recall: 0.9603246167718665 | global_auc: 0.9950849742472784| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 51 | global_acc: 97.606% | global_loss: 0.07917212694883347 | global_f1: 0.9673321234119783 | global_precision: 0.9735159817351599 | global_recall: 0.9612263300270514 | global_auc: 0.9951424293836013| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 52 | global_acc: 97.640% | global_loss: 0.07859507203102112 | global_f1: 0.9677712210621879 | global_precision: 0.9744058500914077 | global_recall: 0.9612263300270514 | global_auc: 0.9951599983095843| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 53 | global_acc: 97.640% | global_loss: 0.07803850620985031 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9952264753268176| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 54 | global_acc: 97.640% | global_loss: 0.07751520723104477 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.995268260880507| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 55 | global_acc: 97.606% | global_loss: 0.07699798047542572 | global_f1: 0.9673321234119783 | global_precision: 0.9735159817351599 | global_recall: 0.9612263300270514 | global_auc: 0.9952962761949125| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 56 | global_acc: 97.673% | global_loss: 0.07656169682741165 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.9953508823162113| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 57 | global_acc: 97.640% | global_loss: 0.07611171156167984 | global_f1: 0.9677712210621879 | global_precision: 0.9744058500914077 | global_recall: 0.9612263300270514 | global_auc: 0.9953883943473641| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 58 | global_acc: 97.640% | global_loss: 0.07568647712469101 | global_f1: 0.9677712210621879 | global_precision: 0.9744058500914077 | global_recall: 0.9612263300270514 | global_auc: 0.9954187838409565| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 59 | global_acc: 97.673% | global_loss: 0.07529262453317642 | global_f1: 0.9682107175295187 | global_precision: 0.9752973467520586 | global_recall: 0.9612263300270514 | global_auc: 0.9954600945588087| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 60 | global_acc: 97.706% | global_loss: 0.07491670548915863 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.9954828866790029| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 61 | global_acc: 97.739% | global_loss: 0.07456967234611511 | global_f1: 0.9691749773345422 | global_precision: 0.9744758432087511 | global_recall: 0.9639314697926059 | global_auc: 0.9955336941136026| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 62 | global_acc: 97.773% | global_loss: 0.07423373311758041 | global_f1: 0.9696420480289986 | global_precision: 0.9744990892531876 | global_recall: 0.9648331830477908 | global_auc: 0.9955792783539911| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 63 | global_acc: 97.773% | global_loss: 0.07386631518602371 | global_f1: 0.9695869269178392 | global_precision: 0.9762340036563071 | global_recall: 0.9630297565374211 | global_auc: 0.995571680980593| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 64 | global_acc: 97.806% | global_loss: 0.07362900674343109 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9956243877585422| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 65 | global_acc: 97.739% | global_loss: 0.07326819747686386 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9956205890718431| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 66 | global_acc: 97.739% | global_loss: 0.0729878693819046 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9956557269238093| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 67 | global_acc: 97.739% | global_loss: 0.07272392511367798 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9956780442081661| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 68 | global_acc: 97.739% | global_loss: 0.07247601449489594 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9956989369850109| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 69 | global_acc: 97.806% | global_loss: 0.07226943224668503 | global_f1: 0.97 | global_precision: 0.9780018331805683 | global_recall: 0.9621280432822362 | global_auc: 0.9957231536127173| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 70 | global_acc: 97.739% | global_loss: 0.07202959805727005 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9957597159721955| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 71 | global_acc: 97.739% | global_loss: 0.07179831713438034 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9957768100623413| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 72 | global_acc: 97.706% | global_loss: 0.07161521166563034 | global_f1: 0.9686790739900136 | global_precision: 0.9753199268738574 | global_recall: 0.9621280432822362 | global_auc: 0.9958057750484213| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 73 | global_acc: 97.739% | global_loss: 0.07142683118581772 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9958109982426325| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 74 | global_acc: 97.739% | global_loss: 0.07125179469585419 | global_f1: 0.969147005444646 | global_precision: 0.9753424657534246 | global_recall: 0.9630297565374211 | global_auc: 0.9958542083038342| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 75 | global_acc: 97.739% | global_loss: 0.07108432799577713 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9958537334679968| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 76 | global_acc: 97.739% | global_loss: 0.070937879383564 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9958770004240284| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 77 | global_acc: 97.706% | global_loss: 0.070797398686409 | global_f1: 0.9686790739900136 | global_precision: 0.9753199268738574 | global_recall: 0.9621280432822362 | global_auc: 0.9959016918875723| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 78 | global_acc: 97.839% | global_loss: 0.070747509598732 | global_f1: 0.970548255550521 | global_precision: 0.9754098360655737 | global_recall: 0.9657348963029756 | global_auc: 0.9959368297395382| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 79 | global_acc: 97.806% | global_loss: 0.07052210718393326 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9959263833511158| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 80 | global_acc: 97.839% | global_loss: 0.0704391747713089 | global_f1: 0.970548255550521 | global_precision: 0.9754098360655737 | global_recall: 0.9657348963029756 | global_auc: 0.9959548735013588| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 81 | global_acc: 97.839% | global_loss: 0.07028542459011078 | global_f1: 0.9705215419501133 | global_precision: 0.9762773722627737 | global_recall: 0.9648331830477908 | global_auc: 0.9959567728447083| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 82 | global_acc: 97.872% | global_loss: 0.07016655802726746 | global_f1: 0.9709618874773139 | global_precision: 0.9771689497716894 | global_recall: 0.9648331830477908 | global_auc: 0.995966269561456| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 83 | global_acc: 97.872% | global_loss: 0.07012992352247238 | global_f1: 0.970988213961922 | global_precision: 0.9762989972652689 | global_recall: 0.9657348963029756 | global_auc: 0.9959957093833735| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 84 | global_acc: 97.872% | global_loss: 0.0700126588344574 | global_f1: 0.970988213961922 | global_precision: 0.9762989972652689 | global_recall: 0.9657348963029756 | global_auc: 0.9959657947256185| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 85 | global_acc: 97.906% | global_loss: 0.06988998502492905 | global_f1: 0.9714026327734907 | global_precision: 0.9780621572212066 | global_recall: 0.9648331830477908 | global_auc: 0.9959619960389194| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 86 | global_acc: 97.906% | global_loss: 0.06981240212917328 | global_f1: 0.9714026327734907 | global_precision: 0.9780621572212066 | global_recall: 0.9648331830477908 | global_auc: 0.9959857378307885| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 87 | global_acc: 97.939% | global_loss: 0.06973111629486084 | global_f1: 0.9718693284936479 | global_precision: 0.9780821917808219 | global_recall: 0.9657348963029756 | global_auc: 0.9960056809359584| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 88 | global_acc: 98.005% | global_loss: 0.0696682557463646 | global_f1: 0.9728014505893019 | global_precision: 0.9781221513217867 | global_recall: 0.9675383228133454 | global_auc: 0.99602894789199| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 89 | global_acc: 97.972% | global_loss: 0.06962362676858902 | global_f1: 0.9723606705935659 | global_precision: 0.9772313296903461 | global_recall: 0.9675383228133454 | global_auc: 0.9960465168179731| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 90 | global_acc: 97.906% | global_loss: 0.06948384642601013 | global_f1: 0.9714026327734907 | global_precision: 0.9780621572212066 | global_recall: 0.9648331830477908 | global_auc: 0.9960355955937134| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 91 | global_acc: 97.972% | global_loss: 0.06940843909978867 | global_f1: 0.9723356009070295 | global_precision: 0.9781021897810219 | global_recall: 0.9666366095581606 | global_auc: 0.9960517400121842| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 92 | global_acc: 97.906% | global_loss: 0.06934231519699097 | global_f1: 0.9714026327734907 | global_precision: 0.9780621572212066 | global_recall: 0.9648331830477908 | global_auc: 0.9960545890272087| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 93 | global_acc: 98.005% | global_loss: 0.06932559609413147 | global_f1: 0.9728014505893019 | global_precision: 0.9781221513217867 | global_recall: 0.9675383228133454 | global_auc: 0.9960887772074999| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 94 | global_acc: 98.005% | global_loss: 0.06924856454133987 | global_f1: 0.9728014505893019 | global_precision: 0.9781221513217867 | global_recall: 0.9675383228133454 | global_auc: 0.9960992235959223| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 95 | global_acc: 97.972% | global_loss: 0.06918834894895554 | global_f1: 0.9723356009070295 | global_precision: 0.9781021897810219 | global_recall: 0.9666366095581606 | global_auc: 0.9961139435068811| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 96 | global_acc: 98.072% | global_loss: 0.06934460252523422 | global_f1: 0.9737793851717903 | global_precision: 0.9764279238440616 | global_recall: 0.9711451758340848 | global_auc: 0.9961267640744903| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 97 | global_acc: 98.072% | global_loss: 0.06911823898553848 | global_f1: 0.9737318840579711 | global_precision: 0.978161965423112 | global_recall: 0.9693417493237151 | global_auc: 0.9961234402236286| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 98 | global_acc: 97.972% | global_loss: 0.06903259456157684 | global_f1: 0.9723104857013164 | global_precision: 0.9789762340036563 | global_recall: 0.9657348963029756 | global_auc: 0.9961547793888957| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 99 | global_acc: 97.972% | global_loss: 0.06898538023233414 | global_f1: 0.9723104857013164 | global_precision: 0.9789762340036563 | global_recall: 0.9657348963029756 | global_auc: 0.9961638012698061| flobal_FPR: 0.034265103697024346 \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'results\\round-100\\10-clients'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 155\u001b[0m\n\u001b[0;32m    153\u001b[0m all_R \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(all_results, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_acc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_f1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_precision\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_recall\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_auc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglobal_fpr\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    154\u001b[0m flname \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresults/round-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-clients/FedAvg-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset[d]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 155\u001b[0m \u001b[43mall_R\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m all_avg\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mconcatenate(([dataset[d], r, cl], np\u001b[38;5;241m.\u001b[39mmean(all_results, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))))  \u001b[38;5;66;03m# Storing avg values for each dataset\u001b[39;00m\n\u001b[0;32m    158\u001b[0m all_std\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mconcatenate(([dataset[d], r, cl], np\u001b[38;5;241m.\u001b[39mstd(all_results, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))))  \u001b[38;5;66;03m# Storing std values for each dataset\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3773\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3774\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3775\u001b[0m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3776\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3777\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3778\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3779\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3780\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3783\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3789\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m \u001b[43mcsv_formatter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    247\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 737\u001b[0m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    741\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pandas\\io\\common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    598\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'results\\round-100\\10-clients'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e03d2",
   "metadata": {},
   "source": [
    "## fed avg non iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5726ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg non idd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-noniid-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-noniid-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-noniid-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
