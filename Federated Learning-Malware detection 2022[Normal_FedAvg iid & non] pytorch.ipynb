{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b0c136",
   "metadata": {},
   "source": [
    "## fedavg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 62.500% | global_loss: 0.6841890215873718 | global_f1: 0.01742160278745645 | global_precision: 0.2564102564102564 | global_recall: 0.009017132551848512 | global_auc: 0.611553895529468| flobal_FPR: 0.9909828674481514 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6742175817489624 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7249841048703437| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6654787063598633 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7919620739119966| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6572434306144714 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8278494542474303| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6488261818885803 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8483248503910985| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6396223306655884 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.862899461583644| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6288841366767883 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8760604864883088| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6159178018569946 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8883556482435111| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5998576283454895 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8998367039555248| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.580112874507904 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9124492934680158| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.5561453700065613 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9243467802094122| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.5282906293869019 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9353928862943859| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 63.531% | global_loss: 0.49703025817871094 | global_f1: 0.021409455842997326 | global_precision: 1.0 | global_recall: 0.010820559062218215 | global_auc: 0.948025893747884| flobal_FPR: 0.9891794409377818 \n",
      "comm_round: 13 | global_acc: 77.593% | global_loss: 0.462902307510376 | global_f1: 0.5634715025906736 | global_precision: 1.0 | global_recall: 0.39224526600541026 | global_auc: 0.9594708619362571| flobal_FPR: 0.6077547339945897 \n",
      "comm_round: 14 | global_acc: 87.101% | global_loss: 0.42712730169296265 | global_f1: 0.789587852494577 | global_precision: 0.9904761904761905 | global_recall: 0.6564472497745717 | global_auc: 0.9687206640484218| flobal_FPR: 0.3435527502254283 \n",
      "comm_round: 15 | global_acc: 91.789% | global_loss: 0.3907702565193176 | global_f1: 0.8765617191404298 | global_precision: 0.9831838565022422 | global_recall: 0.7908025247971145 | global_auc: 0.9751409194056385| flobal_FPR: 0.20919747520288548 \n",
      "comm_round: 16 | global_acc: 93.152% | global_loss: 0.3536255955696106 | global_f1: 0.9001937984496123 | global_precision: 0.9727748691099476 | global_recall: 0.8376916140667268 | global_auc: 0.9795203303337954| flobal_FPR: 0.1623083859332732 \n",
      "comm_round: 17 | global_acc: 93.650% | global_loss: 0.31588390469551086 | global_f1: 0.9093497864261985 | global_precision: 0.9599198396793587 | global_recall: 0.8638412984670875 | global_auc: 0.9823251856251998| flobal_FPR: 0.13615870153291254 \n",
      "comm_round: 18 | global_acc: 94.215% | global_loss: 0.2786096930503845 | global_f1: 0.9183098591549295 | global_precision: 0.9578844270323212 | global_recall: 0.8818755635707844 | global_auc: 0.9845464676724639| flobal_FPR: 0.11812443642921551 \n",
      "comm_round: 19 | global_acc: 94.382% | global_loss: 0.24434755742549896 | global_f1: 0.9208430913348946 | global_precision: 0.9580896686159844 | global_recall: 0.8863841298467088 | global_auc: 0.9861034543832334| flobal_FPR: 0.11361587015329125 \n",
      "comm_round: 20 | global_acc: 94.415% | global_loss: 0.21483691036701202 | global_f1: 0.9218604651162791 | global_precision: 0.951969260326609 | global_recall: 0.8935978358881875 | global_auc: 0.9871637628081031| flobal_FPR: 0.10640216411181244 \n",
      "comm_round: 21 | global_acc: 94.415% | global_loss: 0.19066962599754333 | global_f1: 0.9220055710306406 | global_precision: 0.9502392344497608 | global_recall: 0.8954012623985572 | global_auc: 0.9880303382113218| flobal_FPR: 0.10459873760144274 \n",
      "comm_round: 22 | global_acc: 94.548% | global_loss: 0.1714194267988205 | global_f1: 0.9240037071362373 | global_precision: 0.9504289799809342 | global_recall: 0.8990081154192967 | global_auc: 0.9889690886618224| flobal_FPR: 0.10099188458070334 \n",
      "comm_round: 23 | global_acc: 94.880% | global_loss: 0.15621276199817657 | global_f1: 0.9287696577243293 | global_precision: 0.9534662867996201 | global_recall: 0.9053201082055906 | global_auc: 0.9896186640873584| flobal_FPR: 0.09467989179440937 \n",
      "comm_round: 24 | global_acc: 95.412% | global_loss: 0.14416129887104034 | global_f1: 0.9366391184573003 | global_precision: 0.9541627689429373 | global_recall: 0.9197475202885482 | global_auc: 0.990273937542943| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 25 | global_acc: 95.645% | global_loss: 0.13452710211277008 | global_f1: 0.939825447864033 | global_precision: 0.9578651685393258 | global_recall: 0.9224526600541028 | global_auc: 0.9909016705199596| flobal_FPR: 0.0775473399458972 \n",
      "comm_round: 26 | global_acc: 95.944% | global_loss: 0.12668246030807495 | global_f1: 0.9440366972477064 | global_precision: 0.9607843137254902 | global_recall: 0.9278629395852119 | global_auc: 0.9914339614936626| flobal_FPR: 0.0721370604147881 \n",
      "comm_round: 27 | global_acc: 96.110% | global_loss: 0.12016358971595764 | global_f1: 0.9464530892448513 | global_precision: 0.9609665427509294 | global_recall: 0.9323715058611362 | global_auc: 0.9918271255670134| flobal_FPR: 0.06762849413886383 \n",
      "comm_round: 28 | global_acc: 96.177% | global_loss: 0.11471975594758987 | global_f1: 0.9474645957058018 | global_precision: 0.9601851851851851 | global_recall: 0.9350766456266907 | global_auc: 0.9922397579096967| flobal_FPR: 0.06492335437330929 \n",
      "comm_round: 29 | global_acc: 96.277% | global_loss: 0.11012599617242813 | global_f1: 0.9488117001828154 | global_precision: 0.9620018535681186 | global_recall: 0.9359783588818755 | global_auc: 0.9925165872028893| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 30 | global_acc: 96.310% | global_loss: 0.1062014251947403 | global_f1: 0.9492455418381345 | global_precision: 0.9628942486085343 | global_recall: 0.9359783588818755 | global_auc: 0.9928119350937397| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 31 | global_acc: 96.642% | global_loss: 0.10282468050718307 | global_f1: 0.9540282203004097 | global_precision: 0.9632352941176471 | global_recall: 0.9449954914337241 | global_auc: 0.9930431801465439| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 32 | global_acc: 96.642% | global_loss: 0.09987568110227585 | global_f1: 0.9540282203004097 | global_precision: 0.9632352941176471 | global_recall: 0.9449954914337241 | global_auc: 0.9932763245426974| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 33 | global_acc: 96.676% | global_loss: 0.0973174124956131 | global_f1: 0.9545040946314832 | global_precision: 0.9632690541781451 | global_recall: 0.9458972046889089 | global_auc: 0.9934657840418122| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 34 | global_acc: 96.742% | global_loss: 0.09501925110816956 | global_f1: 0.9553734061930783 | global_precision: 0.9650413983440662 | global_recall: 0.9458972046889089 | global_auc: 0.9935854426728319| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 35 | global_acc: 96.908% | global_loss: 0.09298381209373474 | global_f1: 0.957630979498861 | global_precision: 0.9677716390423573 | global_recall: 0.9477006311992786 | global_auc: 0.9937335914540946| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 36 | global_acc: 96.941% | global_loss: 0.09117542207241058 | global_f1: 0.9581056466302368 | global_precision: 0.9678012879484821 | global_recall: 0.9486023444544635 | global_auc: 0.9938926614596169| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 37 | global_acc: 96.975% | global_loss: 0.0894419401884079 | global_f1: 0.9585798816568047 | global_precision: 0.9678308823529411 | global_recall: 0.9495040577096483 | global_auc: 0.9940583791668626| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 38 | global_acc: 97.041% | global_loss: 0.08792425692081451 | global_f1: 0.9595270577535243 | global_precision: 0.9678899082568807 | global_recall: 0.951307484220018 | global_auc: 0.994246414158465| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 39 | global_acc: 97.108% | global_loss: 0.08654525130987167 | global_f1: 0.9604365620736699 | global_precision: 0.9688073394495413 | global_recall: 0.9522091974752029 | global_auc: 0.9943527773860381| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 40 | global_acc: 97.174% | global_loss: 0.08526143431663513 | global_f1: 0.9613811903680145 | global_precision: 0.9688644688644689 | global_recall: 0.9540126239855726 | global_auc: 0.9944610399569609| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 41 | global_acc: 97.141% | global_loss: 0.08409222215414047 | global_f1: 0.96094459582198 | global_precision: 0.9679780420860018 | global_recall: 0.9540126239855726 | global_auc: 0.9945887707972161| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 42 | global_acc: 97.207% | global_loss: 0.08299613744020462 | global_f1: 0.9618528610354224 | global_precision: 0.9688929551692589 | global_recall: 0.9549143372407575 | global_auc: 0.9946599961728231| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 43 | global_acc: 97.307% | global_loss: 0.08200176060199738 | global_f1: 0.9631650750341064 | global_precision: 0.9715596330275229 | global_recall: 0.9549143372407575 | global_auc: 0.9947236241750321| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 44 | global_acc: 97.307% | global_loss: 0.08102161437273026 | global_f1: 0.9631985461154021 | global_precision: 0.9706959706959707 | global_recall: 0.9558160504959423 | global_auc: 0.9948128933124596| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 45 | global_acc: 97.340% | global_loss: 0.08014142513275146 | global_f1: 0.9636032757051866 | global_precision: 0.9724517906336089 | global_recall: 0.9549143372407575 | global_auc: 0.9948807948372048| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 46 | global_acc: 97.374% | global_loss: 0.07930885255336761 | global_f1: 0.9641398093508853 | global_precision: 0.9707495429616088 | global_recall: 0.957619477006312 | global_auc: 0.9949657904520959| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 47 | global_acc: 97.540% | global_loss: 0.07852225005626678 | global_f1: 0.9663330300272976 | global_precision: 0.9752066115702479 | global_recall: 0.957619477006312 | global_auc: 0.9950289436184675| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 48 | global_acc: 97.540% | global_loss: 0.07777631282806396 | global_f1: 0.9663330300272976 | global_precision: 0.9752066115702479 | global_recall: 0.957619477006312 | global_auc: 0.9950849742472784| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 49 | global_acc: 97.606% | global_loss: 0.07707898318767548 | global_f1: 0.9672131147540984 | global_precision: 0.9770009199632015 | global_recall: 0.957619477006312 | global_auc: 0.9951239107859434| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 50 | global_acc: 97.640% | global_loss: 0.07640174776315689 | global_f1: 0.9676832043695949 | global_precision: 0.9770220588235294 | global_recall: 0.9585211902614968 | global_auc: 0.9951794665789169| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 51 | global_acc: 97.606% | global_loss: 0.07575535774230957 | global_f1: 0.9672429481346679 | global_precision: 0.9761248852157943 | global_recall: 0.9585211902614968 | global_auc: 0.9952478429394997| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 52 | global_acc: 97.673% | global_loss: 0.0751727893948555 | global_f1: 0.9681238615664846 | global_precision: 0.9779208831646734 | global_recall: 0.9585211902614968 | global_auc: 0.9952958013590751| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 53 | global_acc: 97.673% | global_loss: 0.07461359351873398 | global_f1: 0.9681818181818183 | global_precision: 0.9761686526122824 | global_recall: 0.9603246167718665 | global_auc: 0.9953466087936746| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 54 | global_acc: 97.706% | global_loss: 0.07407665252685547 | global_f1: 0.9686221009549795 | global_precision: 0.9770642201834863 | global_recall: 0.9603246167718665 | global_auc: 0.9953860201681773| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 55 | global_acc: 97.673% | global_loss: 0.07359086722135544 | global_f1: 0.9681818181818183 | global_precision: 0.9761686526122824 | global_recall: 0.9603246167718665 | global_auc: 0.9953964665565997| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 56 | global_acc: 97.673% | global_loss: 0.0731019452214241 | global_f1: 0.9681818181818183 | global_precision: 0.9761686526122824 | global_recall: 0.9603246167718665 | global_auc: 0.9954197335126314| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 57 | global_acc: 97.640% | global_loss: 0.07269000262022018 | global_f1: 0.967741935483871 | global_precision: 0.9752747252747253 | global_recall: 0.9603246167718665 | global_auc: 0.9954624687379955| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 58 | global_acc: 97.706% | global_loss: 0.07222425937652588 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.9954771886489542| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 59 | global_acc: 97.739% | global_loss: 0.07181798666715622 | global_f1: 0.969090909090909 | global_precision: 0.9770852428964253 | global_recall: 0.9612263300270514 | global_auc: 0.9955156503517821| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 60 | global_acc: 97.773% | global_loss: 0.07146898657083511 | global_f1: 0.9695592912312585 | global_precision: 0.9771062271062271 | global_recall: 0.9621280432822362 | global_auc: 0.9955574359054716| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 61 | global_acc: 97.739% | global_loss: 0.07107676565647125 | global_f1: 0.969090909090909 | global_precision: 0.9770852428964253 | global_recall: 0.9612263300270514 | global_auc: 0.9955693068014061| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 62 | global_acc: 97.839% | global_loss: 0.07073411345481873 | global_f1: 0.9704142011834319 | global_precision: 0.9797794117647058 | global_recall: 0.9612263300270514 | global_auc: 0.9955887750707386| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 63 | global_acc: 97.839% | global_loss: 0.07040610164403915 | global_f1: 0.9704411095952706 | global_precision: 0.9788990825688073 | global_recall: 0.9621280432822362 | global_auc: 0.9956072936683964| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 64 | global_acc: 97.839% | global_loss: 0.07008583843708038 | global_f1: 0.9704679691049523 | global_precision: 0.978021978021978 | global_recall: 0.9630297565374211 | global_auc: 0.9956376831619889| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 65 | global_acc: 97.839% | global_loss: 0.0697992816567421 | global_f1: 0.9704679691049523 | global_precision: 0.978021978021978 | global_recall: 0.9630297565374211 | global_auc: 0.9956794687156784| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 66 | global_acc: 97.906% | global_loss: 0.0695347711443901 | global_f1: 0.9713506139154162 | global_precision: 0.9798165137614679 | global_recall: 0.9630297565374211 | global_auc: 0.9956761448648166| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 67 | global_acc: 97.939% | global_loss: 0.06938678026199341 | global_f1: 0.9718948322756119 | global_precision: 0.9772105742935278 | global_recall: 0.9666366095581606 | global_auc: 0.995741672210375| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 68 | global_acc: 97.906% | global_loss: 0.06898856163024902 | global_f1: 0.9713506139154162 | global_precision: 0.9798165137614679 | global_recall: 0.9630297565374211 | global_auc: 0.9957459457329114| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 69 | global_acc: 97.939% | global_loss: 0.06873756647109985 | global_f1: 0.9718437783832881 | global_precision: 0.9789569990850869 | global_recall: 0.9648331830477908 | global_auc: 0.9957730113756422| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 70 | global_acc: 97.972% | global_loss: 0.06851700693368912 | global_f1: 0.9722853248523398 | global_precision: 0.9798534798534798 | global_recall: 0.9648331830477908 | global_auc: 0.9958238188102417| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 71 | global_acc: 98.072% | global_loss: 0.068315289914608 | global_f1: 0.9736603088101726 | global_precision: 0.9807868252516011 | global_recall: 0.9666366095581606 | global_auc: 0.9958532586321592| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 72 | global_acc: 98.039% | global_loss: 0.06813398003578186 | global_f1: 0.9732183386291421 | global_precision: 0.979890310786106 | global_recall: 0.9666366095581606 | global_auc: 0.9958784249315406| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 73 | global_acc: 98.039% | global_loss: 0.06799539923667908 | global_f1: 0.9732426303854875 | global_precision: 0.9790145985401459 | global_recall: 0.9675383228133454 | global_auc: 0.9959092892609702| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 74 | global_acc: 98.072% | global_loss: 0.06780493259429932 | global_f1: 0.9736842105263159 | global_precision: 0.9799086757990868 | global_recall: 0.9675383228133454 | global_auc: 0.9959373045753757| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 75 | global_acc: 98.172% | global_loss: 0.06761032342910767 | global_f1: 0.9750113584734212 | global_precision: 0.9826007326007326 | global_recall: 0.9675383228133454 | global_auc: 0.9959510748146597| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 76 | global_acc: 98.172% | global_loss: 0.06746626645326614 | global_f1: 0.9750113584734212 | global_precision: 0.9826007326007326 | global_recall: 0.9675383228133454 | global_auc: 0.995977665621553| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 77 | global_acc: 98.138% | global_loss: 0.06727608293294907 | global_f1: 0.9745454545454545 | global_precision: 0.9825847846012832 | global_recall: 0.9666366095581606 | global_auc: 0.9959719675915044| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 78 | global_acc: 98.138% | global_loss: 0.06718206405639648 | global_f1: 0.974568574023615 | global_precision: 0.9817017383348582 | global_recall: 0.9675383228133454 | global_auc: 0.996018976339405| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 79 | global_acc: 98.172% | global_loss: 0.06701824814081192 | global_f1: 0.9750113584734212 | global_precision: 0.9826007326007326 | global_recall: 0.9675383228133454 | global_auc: 0.99602894789199| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 80 | global_acc: 98.172% | global_loss: 0.06687857955694199 | global_f1: 0.9750113584734212 | global_precision: 0.9826007326007326 | global_recall: 0.9675383228133454 | global_auc: 0.9960360704295508| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 81 | global_acc: 98.205% | global_loss: 0.06677577644586563 | global_f1: 0.9754768392370572 | global_precision: 0.9826166514181153 | global_recall: 0.9684400360685302 | global_auc: 0.9960531645196964| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 82 | global_acc: 98.205% | global_loss: 0.06665454059839249 | global_f1: 0.9754768392370572 | global_precision: 0.9826166514181153 | global_recall: 0.9684400360685302 | global_auc: 0.9960621864006067| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 83 | global_acc: 98.271% | global_loss: 0.06669344753026962 | global_f1: 0.9764492753623188 | global_precision: 0.9808917197452229 | global_recall: 0.9720468890892696 | global_auc: 0.9961087203126698| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 84 | global_acc: 98.238% | global_loss: 0.06653647869825363 | global_f1: 0.9759855006796557 | global_precision: 0.9808743169398907 | global_recall: 0.9711451758340848 | global_auc: 0.9961039719542961| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 85 | global_acc: 98.271% | global_loss: 0.06639625877141953 | global_f1: 0.9764279238440617 | global_precision: 0.9817684594348223 | global_recall: 0.9711451758340848 | global_auc: 0.9961063461334831| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 86 | global_acc: 98.238% | global_loss: 0.06627898663282394 | global_f1: 0.9759637188208617 | global_precision: 0.9817518248175182 | global_recall: 0.9702434625788999 | global_auc: 0.9961144183427184| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 87 | global_acc: 98.271% | global_loss: 0.06623424589633942 | global_f1: 0.9764279238440617 | global_precision: 0.9817684594348223 | global_recall: 0.9711451758340848 | global_auc: 0.9961395846420995| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 88 | global_acc: 98.238% | global_loss: 0.06612185388803482 | global_f1: 0.9759418974126192 | global_precision: 0.9826325411334552 | global_recall: 0.9693417493237151 | global_auc: 0.9961386349704249| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 89 | global_acc: 98.271% | global_loss: 0.06605148315429688 | global_f1: 0.9763851044504995 | global_precision: 0.9835315645013724 | global_recall: 0.9693417493237151 | global_auc: 0.9961452826721482| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 90 | global_acc: 98.271% | global_loss: 0.06598436832427979 | global_f1: 0.9763851044504995 | global_precision: 0.9835315645013724 | global_recall: 0.9693417493237151 | global_auc: 0.9961533548813837| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 91 | global_acc: 98.238% | global_loss: 0.06592956185340881 | global_f1: 0.9759637188208617 | global_precision: 0.9817518248175182 | global_recall: 0.9702434625788999 | global_auc: 0.9961709238073668| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 92 | global_acc: 98.238% | global_loss: 0.06587947905063629 | global_f1: 0.9759637188208617 | global_precision: 0.9817518248175182 | global_recall: 0.9702434625788999 | global_auc: 0.9961851688824881| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 93 | global_acc: 98.238% | global_loss: 0.06577123701572418 | global_f1: 0.9759418974126192 | global_precision: 0.9826325411334552 | global_recall: 0.9693417493237151 | global_auc: 0.9961785211807648| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 94 | global_acc: 98.271% | global_loss: 0.06573469936847687 | global_f1: 0.9763851044504995 | global_precision: 0.9835315645013724 | global_recall: 0.9693417493237151 | global_auc: 0.9961661754489929| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 95 | global_acc: 98.271% | global_loss: 0.06569293886423111 | global_f1: 0.9764065335753177 | global_precision: 0.982648401826484 | global_recall: 0.9702434625788999 | global_auc: 0.9961972771963412| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 96 | global_acc: 98.238% | global_loss: 0.06563981622457504 | global_f1: 0.9759418974126192 | global_precision: 0.9826325411334552 | global_recall: 0.9693417493237151 | global_auc: 0.9961970397784227| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 97 | global_acc: 98.238% | global_loss: 0.06560180336236954 | global_f1: 0.9759418974126192 | global_precision: 0.9826325411334552 | global_recall: 0.9693417493237151 | global_auc: 0.996202737808471| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 98 | global_acc: 98.305% | global_loss: 0.06559838354587555 | global_f1: 0.9768707482993196 | global_precision: 0.9826642335766423 | global_recall: 0.9711451758340848 | global_auc: 0.9962136590327308| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 99 | global_acc: 98.305% | global_loss: 0.06556158512830734 | global_f1: 0.976891708201178 | global_precision: 0.9817850637522769 | global_recall: 0.9720468890892696 | global_auc: 0.9962179325552675| flobal_FPR: 0.027953110910730387 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e03d2",
   "metadata": {},
   "source": [
    "## fed avg non iid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e5726ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg non idd 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6508846879005432 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8454494819778432| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.5970965623855591 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9058310315666115| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.963% | global_loss: 0.507991373538971 | global_f1: 0.04409171075837743 | global_precision: 1.0 | global_recall: 0.02254283137962128 | global_auc: 0.9528298079146588| flobal_FPR: 0.9774571686203787 \n",
      "comm_round: 3 | global_acc: 89.428% | global_loss: 0.4051526188850403 | global_f1: 0.8333333333333333 | global_precision: 0.9949937421777222 | global_recall: 0.7168620378719567 | global_auc: 0.9758284816981649| flobal_FPR: 0.28313796212804326 \n",
      "comm_round: 4 | global_acc: 94.315% | global_loss: 0.306283175945282 | global_f1: 0.9184549356223175 | global_precision: 0.9746963562753036 | global_recall: 0.8683498647430117 | global_auc: 0.9838418112897918| flobal_FPR: 0.13165013525698827 \n",
      "comm_round: 5 | global_acc: 95.047% | global_loss: 0.2176462709903717 | global_f1: 0.9306008383791338 | global_precision: 0.9624277456647399 | global_recall: 0.9008115419296664 | global_auc: 0.9870616731030664| flobal_FPR: 0.09918845807033363 \n",
      "comm_round: 6 | global_acc: 95.246% | global_loss: 0.16081222891807556 | global_f1: 0.933826931975937 | global_precision: 0.9591254752851711 | global_recall: 0.9098286744815148 | global_auc: 0.9896965371646886| flobal_FPR: 0.09017132551848513 \n",
      "comm_round: 7 | global_acc: 96.011% | global_loss: 0.13043786585330963 | global_f1: 0.9451052150045745 | global_precision: 0.9591457753017641 | global_recall: 0.9314697926059513 | global_auc: 0.9914049965075825| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 8 | global_acc: 96.210% | global_loss: 0.11360412836074829 | global_f1: 0.947992700729927 | global_precision: 0.9593721144967683 | global_recall: 0.9368800721370604 | global_auc: 0.9924695784549885| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 9 | global_acc: 96.576% | global_loss: 0.10303506255149841 | global_f1: 0.9531178880291307 | global_precision: 0.9623161764705882 | global_recall: 0.9440937781785392 | global_auc: 0.9931324492839713| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 10 | global_acc: 96.809% | global_loss: 0.09595229476690292 | global_f1: 0.9563239308462238 | global_precision: 0.9651056014692379 | global_recall: 0.9477006311992786 | global_auc: 0.9936694886160482| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 11 | global_acc: 96.875% | global_loss: 0.09087682515382767 | global_f1: 0.9572727272727274 | global_precision: 0.9651695692025665 | global_recall: 0.9495040577096483 | global_auc: 0.9941300793783069| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 12 | global_acc: 96.875% | global_loss: 0.08687150478363037 | global_f1: 0.9571948998178506 | global_precision: 0.9668813247470102 | global_recall: 0.9477006311992786 | global_auc: 0.9945127970632354| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 13 | global_acc: 97.141% | global_loss: 0.08381661027669907 | global_f1: 0.96094459582198 | global_precision: 0.9679780420860018 | global_recall: 0.9540126239855726 | global_auc: 0.9948413834627023| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 14 | global_acc: 97.141% | global_loss: 0.08140356838703156 | global_f1: 0.961050724637681 | global_precision: 0.9654231119199272 | global_recall: 0.9567177637511272 | global_auc: 0.9949567685711858| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 15 | global_acc: 97.340% | global_loss: 0.07920224219560623 | global_f1: 0.9636032757051866 | global_precision: 0.9724517906336089 | global_recall: 0.9549143372407575 | global_auc: 0.9950208714092319| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 16 | global_acc: 97.374% | global_loss: 0.07738208025693893 | global_f1: 0.9641723356009071 | global_precision: 0.9698905109489051 | global_recall: 0.9585211902614968 | global_auc: 0.9951723440413563| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 17 | global_acc: 97.573% | global_loss: 0.07623934000730515 | global_f1: 0.9669832654907281 | global_precision: 0.97005444646098 | global_recall: 0.9639314697926059 | global_auc: 0.9953361624052525| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 18 | global_acc: 97.640% | global_loss: 0.074714794754982 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9954088122883715| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 19 | global_acc: 97.673% | global_loss: 0.07371442019939423 | global_f1: 0.9682107175295187 | global_precision: 0.9752973467520586 | global_recall: 0.9612263300270514 | global_auc: 0.9955014052766606| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 20 | global_acc: 97.739% | global_loss: 0.07272734493017197 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9955892499065762| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 21 | global_acc: 97.806% | global_loss: 0.07216405123472214 | global_f1: 0.9701627486437614 | global_precision: 0.9728014505893019 | global_recall: 0.9675383228133454 | global_auc: 0.9956585759388334| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 22 | global_acc: 97.906% | global_loss: 0.07163619250059128 | global_f1: 0.9715318572074108 | global_precision: 0.9737318840579711 | global_recall: 0.9693417493237151 | global_auc: 0.9957454708970741| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 23 | global_acc: 97.906% | global_loss: 0.07187081128358841 | global_f1: 0.9715575620767495 | global_precision: 0.972875226039783 | global_recall: 0.9702434625788999 | global_auc: 0.9957948538241616| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 24 | global_acc: 98.005% | global_loss: 0.07064340263605118 | global_f1: 0.972875226039783 | global_precision: 0.9755213055303718 | global_recall: 0.9702434625788999 | global_auc: 0.9958309413478025| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 25 | global_acc: 98.005% | global_loss: 0.0704784095287323 | global_f1: 0.9729486023444545 | global_precision: 0.9729486023444545 | global_recall: 0.9729486023444545 | global_auc: 0.9958679785431183| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 26 | global_acc: 98.138% | global_loss: 0.06984202563762665 | global_f1: 0.9746835443037976 | global_precision: 0.9773345421577516 | global_recall: 0.9720468890892696 | global_auc: 0.9958883964841255| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 27 | global_acc: 98.172% | global_loss: 0.06977711617946625 | global_f1: 0.9751693002257337 | global_precision: 0.976491862567812 | global_recall: 0.9738503155996393 | global_auc: 0.9959263833511158| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 28 | global_acc: 98.172% | global_loss: 0.06978564709424973 | global_f1: 0.9751693002257337 | global_precision: 0.976491862567812 | global_recall: 0.9738503155996393 | global_auc: 0.995971492755667| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 29 | global_acc: 98.072% | global_loss: 0.06942308694124222 | global_f1: 0.9737793851717903 | global_precision: 0.9764279238440616 | global_recall: 0.9711451758340848 | global_auc: 0.9959964216371294| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 30 | global_acc: 98.105% | global_loss: 0.06904306262731552 | global_f1: 0.9742198100407056 | global_precision: 0.9773139745916516 | global_recall: 0.9711451758340848 | global_auc: 0.9959990332342351| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 31 | global_acc: 98.039% | global_loss: 0.06917306035757065 | global_f1: 0.973339358337099 | global_precision: 0.9755434782608695 | global_recall: 0.9711451758340848 | global_auc: 0.9959596218597325| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 32 | global_acc: 98.072% | global_loss: 0.06898877769708633 | global_f1: 0.9737793851717903 | global_precision: 0.9764279238440616 | global_recall: 0.9711451758340848 | global_auc: 0.9959805146365772| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 33 | global_acc: 98.105% | global_loss: 0.0689084455370903 | global_f1: 0.9741730856366108 | global_precision: 0.9790528233151184 | global_recall: 0.9693417493237151 | global_auc: 0.9960417684595994| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 34 | global_acc: 98.105% | global_loss: 0.06898422539234161 | global_f1: 0.9742198100407056 | global_precision: 0.9773139745916516 | global_recall: 0.9711451758340848 | global_auc: 0.9960109041301697| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 35 | global_acc: 98.238% | global_loss: 0.0696113184094429 | global_f1: 0.9761153672825597 | global_precision: 0.9756756756756757 | global_recall: 0.9765554553651938 | global_auc: 0.99601446539895| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 36 | global_acc: 98.172% | global_loss: 0.0693119466304779 | global_f1: 0.9751693002257337 | global_precision: 0.976491862567812 | global_recall: 0.9738503155996393 | global_auc: 0.9960194511752425| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 37 | global_acc: 98.238% | global_loss: 0.06920676678419113 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.9960650354156309| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 38 | global_acc: 98.138% | global_loss: 0.06911002099514008 | global_f1: 0.9746376811594203 | global_precision: 0.9790718835304822 | global_recall: 0.9702434625788999 | global_auc: 0.9960403439520871| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 39 | global_acc: 98.271% | global_loss: 0.06972496211528778 | global_f1: 0.9765342960288809 | global_precision: 0.9774164408310749 | global_recall: 0.975653742110009 | global_auc: 0.9960949500733858| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 40 | global_acc: 98.371% | global_loss: 0.06998339295387268 | global_f1: 0.9779179810725552 | global_precision: 0.9774774774774775 | global_recall: 0.9783588818755635 | global_auc: 0.9960859281924757| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 41 | global_acc: 98.205% | global_loss: 0.06945628672838211 | global_f1: 0.9755434782608696 | global_precision: 0.9799818016378526 | global_recall: 0.9711451758340848 | global_auc: 0.9961186918652549| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 42 | global_acc: 98.338% | global_loss: 0.06961008161306381 | global_f1: 0.977416440831075 | global_precision: 0.9791855203619909 | global_recall: 0.975653742110009 | global_auc: 0.9961566787322452| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 43 | global_acc: 98.371% | global_loss: 0.07012800872325897 | global_f1: 0.9778980604420388 | global_precision: 0.9783393501805054 | global_recall: 0.9774571686203787 | global_auc: 0.9961666502848302| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 44 | global_acc: 98.371% | global_loss: 0.06992626190185547 | global_f1: 0.9778581111613194 | global_precision: 0.980072463768116 | global_recall: 0.975653742110009 | global_auc: 0.9962027378084712| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 45 | global_acc: 98.404% | global_loss: 0.07015756517648697 | global_f1: 0.978319783197832 | global_precision: 0.9800904977375565 | global_recall: 0.9765554553651938 | global_auc: 0.9962084358385197| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 46 | global_acc: 98.371% | global_loss: 0.07065626978874207 | global_f1: 0.9778781038374718 | global_precision: 0.9792043399638336 | global_recall: 0.9765554553651938 | global_auc: 0.9962279041078522| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 47 | global_acc: 98.438% | global_loss: 0.07049645483493805 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9962359763170878| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 48 | global_acc: 98.371% | global_loss: 0.07040760666131973 | global_f1: 0.9778581111613194 | global_precision: 0.980072463768116 | global_recall: 0.975653742110009 | global_auc: 0.9962383504962745| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 49 | global_acc: 98.404% | global_loss: 0.07104908674955368 | global_f1: 0.9783393501805054 | global_precision: 0.979223125564589 | global_recall: 0.9774571686203787 | global_auc: 0.9962559194222578| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 50 | global_acc: 98.305% | global_loss: 0.07096655666828156 | global_f1: 0.9769126301493889 | global_precision: 0.980909090909091 | global_recall: 0.9729486023444545 | global_auc: 0.9962298034512018| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 51 | global_acc: 98.404% | global_loss: 0.07131646573543549 | global_f1: 0.978319783197832 | global_precision: 0.9800904977375565 | global_recall: 0.9765554553651938 | global_auc: 0.9962340769737382| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 52 | global_acc: 98.371% | global_loss: 0.07183865457773209 | global_f1: 0.9779179810725552 | global_precision: 0.9774774774774775 | global_recall: 0.9783588818755635 | global_auc: 0.9962364511529251| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 53 | global_acc: 98.371% | global_loss: 0.07123260200023651 | global_f1: 0.9778581111613194 | global_precision: 0.980072463768116 | global_recall: 0.975653742110009 | global_auc: 0.9962435736904859| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 54 | global_acc: 98.404% | global_loss: 0.07138419896364212 | global_f1: 0.9783001808318263 | global_precision: 0.9809610154125114 | global_recall: 0.975653742110009 | global_auc: 0.9962255299286654| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 55 | global_acc: 98.371% | global_loss: 0.07150257378816605 | global_f1: 0.9778581111613194 | global_precision: 0.980072463768116 | global_recall: 0.975653742110009 | global_auc: 0.9962383504962746| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 56 | global_acc: 98.371% | global_loss: 0.07187873870134354 | global_f1: 0.9778380823156942 | global_precision: 0.9809437386569873 | global_recall: 0.9747520288548241 | global_auc: 0.9962502213922092| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 57 | global_acc: 98.404% | global_loss: 0.0718231201171875 | global_f1: 0.978319783197832 | global_precision: 0.9800904977375565 | global_recall: 0.9765554553651938 | global_auc: 0.9962706393332166| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 58 | global_acc: 98.404% | global_loss: 0.07481642812490463 | global_f1: 0.9784560143626571 | global_precision: 0.9740840035746202 | global_recall: 0.9828674481514879 | global_auc: 0.9962739631840781| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 59 | global_acc: 98.438% | global_loss: 0.07222472876310349 | global_f1: 0.9787618617261635 | global_precision: 0.9809782608695652 | global_recall: 0.9765554553651938 | global_auc: 0.9962533078251523| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 60 | global_acc: 98.438% | global_loss: 0.0722961276769638 | global_f1: 0.9787618617261635 | global_precision: 0.9809782608695652 | global_recall: 0.9765554553651938 | global_auc: 0.9962677903181922| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 61 | global_acc: 98.471% | global_loss: 0.07247655838727951 | global_f1: 0.9792418772563176 | global_precision: 0.980126467931346 | global_recall: 0.9783588818755635 | global_auc: 0.9963162235736049| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 62 | global_acc: 98.438% | global_loss: 0.07259418815374374 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9962763373632652| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 63 | global_acc: 98.471% | global_loss: 0.07298433780670166 | global_f1: 0.9792418772563176 | global_precision: 0.980126467931346 | global_recall: 0.9783588818755635 | global_auc: 0.9963110003793938| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 64 | global_acc: 98.371% | global_loss: 0.07384835928678513 | global_f1: 0.9779179810725552 | global_precision: 0.9774774774774775 | global_recall: 0.9783588818755635 | global_auc: 0.9962946185430043| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 65 | global_acc: 98.438% | global_loss: 0.07304602861404419 | global_f1: 0.9787618617261635 | global_precision: 0.9809782608695652 | global_recall: 0.9765554553651938 | global_auc: 0.9963152739019302| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 66 | global_acc: 98.404% | global_loss: 0.07307150959968567 | global_f1: 0.9783001808318263 | global_precision: 0.9809610154125114 | global_recall: 0.975653742110009 | global_auc: 0.9963114752152313| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 67 | global_acc: 98.438% | global_loss: 0.07336671650409698 | global_f1: 0.9787618617261635 | global_precision: 0.9809782608695652 | global_recall: 0.9765554553651938 | global_auc: 0.9963257202903527| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 68 | global_acc: 98.438% | global_loss: 0.07346581667661667 | global_f1: 0.9787618617261635 | global_precision: 0.9809782608695652 | global_recall: 0.9765554553651938 | global_auc: 0.9963428143804983| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 69 | global_acc: 98.471% | global_loss: 0.07392784208059311 | global_f1: 0.9792043399638336 | global_precision: 0.9818676337262012 | global_recall: 0.9765554553651938 | global_auc: 0.9963299938128889| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 70 | global_acc: 98.404% | global_loss: 0.07384485006332397 | global_f1: 0.9783393501805054 | global_precision: 0.979223125564589 | global_recall: 0.9774571686203787 | global_auc: 0.9963176480811171| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 71 | global_acc: 98.371% | global_loss: 0.07577411085367203 | global_f1: 0.9779577147998201 | global_precision: 0.9757630161579892 | global_recall: 0.9801623083859333 | global_auc: 0.9963620452319122| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 72 | global_acc: 98.438% | global_loss: 0.07488763332366943 | global_f1: 0.9788383610986042 | global_precision: 0.9775179856115108 | global_recall: 0.9801623083859333 | global_auc: 0.9963627574856683| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 73 | global_acc: 98.471% | global_loss: 0.0744614526629448 | global_f1: 0.9792418772563176 | global_precision: 0.980126467931346 | global_recall: 0.9783588818755635 | global_auc: 0.9963584839631319| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 74 | global_acc: 98.471% | global_loss: 0.07438146322965622 | global_f1: 0.9792418772563176 | global_precision: 0.980126467931346 | global_recall: 0.9783588818755635 | global_auc: 0.9963670310082046| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 75 | global_acc: 98.471% | global_loss: 0.07446898519992828 | global_f1: 0.9792418772563176 | global_precision: 0.980126467931346 | global_recall: 0.9783588818755635 | global_auc: 0.9963665561723674| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 76 | global_acc: 98.504% | global_loss: 0.0743587464094162 | global_f1: 0.9796656122910077 | global_precision: 0.9818840579710145 | global_recall: 0.9774571686203787 | global_auc: 0.9963428143804984| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 77 | global_acc: 98.504% | global_loss: 0.07454003393650055 | global_f1: 0.979702300405954 | global_precision: 0.98014440433213 | global_recall: 0.9792605951307484 | global_auc: 0.9963760528891149| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 78 | global_acc: 98.471% | global_loss: 0.07474777847528458 | global_f1: 0.9792418772563176 | global_precision: 0.980126467931346 | global_recall: 0.9783588818755635 | global_auc: 0.9963722542024158| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 79 | global_acc: 98.504% | global_loss: 0.07463338226079941 | global_f1: 0.9796656122910077 | global_precision: 0.9818840579710145 | global_recall: 0.9774571686203787 | global_auc: 0.9963670310082047| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 80 | global_acc: 98.537% | global_loss: 0.07507256418466568 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9963926721434232| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 81 | global_acc: 98.438% | global_loss: 0.07506810128688812 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9963971830838783| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 82 | global_acc: 98.504% | global_loss: 0.07525115460157394 | global_f1: 0.9796839729119639 | global_precision: 0.9810126582278481 | global_recall: 0.9783588818755635 | global_auc: 0.9963665561723672| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 83 | global_acc: 98.537% | global_loss: 0.0756579339504242 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964050178751951| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 84 | global_acc: 98.537% | global_loss: 0.07551049441099167 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964031185318456| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 85 | global_acc: 98.537% | global_loss: 0.0762692391872406 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964069172185446| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 86 | global_acc: 98.537% | global_loss: 0.07593464106321335 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.996389823128399| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 87 | global_acc: 98.537% | global_loss: 0.07607442885637283 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964059675468698| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 88 | global_acc: 98.537% | global_loss: 0.07627823948860168 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964064423827073| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 89 | global_acc: 98.537% | global_loss: 0.0763482004404068 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964031185318456| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 90 | global_acc: 98.570% | global_loss: 0.07630617916584015 | global_f1: 0.98058690744921 | global_precision: 0.9819168173598554 | global_recall: 0.9792605951307484 | global_auc: 0.99641498942778| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 91 | global_acc: 98.504% | global_loss: 0.07656034082174301 | global_f1: 0.979702300405954 | global_precision: 0.98014440433213 | global_recall: 0.9792605951307484 | global_auc: 0.9964344576971127| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 92 | global_acc: 98.537% | global_loss: 0.07716453075408936 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964672213698919| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 93 | global_acc: 98.471% | global_loss: 0.07848303020000458 | global_f1: 0.97931654676259 | global_precision: 0.9766816143497757 | global_recall: 0.981965734896303 | global_auc: 0.9964354073687873| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 94 | global_acc: 98.504% | global_loss: 0.07699176669120789 | global_f1: 0.979702300405954 | global_precision: 0.98014440433213 | global_recall: 0.9792605951307484 | global_auc: 0.9964691207132415| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 95 | global_acc: 98.537% | global_loss: 0.07707645744085312 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964710200565909| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 96 | global_acc: 98.537% | global_loss: 0.07754315435886383 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964814664450133| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 97 | global_acc: 98.537% | global_loss: 0.07734919339418411 | global_f1: 0.9801084990958407 | global_precision: 0.9827742520398912 | global_recall: 0.9774571686203787 | global_auc: 0.9964501272797462| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 98 | global_acc: 98.537% | global_loss: 0.0790819376707077 | global_f1: 0.9802158273381296 | global_precision: 0.9775784753363229 | global_recall: 0.9828674481514879 | global_auc: 0.9964558253097947| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 99 | global_acc: 98.537% | global_loss: 0.07789722084999084 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9964669839519733| flobal_FPR: 0.019837691614066726 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg non idd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model(local_model, train_loader, loss, optimizer)\n",
    "\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ...\n",
    "                \n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-noniid-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-noniid-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-noniid-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
