{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\drebin.csv'\n",
    "malgenome_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\malgenome.csv'\n",
    "kronodroid_data_path = r'H:\\GIT project\\DW-FedAvg\\data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'H:\\GIT project\\DW-FedAvg\\data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 5\n",
      "No. of Rounds: 200\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg 2017|\n",
      "|=======================|\n",
      "[tensor([[-0.0053,  0.0590,  0.0018,  ..., -0.0481,  0.0512,  0.0559],\n",
      "        [-0.0221, -0.0615,  0.0247,  ..., -0.0115, -0.0480, -0.0197],\n",
      "        [-0.0148, -0.0484, -0.0506,  ...,  0.0346, -0.0553, -0.0088],\n",
      "        ...,\n",
      "        [-0.0356,  0.0318,  0.0298,  ..., -0.0449,  0.0507,  0.0155],\n",
      "        [-0.0256, -0.0443,  0.0537,  ...,  0.0578, -0.0459, -0.0309],\n",
      "        [ 0.0149,  0.0130, -0.0198,  ..., -0.0031, -0.0446, -0.0598]]), tensor([ 0.0121,  0.0671,  0.0167,  0.0387,  0.0363, -0.0159, -0.0429,  0.0425,\n",
      "        -0.0571,  0.0180, -0.0542,  0.0583, -0.0541,  0.0524,  0.0329, -0.0453,\n",
      "        -0.0371,  0.0226,  0.0621, -0.0539,  0.0446, -0.0288,  0.0596, -0.0619,\n",
      "         0.0555, -0.0239, -0.0026,  0.0362,  0.0493, -0.0556, -0.0438, -0.0463,\n",
      "        -0.0673, -0.0269, -0.0510,  0.0004,  0.0334,  0.0192,  0.0205, -0.0130,\n",
      "         0.0587, -0.0171,  0.0478, -0.0009,  0.0058,  0.0529, -0.0116, -0.0326,\n",
      "         0.0078,  0.0040, -0.0618,  0.0227,  0.0617, -0.0015, -0.0383, -0.0512,\n",
      "        -0.0616,  0.0205,  0.0631, -0.0251, -0.0008, -0.0297,  0.0302, -0.0671,\n",
      "         0.0416,  0.0178, -0.0362,  0.0348, -0.0613, -0.0141, -0.0140,  0.0423,\n",
      "         0.0175,  0.0365, -0.0516, -0.0540,  0.0464,  0.0073,  0.0556,  0.0082,\n",
      "        -0.0093, -0.0647,  0.0030,  0.0015, -0.0538, -0.0038,  0.0213, -0.0052,\n",
      "        -0.0094, -0.0634,  0.0356,  0.0193, -0.0391,  0.0561,  0.0019,  0.0348,\n",
      "        -0.0287, -0.0034,  0.0457, -0.0507,  0.0129,  0.0658,  0.0035, -0.0106,\n",
      "         0.0291, -0.0010, -0.0572,  0.0099, -0.0354,  0.0291,  0.0215, -0.0204,\n",
      "        -0.0437,  0.0148, -0.0546,  0.0218,  0.0455, -0.0033, -0.0182, -0.0637,\n",
      "        -0.0659,  0.0034,  0.0264,  0.0097,  0.0274,  0.0499,  0.0132, -0.0241,\n",
      "        -0.0659, -0.0015,  0.0227, -0.0571,  0.0602,  0.0355, -0.0015, -0.0362,\n",
      "        -0.0645, -0.0248, -0.0648, -0.0129,  0.0025, -0.0519,  0.0084, -0.0391,\n",
      "         0.0073, -0.0500,  0.0200,  0.0659,  0.0167, -0.0331,  0.0105,  0.0550,\n",
      "        -0.0195,  0.0395,  0.0045,  0.0595, -0.0406, -0.0395, -0.0036,  0.0634,\n",
      "         0.0169,  0.0311, -0.0157,  0.0037,  0.0408, -0.0101,  0.0459, -0.0041,\n",
      "         0.0172,  0.0377,  0.0363, -0.0572,  0.0593,  0.0290,  0.0230,  0.0221,\n",
      "         0.0083, -0.0262,  0.0552, -0.0514,  0.0670, -0.0657,  0.0642,  0.0548,\n",
      "         0.0019,  0.0662,  0.0618, -0.0252,  0.0632, -0.0269,  0.0563, -0.0161,\n",
      "        -0.0343,  0.0157,  0.0185,  0.0352, -0.0041, -0.0033, -0.0621,  0.0397]), tensor([[-0.0247,  0.0061,  0.0088,  ...,  0.0434,  0.0571,  0.0552],\n",
      "        [-0.0158, -0.0370,  0.0358,  ...,  0.0364,  0.0155,  0.0347],\n",
      "        [ 0.0638,  0.0514,  0.0428,  ...,  0.0265, -0.0503,  0.0597],\n",
      "        ...,\n",
      "        [ 0.0425, -0.0660, -0.0444,  ...,  0.0004,  0.0594, -0.0200],\n",
      "        [-0.0214, -0.0155,  0.0099,  ..., -0.0067, -0.0134,  0.0363],\n",
      "        [-0.0090,  0.0570,  0.0248,  ...,  0.0553, -0.0494, -0.0547]]), tensor([-0.0410,  0.0252, -0.0461, -0.0366, -0.0008, -0.0017, -0.0651,  0.0273,\n",
      "        -0.0131,  0.0055, -0.0246, -0.0516, -0.0468,  0.0237,  0.0485,  0.0259,\n",
      "        -0.0182,  0.0033, -0.0151,  0.0535, -0.0567,  0.0541,  0.0404,  0.0706,\n",
      "         0.0112,  0.0324,  0.0057,  0.0552,  0.0157,  0.0291, -0.0482, -0.0125,\n",
      "         0.0404,  0.0588, -0.0568, -0.0272,  0.0536,  0.0612,  0.0678, -0.0616,\n",
      "        -0.0152, -0.0097, -0.0300,  0.0328,  0.0328, -0.0145,  0.0114, -0.0314,\n",
      "         0.0078, -0.0552,  0.0085, -0.0614, -0.0167,  0.0097, -0.0661,  0.0574,\n",
      "        -0.0602,  0.0260,  0.0171,  0.0191,  0.0252, -0.0099,  0.0299, -0.0480,\n",
      "         0.0382,  0.0458, -0.0629, -0.0034, -0.0323,  0.0642,  0.0337, -0.0161,\n",
      "        -0.0268,  0.0344,  0.0034,  0.0615,  0.0449, -0.0072, -0.0183, -0.0634,\n",
      "        -0.0524,  0.0208, -0.0286, -0.0514,  0.0128, -0.0239,  0.0561, -0.0454,\n",
      "        -0.0634, -0.0130,  0.0441, -0.0004, -0.0138, -0.0019,  0.0178, -0.0182,\n",
      "        -0.0446,  0.0209, -0.0491,  0.0571]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0016,  0.0883],\n",
      "        [-0.0076, -0.0842, -0.0161,  ...,  0.0451,  0.0600, -0.0553],\n",
      "        [ 0.0753, -0.0663,  0.0633,  ..., -0.0435,  0.0811, -0.0672],\n",
      "        ...,\n",
      "        [-0.0694,  0.0925,  0.0705,  ...,  0.0109, -0.0975, -0.0931],\n",
      "        [-0.0209,  0.0587, -0.0464,  ..., -0.0224,  0.0192,  0.0256],\n",
      "        [-0.0676, -0.0800,  0.0431,  ..., -0.0904, -0.0858,  0.0987]]), tensor([ 0.0394, -0.0284,  0.0192,  0.0944, -0.0275, -0.0159, -0.0727,  0.0765,\n",
      "         0.0946, -0.0982, -0.0397, -0.0918,  0.0722, -0.0376, -0.0583, -0.0279,\n",
      "        -0.0724,  0.0684, -0.0904, -0.0814,  0.0781, -0.0490,  0.0189,  0.0179,\n",
      "        -0.0772,  0.0675,  0.0325,  0.0854,  0.0381, -0.0308,  0.0642, -0.0660,\n",
      "        -0.0489,  0.0572, -0.0016,  0.0452,  0.0039, -0.0558,  0.0488,  0.0005,\n",
      "         0.0229, -0.0018,  0.0497, -0.0495, -0.0163,  0.0594,  0.0419,  0.0249,\n",
      "         0.0739,  0.0366]), tensor([[-0.0066,  0.0275,  0.0970,  0.1408,  0.1133, -0.0268, -0.0069, -0.1271,\n",
      "          0.0576,  0.0925,  0.1053, -0.0147,  0.0002, -0.0016, -0.0915, -0.0615,\n",
      "         -0.0450,  0.0104, -0.1365, -0.1381, -0.0099, -0.0559, -0.1236, -0.0093,\n",
      "          0.1344, -0.0672,  0.0816,  0.0395, -0.0356, -0.1156,  0.0981, -0.1221,\n",
      "          0.0580, -0.0273, -0.1028,  0.1289,  0.0487,  0.0452, -0.0148, -0.0666,\n",
      "          0.0079,  0.0719,  0.0250,  0.1332, -0.0257,  0.1383, -0.1030,  0.1220,\n",
      "          0.0055,  0.0916]]), tensor([-0.0066])]\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6749970316886902 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6527698836319813| flobal_FPR: 1.0 \n",
      "[tensor([[-0.0055,  0.0587,  0.0016,  ..., -0.0481,  0.0512,  0.0560],\n",
      "        [-0.0219, -0.0613,  0.0248,  ..., -0.0115, -0.0480, -0.0197],\n",
      "        [-0.0148, -0.0484, -0.0506,  ...,  0.0347, -0.0552, -0.0088],\n",
      "        ...,\n",
      "        [-0.0356,  0.0318,  0.0298,  ..., -0.0450,  0.0506,  0.0154],\n",
      "        [-0.0258, -0.0445,  0.0536,  ...,  0.0578, -0.0459, -0.0309],\n",
      "        [ 0.0151,  0.0132, -0.0195,  ..., -0.0032, -0.0448, -0.0601]]), tensor([ 0.0122,  0.0671,  0.0169,  0.0390,  0.0360, -0.0156, -0.0429,  0.0423,\n",
      "        -0.0561,  0.0179, -0.0544,  0.0581, -0.0541,  0.0525,  0.0327, -0.0451,\n",
      "        -0.0373,  0.0223,  0.0624, -0.0542,  0.0447, -0.0289,  0.0600, -0.0629,\n",
      "         0.0556, -0.0241, -0.0026,  0.0365,  0.0494, -0.0558, -0.0435, -0.0461,\n",
      "        -0.0675, -0.0271, -0.0510,  0.0017,  0.0328,  0.0186,  0.0203, -0.0132,\n",
      "         0.0589, -0.0169,  0.0478, -0.0009,  0.0058,  0.0535, -0.0117, -0.0327,\n",
      "         0.0083,  0.0040, -0.0621,  0.0227,  0.0616, -0.0015, -0.0384, -0.0513,\n",
      "        -0.0619,  0.0210,  0.0633, -0.0251, -0.0006, -0.0298,  0.0304, -0.0675,\n",
      "         0.0412,  0.0177, -0.0363,  0.0341, -0.0612, -0.0142, -0.0140,  0.0427,\n",
      "         0.0176,  0.0362, -0.0516, -0.0541,  0.0463,  0.0076,  0.0560,  0.0079,\n",
      "        -0.0091, -0.0648,  0.0028,  0.0011, -0.0536, -0.0038,  0.0213, -0.0051,\n",
      "        -0.0093, -0.0638,  0.0343,  0.0192, -0.0392,  0.0562,  0.0018,  0.0348,\n",
      "        -0.0285, -0.0034,  0.0454, -0.0510,  0.0126,  0.0654,  0.0034, -0.0110,\n",
      "         0.0291, -0.0005, -0.0575,  0.0096, -0.0357,  0.0292,  0.0212, -0.0205,\n",
      "        -0.0437,  0.0153, -0.0548,  0.0218,  0.0451, -0.0035, -0.0185, -0.0636,\n",
      "        -0.0660,  0.0036,  0.0265,  0.0097,  0.0275,  0.0497,  0.0133, -0.0244,\n",
      "        -0.0661, -0.0016,  0.0225, -0.0577,  0.0600,  0.0353, -0.0012, -0.0365,\n",
      "        -0.0647, -0.0244, -0.0652, -0.0130,  0.0024, -0.0518,  0.0088, -0.0392,\n",
      "         0.0072, -0.0493,  0.0198,  0.0660,  0.0167, -0.0331,  0.0103,  0.0548,\n",
      "        -0.0197,  0.0395,  0.0047,  0.0594, -0.0406, -0.0396, -0.0037,  0.0631,\n",
      "         0.0170,  0.0312, -0.0157,  0.0041,  0.0408, -0.0098,  0.0458, -0.0041,\n",
      "         0.0173,  0.0376,  0.0363, -0.0577,  0.0590,  0.0287,  0.0228,  0.0221,\n",
      "         0.0092, -0.0262,  0.0552, -0.0517,  0.0672, -0.0659,  0.0644,  0.0550,\n",
      "         0.0014,  0.0663,  0.0618, -0.0256,  0.0632, -0.0270,  0.0561, -0.0164,\n",
      "        -0.0345,  0.0156,  0.0184,  0.0358, -0.0043, -0.0034, -0.0622,  0.0394]), tensor([[-0.0246,  0.0061,  0.0088,  ...,  0.0434,  0.0571,  0.0552],\n",
      "        [-0.0151, -0.0369,  0.0360,  ...,  0.0369,  0.0156,  0.0347],\n",
      "        [ 0.0639,  0.0514,  0.0428,  ...,  0.0265, -0.0503,  0.0595],\n",
      "        ...,\n",
      "        [ 0.0423, -0.0660, -0.0442,  ...,  0.0004,  0.0594, -0.0199],\n",
      "        [-0.0214, -0.0156,  0.0095,  ..., -0.0066, -0.0133,  0.0360],\n",
      "        [-0.0090,  0.0569,  0.0248,  ...,  0.0553, -0.0493, -0.0547]]), tensor([-0.0408,  0.0283, -0.0460, -0.0366, -0.0005, -0.0013, -0.0653,  0.0273,\n",
      "        -0.0152,  0.0039, -0.0250, -0.0518, -0.0458,  0.0239,  0.0522,  0.0264,\n",
      "        -0.0184,  0.0033, -0.0167,  0.0537, -0.0572,  0.0539,  0.0396,  0.0698,\n",
      "         0.0108,  0.0318,  0.0032,  0.0573,  0.0160,  0.0285, -0.0487, -0.0138,\n",
      "         0.0437,  0.0576, -0.0580, -0.0255,  0.0538,  0.0614,  0.0660, -0.0617,\n",
      "        -0.0135, -0.0087, -0.0298,  0.0327,  0.0304, -0.0157,  0.0094, -0.0315,\n",
      "         0.0063, -0.0583,  0.0093, -0.0603, -0.0174,  0.0088, -0.0657,  0.0576,\n",
      "        -0.0612,  0.0267,  0.0170,  0.0187,  0.0269, -0.0106,  0.0294, -0.0479,\n",
      "         0.0387,  0.0465, -0.0631, -0.0028, -0.0321,  0.0637,  0.0340, -0.0162,\n",
      "        -0.0288,  0.0357,  0.0029,  0.0605,  0.0469, -0.0073, -0.0211, -0.0634,\n",
      "        -0.0530,  0.0213, -0.0282, -0.0516,  0.0114, -0.0235,  0.0564, -0.0461,\n",
      "        -0.0633, -0.0135,  0.0438, -0.0007, -0.0135, -0.0019,  0.0165, -0.0185,\n",
      "        -0.0452,  0.0204, -0.0490,  0.0568]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0016,  0.0883],\n",
      "        [-0.0076, -0.0842, -0.0161,  ...,  0.0451,  0.0600, -0.0553],\n",
      "        [ 0.0748, -0.0681,  0.0616,  ..., -0.0443,  0.0806, -0.0683],\n",
      "        ...,\n",
      "        [-0.0696,  0.0921,  0.0695,  ...,  0.0103, -0.0974, -0.0934],\n",
      "        [-0.0209,  0.0587, -0.0464,  ..., -0.0225,  0.0192,  0.0255],\n",
      "        [-0.0683, -0.0823,  0.0412,  ..., -0.0914, -0.0864,  0.0970]]), tensor([ 0.0395, -0.0285,  0.0108,  0.0823, -0.0410, -0.0158, -0.0722,  0.0876,\n",
      "         0.0898, -0.0988, -0.0403, -0.0916,  0.0722, -0.0369, -0.0581, -0.0262,\n",
      "        -0.0714,  0.0680, -0.0906, -0.0747,  0.0793, -0.0472,  0.0229,  0.0184,\n",
      "        -0.0781,  0.0726,  0.0245,  0.0826,  0.0413, -0.0303,  0.0646, -0.0587,\n",
      "        -0.0525,  0.0591, -0.0011,  0.0429,  0.0036, -0.0565,  0.0498,  0.0026,\n",
      "         0.0226, -0.0042,  0.0481, -0.0547, -0.0164,  0.0487,  0.0427,  0.0214,\n",
      "         0.0737,  0.0251]), tensor([[-0.0072,  0.0277,  0.0935,  0.1270,  0.1054, -0.0269, -0.0100, -0.1345,\n",
      "          0.0504,  0.0921,  0.1052, -0.0152,  0.0011, -0.0135, -0.0919, -0.0622,\n",
      "         -0.0488, -0.0022, -0.1365, -0.1420, -0.0172, -0.0569, -0.1241, -0.0099,\n",
      "          0.1341, -0.0713,  0.0762,  0.0364, -0.0422, -0.1150,  0.0982, -0.1245,\n",
      "          0.0540, -0.0315, -0.1030,  0.1284,  0.0490,  0.0447, -0.0156, -0.0653,\n",
      "         -0.0017,  0.0711,  0.0125,  0.1315, -0.0250,  0.1320, -0.1034,  0.1209,\n",
      "          0.0017,  0.0813]]), tensor([-0.0938])]\n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6611643433570862 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7710355837228174| flobal_FPR: 1.0 \n",
      "[tensor([[-0.0059,  0.0583,  0.0012,  ..., -0.0480,  0.0512,  0.0560],\n",
      "        [-0.0216, -0.0611,  0.0251,  ..., -0.0115, -0.0480, -0.0197],\n",
      "        [-0.0150, -0.0485, -0.0507,  ...,  0.0347, -0.0552, -0.0088],\n",
      "        ...,\n",
      "        [-0.0353,  0.0320,  0.0300,  ..., -0.0450,  0.0504,  0.0154],\n",
      "        [-0.0259, -0.0446,  0.0535,  ...,  0.0579, -0.0459, -0.0310],\n",
      "        [ 0.0157,  0.0138, -0.0190,  ..., -0.0032, -0.0448, -0.0602]]), tensor([ 0.0121,  0.0671,  0.0170,  0.0394,  0.0356, -0.0156, -0.0426,  0.0422,\n",
      "        -0.0556,  0.0177, -0.0543,  0.0579, -0.0538,  0.0528,  0.0326, -0.0447,\n",
      "        -0.0372,  0.0223,  0.0625, -0.0543,  0.0448, -0.0291,  0.0604, -0.0633,\n",
      "         0.0555, -0.0242, -0.0025,  0.0367,  0.0495, -0.0560, -0.0434, -0.0457,\n",
      "        -0.0678, -0.0272, -0.0509,  0.0027,  0.0327,  0.0184,  0.0202, -0.0131,\n",
      "         0.0590, -0.0168,  0.0478, -0.0008,  0.0057,  0.0537, -0.0116, -0.0328,\n",
      "         0.0085,  0.0038, -0.0623,  0.0227,  0.0618, -0.0015, -0.0385, -0.0514,\n",
      "        -0.0621,  0.0213,  0.0636, -0.0249, -0.0004, -0.0299,  0.0309, -0.0673,\n",
      "         0.0414,  0.0177, -0.0363,  0.0341, -0.0610, -0.0143, -0.0141,  0.0429,\n",
      "         0.0181,  0.0363, -0.0516, -0.0541,  0.0467,  0.0078,  0.0562,  0.0078,\n",
      "        -0.0089, -0.0650,  0.0028,  0.0010, -0.0535, -0.0038,  0.0212, -0.0049,\n",
      "        -0.0092, -0.0636,  0.0338,  0.0193, -0.0389,  0.0563,  0.0017,  0.0348,\n",
      "        -0.0284, -0.0033,  0.0454, -0.0510,  0.0126,  0.0654,  0.0033, -0.0112,\n",
      "         0.0290, -0.0005, -0.0576,  0.0096, -0.0360,  0.0295,  0.0211, -0.0206,\n",
      "        -0.0436,  0.0153, -0.0548,  0.0219,  0.0449, -0.0037, -0.0190, -0.0635,\n",
      "        -0.0661,  0.0038,  0.0263,  0.0098,  0.0276,  0.0500,  0.0134, -0.0245,\n",
      "        -0.0662, -0.0016,  0.0225, -0.0581,  0.0599,  0.0354, -0.0012, -0.0367,\n",
      "        -0.0646, -0.0240, -0.0652, -0.0130,  0.0023, -0.0517,  0.0089, -0.0391,\n",
      "         0.0071, -0.0488,  0.0204,  0.0661,  0.0167, -0.0331,  0.0103,  0.0546,\n",
      "        -0.0199,  0.0396,  0.0047,  0.0597, -0.0404, -0.0397, -0.0038,  0.0632,\n",
      "         0.0171,  0.0313, -0.0156,  0.0043,  0.0409, -0.0095,  0.0456, -0.0040,\n",
      "         0.0175,  0.0378,  0.0363, -0.0583,  0.0588,  0.0283,  0.0227,  0.0220,\n",
      "         0.0096, -0.0263,  0.0552, -0.0520,  0.0672, -0.0659,  0.0644,  0.0551,\n",
      "         0.0014,  0.0665,  0.0619, -0.0255,  0.0633, -0.0271,  0.0563, -0.0166,\n",
      "        -0.0348,  0.0155,  0.0183,  0.0366, -0.0042, -0.0033, -0.0623,  0.0394]), tensor([[-0.0245,  0.0061,  0.0087,  ...,  0.0435,  0.0572,  0.0553],\n",
      "        [-0.0148, -0.0367,  0.0360,  ...,  0.0371,  0.0156,  0.0346],\n",
      "        [ 0.0642,  0.0512,  0.0429,  ...,  0.0268, -0.0502,  0.0596],\n",
      "        ...,\n",
      "        [ 0.0422, -0.0660, -0.0439,  ...,  0.0004,  0.0594, -0.0199],\n",
      "        [-0.0213, -0.0158,  0.0091,  ..., -0.0063, -0.0133,  0.0359],\n",
      "        [-0.0090,  0.0567,  0.0248,  ...,  0.0553, -0.0492, -0.0546]]), tensor([-4.0433e-02,  2.9221e-02, -4.5116e-02, -3.6592e-02,  3.8409e-05,\n",
      "        -3.1213e-04, -6.5646e-02,  2.7758e-02, -1.6170e-02,  3.7260e-03,\n",
      "        -2.4923e-02, -5.1140e-02, -4.4601e-02,  2.4123e-02,  5.4095e-02,\n",
      "         2.5756e-02, -1.8497e-02,  3.2420e-03, -1.7144e-02,  5.4434e-02,\n",
      "        -5.7299e-02,  5.5800e-02,  3.9147e-02,  6.9540e-02,  1.0314e-02,\n",
      "         3.1211e-02,  2.6412e-03,  5.7859e-02,  1.6112e-02,  2.9100e-02,\n",
      "        -4.9156e-02, -1.4245e-02,  4.4917e-02,  5.7567e-02, -5.8586e-02,\n",
      "        -2.4686e-02,  5.3782e-02,  6.3178e-02,  6.5117e-02, -6.1934e-02,\n",
      "        -1.0501e-02, -6.6279e-03, -2.9890e-02,  3.2997e-02,  2.9394e-02,\n",
      "        -1.6517e-02,  9.5543e-03, -3.1394e-02,  6.2756e-03, -6.0276e-02,\n",
      "         1.0225e-02, -5.9825e-02, -1.7603e-02,  9.7165e-03, -6.5398e-02,\n",
      "         5.8006e-02, -6.1761e-02,  2.8129e-02,  1.7380e-02,  1.8550e-02,\n",
      "         2.7523e-02, -1.0982e-02,  2.9443e-02, -4.7698e-02,  3.9929e-02,\n",
      "         4.6610e-02, -6.3450e-02, -1.6932e-03, -3.1526e-02,  6.3474e-02,\n",
      "         3.4098e-02, -1.4736e-02, -2.9685e-02,  3.7011e-02,  2.5835e-03,\n",
      "         6.0183e-02,  4.8042e-02, -7.2359e-03, -2.1837e-02, -6.3296e-02,\n",
      "        -5.3180e-02,  2.1284e-02, -2.7702e-02, -5.1623e-02,  1.1352e-02,\n",
      "        -2.3377e-02,  5.6584e-02, -4.6260e-02, -6.3064e-02, -1.3628e-02,\n",
      "         4.3905e-02, -7.9582e-04, -1.3481e-02, -1.9199e-03,  1.5270e-02,\n",
      "        -1.8398e-02, -4.5611e-02,  2.0198e-02, -4.8662e-02,  5.6854e-02]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0016,  0.0883],\n",
      "        [-0.0076, -0.0841, -0.0161,  ...,  0.0451,  0.0600, -0.0553],\n",
      "        [ 0.0744, -0.0689,  0.0607,  ..., -0.0444,  0.0805, -0.0687],\n",
      "        ...,\n",
      "        [-0.0700,  0.0909,  0.0685,  ...,  0.0098, -0.0975, -0.0938],\n",
      "        [-0.0209,  0.0587, -0.0464,  ..., -0.0225,  0.0192,  0.0255],\n",
      "        [-0.0687, -0.0835,  0.0402,  ..., -0.0918, -0.0865,  0.0961]]), tensor([ 0.0395, -0.0281,  0.0103,  0.0764, -0.0447, -0.0157, -0.0717,  0.0969,\n",
      "         0.0867, -0.0996, -0.0407, -0.0915,  0.0722, -0.0360, -0.0570, -0.0238,\n",
      "        -0.0674,  0.0686, -0.0909, -0.0669,  0.0806, -0.0443,  0.0284,  0.0185,\n",
      "        -0.0787,  0.0769,  0.0205,  0.0808,  0.0443, -0.0260,  0.0658, -0.0541,\n",
      "        -0.0547,  0.0613,  0.0005,  0.0402,  0.0037, -0.0574,  0.0509,  0.0053,\n",
      "         0.0231, -0.0060,  0.0476, -0.0559, -0.0170,  0.0435,  0.0435,  0.0173,\n",
      "         0.0737,  0.0210]), tensor([[-0.0073,  0.0286,  0.0944,  0.1255,  0.1035, -0.0271, -0.0141, -0.1472,\n",
      "          0.0447,  0.0915,  0.1050, -0.0154,  0.0004, -0.0180, -0.0925, -0.0638,\n",
      "         -0.0584, -0.0155, -0.1364, -0.1508, -0.0235, -0.0634, -0.1283, -0.0101,\n",
      "          0.1339, -0.0768,  0.0741,  0.0318, -0.0499, -0.1219,  0.0986, -0.1270,\n",
      "          0.0499, -0.0386, -0.1038,  0.1277,  0.0499,  0.0440, -0.0208, -0.0665,\n",
      "         -0.0127,  0.0713,  0.0056,  0.1309, -0.0234,  0.1306, -0.1040,  0.1198,\n",
      "         -0.0010,  0.0786]]), tensor([-0.1584])]\n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6547876000404358 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8170253339164317| flobal_FPR: 1.0 \n",
      "[tensor([[-0.0060,  0.0581,  0.0011,  ..., -0.0479,  0.0514,  0.0562],\n",
      "        [-0.0214, -0.0610,  0.0252,  ..., -0.0115, -0.0480, -0.0197],\n",
      "        [-0.0151, -0.0486, -0.0508,  ...,  0.0348, -0.0551, -0.0088],\n",
      "        ...,\n",
      "        [-0.0352,  0.0322,  0.0301,  ..., -0.0450,  0.0503,  0.0153],\n",
      "        [-0.0259, -0.0446,  0.0535,  ...,  0.0579, -0.0459, -0.0310],\n",
      "        [ 0.0159,  0.0141, -0.0187,  ..., -0.0033, -0.0449, -0.0604]]), tensor([ 1.2411e-02,  6.6958e-02,  1.7188e-02,  3.9555e-02,  3.5508e-02,\n",
      "        -1.5390e-02, -4.2467e-02,  4.2151e-02, -5.5133e-02,  1.7524e-02,\n",
      "        -5.4175e-02,  5.7831e-02, -5.4008e-02,  5.2740e-02,  3.2669e-02,\n",
      "        -4.4428e-02, -3.7649e-02,  2.2188e-02,  6.2605e-02, -5.4611e-02,\n",
      "         4.4918e-02, -2.9288e-02,  6.0908e-02, -6.3736e-02,  5.5521e-02,\n",
      "        -2.4451e-02, -2.4266e-03,  3.6979e-02,  4.9559e-02, -5.6219e-02,\n",
      "        -4.3206e-02, -4.5638e-02, -6.7927e-02, -2.7444e-02, -5.0978e-02,\n",
      "         3.9670e-03,  3.2681e-02,  1.8275e-02,  2.0085e-02, -1.3002e-02,\n",
      "         5.9156e-02, -1.6851e-02,  4.7647e-02, -8.4914e-04,  5.6260e-03,\n",
      "         5.4102e-02, -1.1693e-02, -3.2947e-02,  8.7536e-03,  3.7387e-03,\n",
      "        -6.2451e-02,  2.2618e-02,  6.1941e-02, -1.5410e-03, -3.8668e-02,\n",
      "        -5.1470e-02, -6.2123e-02,  2.1897e-02,  6.3654e-02, -2.4796e-02,\n",
      "        -2.4599e-04, -3.0124e-02,  3.0980e-02, -6.7518e-02,  4.0975e-02,\n",
      "         1.7753e-02, -3.6357e-02,  3.3943e-02, -6.0960e-02, -1.4305e-02,\n",
      "        -1.4183e-02,  4.3036e-02,  1.8144e-02,  3.6071e-02, -5.1597e-02,\n",
      "        -5.4217e-02,  4.6958e-02,  7.6447e-03,  5.6124e-02,  7.7714e-03,\n",
      "        -8.7808e-03, -6.5099e-02,  2.6709e-03,  5.2550e-04, -5.3342e-02,\n",
      "        -3.6880e-03,  2.1193e-02, -4.7851e-03, -9.1228e-03, -6.3817e-02,\n",
      "         3.3452e-02,  1.9458e-02, -3.8895e-02,  5.6332e-02,  1.5159e-03,\n",
      "         3.4886e-02, -2.8235e-02, -3.1686e-03,  4.5208e-02, -5.1081e-02,\n",
      "         1.2327e-02,  6.5484e-02,  3.2154e-03, -1.1323e-02,  2.9077e-02,\n",
      "         4.2781e-06, -5.7322e-02,  9.5510e-03, -3.6243e-02,  2.9491e-02,\n",
      "         2.0850e-02, -2.0951e-02, -4.3542e-02,  1.5304e-02, -5.4994e-02,\n",
      "         2.1906e-02,  4.4664e-02, -3.7469e-03, -1.9303e-02, -6.3436e-02,\n",
      "        -6.6339e-02,  3.7399e-03,  2.6265e-02,  9.6257e-03,  2.7673e-02,\n",
      "         5.0207e-02,  1.3361e-02, -2.4502e-02, -6.6283e-02, -1.5572e-03,\n",
      "         2.2476e-02, -5.8161e-02,  5.9897e-02,  3.5097e-02, -1.0359e-03,\n",
      "        -3.6768e-02, -6.4479e-02, -2.3757e-02, -6.5565e-02, -1.3079e-02,\n",
      "         2.1029e-03, -5.1529e-02,  8.9873e-03, -3.9141e-02,  7.0196e-03,\n",
      "        -4.8600e-02,  2.0775e-02,  6.6116e-02,  1.6723e-02, -3.3317e-02,\n",
      "         1.0139e-02,  5.4541e-02, -1.9874e-02,  3.9643e-02,  4.7762e-03,\n",
      "         5.9589e-02, -4.0234e-02, -3.9745e-02, -4.1989e-03,  6.3407e-02,\n",
      "         1.7097e-02,  3.1239e-02, -1.5497e-02,  4.7082e-03,  4.0891e-02,\n",
      "        -9.1975e-03,  4.5676e-02, -3.7168e-03,  1.7559e-02,  3.7805e-02,\n",
      "         3.6365e-02, -5.8856e-02,  5.8570e-02,  2.8073e-02,  2.2546e-02,\n",
      "         2.1879e-02,  1.0233e-02, -2.6314e-02,  5.5706e-02, -5.2105e-02,\n",
      "         6.7503e-02, -6.6150e-02,  6.4411e-02,  5.5063e-02,  1.2764e-03,\n",
      "         6.6833e-02,  6.2004e-02, -2.6021e-02,  6.3330e-02, -2.7255e-02,\n",
      "         5.6425e-02, -1.6881e-02, -3.4852e-02,  1.5431e-02,  1.8092e-02,\n",
      "         3.7211e-02, -4.2073e-03, -3.3092e-03, -6.2306e-02,  3.9107e-02]), tensor([[-0.0246,  0.0061,  0.0087,  ...,  0.0436,  0.0572,  0.0553],\n",
      "        [-0.0144, -0.0367,  0.0360,  ...,  0.0374,  0.0156,  0.0345],\n",
      "        [ 0.0643,  0.0512,  0.0431,  ...,  0.0269, -0.0501,  0.0595],\n",
      "        ...,\n",
      "        [ 0.0422, -0.0659, -0.0439,  ...,  0.0005,  0.0594, -0.0198],\n",
      "        [-0.0215, -0.0160,  0.0091,  ..., -0.0063, -0.0133,  0.0356],\n",
      "        [-0.0092,  0.0566,  0.0248,  ...,  0.0552, -0.0492, -0.0547]]), tensor([-4.0324e-02,  3.0934e-02, -4.4315e-02, -3.6597e-02,  9.5628e-05,\n",
      "        -3.8573e-05, -6.6076e-02,  2.8354e-02, -1.7859e-02,  3.5796e-03,\n",
      "        -2.4512e-02, -5.1324e-02, -4.4277e-02,  2.4115e-02,  5.6381e-02,\n",
      "         2.5489e-02, -1.8568e-02,  3.2612e-03, -1.7997e-02,  5.4543e-02,\n",
      "        -5.6951e-02,  5.3411e-02,  3.8907e-02,  6.8957e-02,  9.6683e-03,\n",
      "         3.0666e-02,  2.3275e-03,  5.7816e-02,  1.6631e-02,  2.9546e-02,\n",
      "        -4.9189e-02, -1.3747e-02,  4.6904e-02,  5.7296e-02, -5.9260e-02,\n",
      "        -2.3716e-02,  5.3720e-02,  6.3447e-02,  6.3423e-02, -6.2229e-02,\n",
      "        -9.4559e-03, -6.7237e-03, -2.9844e-02,  3.3011e-02,  2.8532e-02,\n",
      "        -1.7114e-02,  9.9203e-03, -3.1495e-02,  5.3876e-03, -6.1662e-02,\n",
      "         1.0507e-02, -5.9702e-02, -1.7659e-02,  1.0663e-02, -6.5218e-02,\n",
      "         5.7636e-02, -6.1965e-02,  2.7931e-02,  1.7025e-02,  1.7464e-02,\n",
      "         2.8079e-02, -1.1649e-02,  2.8443e-02, -4.7563e-02,  4.0691e-02,\n",
      "         4.6876e-02, -6.3633e-02, -1.8184e-03, -3.1636e-02,  6.3599e-02,\n",
      "         3.3719e-02, -1.4183e-02, -2.9697e-02,  3.8185e-02,  2.7123e-03,\n",
      "         6.0061e-02,  4.9004e-02, -6.4643e-03, -2.2068e-02, -6.3521e-02,\n",
      "        -5.3496e-02,  2.1131e-02, -2.7439e-02, -5.1752e-02,  1.0560e-02,\n",
      "        -2.3052e-02,  5.6555e-02, -4.6449e-02, -6.2914e-02, -1.2863e-02,\n",
      "         4.3650e-02, -8.4433e-04, -1.3710e-02, -1.9197e-03,  1.4810e-02,\n",
      "        -1.8320e-02, -4.5655e-02,  1.9861e-02, -4.8394e-02,  5.6105e-02]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0015,  0.0884],\n",
      "        [-0.0076, -0.0841, -0.0161,  ...,  0.0451,  0.0600, -0.0553],\n",
      "        [ 0.0742, -0.0697,  0.0601,  ..., -0.0444,  0.0801, -0.0690],\n",
      "        ...,\n",
      "        [-0.0700,  0.0920,  0.0690,  ...,  0.0100, -0.0973, -0.0936],\n",
      "        [-0.0209,  0.0587, -0.0464,  ..., -0.0225,  0.0192,  0.0256],\n",
      "        [-0.0689, -0.0842,  0.0397,  ..., -0.0919, -0.0868,  0.0955]]), tensor([ 0.0396, -0.0281,  0.0072,  0.0718, -0.0457, -0.0157, -0.0712,  0.1045,\n",
      "         0.0845, -0.1000, -0.0408, -0.0915,  0.0722, -0.0350, -0.0588, -0.0210,\n",
      "        -0.0681,  0.0695, -0.0910, -0.0666,  0.0818, -0.0420,  0.0330,  0.0187,\n",
      "        -0.0790,  0.0808,  0.0177,  0.0798,  0.0465, -0.0270,  0.0664, -0.0483,\n",
      "        -0.0553,  0.0623,  0.0010,  0.0392,  0.0029, -0.0582,  0.0518,  0.0081,\n",
      "         0.0236, -0.0099,  0.0474, -0.0567, -0.0174,  0.0401,  0.0445,  0.0183,\n",
      "         0.0737,  0.0177]), tensor([[-0.0077,  0.0290,  0.0941,  0.1241,  0.1031, -0.0272, -0.0159, -0.1530,\n",
      "          0.0449,  0.0912,  0.1049, -0.0155,  0.0010, -0.0234, -0.0917, -0.0653,\n",
      "         -0.0620, -0.0205, -0.1365, -0.1527, -0.0259, -0.0636, -0.1302, -0.0098,\n",
      "          0.1338, -0.0808,  0.0734,  0.0321, -0.0501, -0.1198,  0.0990, -0.1292,\n",
      "          0.0499, -0.0395, -0.1040,  0.1274,  0.0502,  0.0435, -0.0203, -0.0677,\n",
      "         -0.0121,  0.0702,  0.0023,  0.1306, -0.0224,  0.1296, -0.1046,  0.1205,\n",
      "         -0.0018,  0.0773]]), tensor([-0.2090])]\n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6465977430343628 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8640146135477312| flobal_FPR: 1.0 \n",
      "[tensor([[-0.0062,  0.0579,  0.0009,  ..., -0.0478,  0.0516,  0.0563],\n",
      "        [-0.0211, -0.0607,  0.0254,  ..., -0.0115, -0.0480, -0.0197],\n",
      "        [-0.0153, -0.0488, -0.0510,  ...,  0.0348, -0.0551, -0.0088],\n",
      "        ...,\n",
      "        [-0.0349,  0.0324,  0.0304,  ..., -0.0451,  0.0502,  0.0153],\n",
      "        [-0.0258, -0.0445,  0.0536,  ...,  0.0579, -0.0459, -0.0311],\n",
      "        [ 0.0163,  0.0145, -0.0183,  ..., -0.0034, -0.0450, -0.0606]]), tensor([ 1.2572e-02,  6.6838e-02,  1.7345e-02,  3.9889e-02,  3.5430e-02,\n",
      "        -1.5272e-02, -4.2231e-02,  4.2089e-02, -5.4737e-02,  1.7391e-02,\n",
      "        -5.3963e-02,  5.7779e-02, -5.3914e-02,  5.2890e-02,  3.2601e-02,\n",
      "        -4.4023e-02, -3.7799e-02,  2.2294e-02,  6.2730e-02, -5.4816e-02,\n",
      "         4.4977e-02, -2.9385e-02,  6.1394e-02, -6.3942e-02,  5.5386e-02,\n",
      "        -2.4527e-02, -2.3331e-03,  3.7195e-02,  4.9722e-02, -5.6414e-02,\n",
      "        -4.3046e-02, -4.5507e-02, -6.8051e-02, -2.7640e-02, -5.0903e-02,\n",
      "         5.0817e-03,  3.2772e-02,  1.8180e-02,  2.0063e-02, -1.2805e-02,\n",
      "         5.9385e-02, -1.6854e-02,  4.7517e-02, -7.3665e-04,  5.5189e-03,\n",
      "         5.4436e-02, -1.1554e-02, -3.3128e-02,  8.9417e-03,  3.6173e-03,\n",
      "        -6.2613e-02,  2.2515e-02,  6.2142e-02, -1.6241e-03, -3.8825e-02,\n",
      "        -5.1667e-02, -6.2110e-02,  2.2283e-02,  6.3843e-02, -2.4545e-02,\n",
      "        -9.2330e-05, -3.0176e-02,  3.1413e-02, -6.7420e-02,  4.0950e-02,\n",
      "         1.7908e-02, -3.6341e-02,  3.3885e-02, -6.0893e-02, -1.4332e-02,\n",
      "        -1.4356e-02,  4.3219e-02,  1.8428e-02,  3.5994e-02, -5.1635e-02,\n",
      "        -5.4161e-02,  4.7361e-02,  7.6141e-03,  5.6137e-02,  7.6820e-03,\n",
      "        -8.5838e-03, -6.5227e-02,  2.7048e-03,  2.9102e-04, -5.3184e-02,\n",
      "        -3.6188e-03,  2.1140e-02, -4.6648e-03, -9.0719e-03, -6.3785e-02,\n",
      "         3.3278e-02,  1.9712e-02, -3.8712e-02,  5.6322e-02,  1.3408e-03,\n",
      "         3.5009e-02, -2.8121e-02, -3.0062e-03,  4.5180e-02, -5.0990e-02,\n",
      "         1.2148e-02,  6.5612e-02,  3.2250e-03, -1.1300e-02,  2.9067e-02,\n",
      "         2.5280e-04, -5.7079e-02,  9.5452e-03, -3.6437e-02,  2.9703e-02,\n",
      "         2.0742e-02, -2.1180e-02, -4.3428e-02,  1.5279e-02, -5.5079e-02,\n",
      "         2.1966e-02,  4.4413e-02, -3.8531e-03, -1.9795e-02, -6.3331e-02,\n",
      "        -6.6525e-02,  3.8567e-03,  2.6086e-02,  9.5912e-03,  2.7763e-02,\n",
      "         5.0509e-02,  1.3376e-02, -2.4464e-02, -6.6327e-02, -1.4394e-03,\n",
      "         2.2370e-02, -5.8240e-02,  5.9955e-02,  3.4984e-02, -1.0422e-03,\n",
      "        -3.6942e-02, -6.4322e-02, -2.3425e-02, -6.5759e-02, -1.3139e-02,\n",
      "         1.9723e-03, -5.1438e-02,  9.0022e-03, -3.9169e-02,  6.9434e-03,\n",
      "        -4.8288e-02,  2.1466e-02,  6.6189e-02,  1.6739e-02, -3.3390e-02,\n",
      "         1.0032e-02,  5.4360e-02, -1.9943e-02,  3.9784e-02,  4.8010e-03,\n",
      "         5.9602e-02, -4.0070e-02, -3.9839e-02, -4.4657e-03,  6.3664e-02,\n",
      "         1.7066e-02,  3.1249e-02, -1.5376e-02,  5.0682e-03,  4.0992e-02,\n",
      "        -8.9468e-03,  4.5708e-02, -3.4013e-03,  1.7812e-02,  3.8013e-02,\n",
      "         3.6371e-02, -5.9459e-02,  5.8333e-02,  2.7802e-02,  2.2443e-02,\n",
      "         2.1736e-02,  1.0603e-02, -2.6359e-02,  5.6024e-02, -5.2250e-02,\n",
      "         6.7702e-02, -6.6321e-02,  6.4459e-02,  5.5141e-02,  1.3507e-03,\n",
      "         6.7253e-02,  6.2099e-02, -2.6271e-02,  6.3451e-02, -2.7337e-02,\n",
      "         5.6590e-02, -1.7105e-02, -3.5016e-02,  1.5357e-02,  1.7872e-02,\n",
      "         3.7967e-02, -4.1034e-03, -3.1910e-03, -6.2256e-02,  3.8998e-02]), tensor([[-0.0245,  0.0061,  0.0087,  ...,  0.0436,  0.0573,  0.0554],\n",
      "        [-0.0142, -0.0367,  0.0359,  ...,  0.0375,  0.0156,  0.0343],\n",
      "        [ 0.0644,  0.0512,  0.0433,  ...,  0.0273, -0.0499,  0.0595],\n",
      "        ...,\n",
      "        [ 0.0423, -0.0658, -0.0437,  ...,  0.0005,  0.0594, -0.0197],\n",
      "        [-0.0217, -0.0162,  0.0089,  ..., -0.0061, -0.0132,  0.0355],\n",
      "        [-0.0093,  0.0566,  0.0248,  ...,  0.0552, -0.0491, -0.0547]]), tensor([-0.0401,  0.0316, -0.0430, -0.0366,  0.0003,  0.0004, -0.0666,  0.0290,\n",
      "        -0.0189,  0.0037, -0.0241, -0.0510, -0.0429,  0.0242,  0.0582,  0.0252,\n",
      "        -0.0187,  0.0033, -0.0180,  0.0552, -0.0566,  0.0525,  0.0388,  0.0687,\n",
      "         0.0092,  0.0302,  0.0023,  0.0576,  0.0170,  0.0297, -0.0492, -0.0134,\n",
      "         0.0481,  0.0575, -0.0596, -0.0232,  0.0538,  0.0647,  0.0625, -0.0626,\n",
      "        -0.0076, -0.0056, -0.0299,  0.0330,  0.0280, -0.0175,  0.0105, -0.0314,\n",
      "         0.0052, -0.0627,  0.0112, -0.0595, -0.0177,  0.0121, -0.0651,  0.0575,\n",
      "        -0.0622,  0.0282,  0.0172,  0.0167,  0.0283, -0.0119,  0.0279, -0.0474,\n",
      "         0.0414,  0.0468, -0.0640, -0.0018, -0.0312,  0.0637,  0.0334, -0.0129,\n",
      "        -0.0298,  0.0397,  0.0027,  0.0600,  0.0500, -0.0058, -0.0220, -0.0634,\n",
      "        -0.0535,  0.0209, -0.0274, -0.0518,  0.0101, -0.0231,  0.0564, -0.0465,\n",
      "        -0.0627, -0.0121,  0.0434, -0.0008, -0.0139, -0.0019,  0.0141, -0.0180,\n",
      "        -0.0456,  0.0199, -0.0481,  0.0559]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0015,  0.0884],\n",
      "        [-0.0075, -0.0840, -0.0160,  ...,  0.0452,  0.0600, -0.0552],\n",
      "        [ 0.0740, -0.0701,  0.0597,  ..., -0.0442,  0.0799, -0.0691],\n",
      "        ...,\n",
      "        [-0.0702,  0.0923,  0.0690,  ...,  0.0101, -0.0973, -0.0936],\n",
      "        [-0.0208,  0.0587, -0.0464,  ..., -0.0225,  0.0192,  0.0256],\n",
      "        [-0.0691, -0.0848,  0.0393,  ..., -0.0918, -0.0869,  0.0951]]), tensor([ 0.0395, -0.0277,  0.0078,  0.0703, -0.0455, -0.0156, -0.0704,  0.1107,\n",
      "         0.0826, -0.1005, -0.0408, -0.0914,  0.0722, -0.0342, -0.0599, -0.0179,\n",
      "        -0.0664,  0.0704, -0.0914, -0.0635,  0.0829, -0.0392,  0.0382,  0.0188,\n",
      "        -0.0793,  0.0840,  0.0163,  0.0791,  0.0489, -0.0263,  0.0680, -0.0424,\n",
      "        -0.0561,  0.0637,  0.0006,  0.0382,  0.0030, -0.0590,  0.0526,  0.0107,\n",
      "         0.0241, -0.0108,  0.0473, -0.0571, -0.0181,  0.0394,  0.0452,  0.0175,\n",
      "         0.0738,  0.0164]), tensor([[-0.0076,  0.0300,  0.0957,  0.1266,  0.1028, -0.0273, -0.0195, -0.1644,\n",
      "          0.0431,  0.0909,  0.1049, -0.0155,  0.0004, -0.0253, -0.0910, -0.0676,\n",
      "         -0.0704, -0.0272, -0.1364, -0.1598, -0.0289, -0.0691, -0.1348, -0.0098,\n",
      "          0.1337, -0.0847,  0.0738,  0.0314, -0.0551, -0.1242,  0.0996, -0.1325,\n",
      "          0.0488, -0.0434, -0.1040,  0.1272,  0.0512,  0.0427, -0.0231, -0.0696,\n",
      "         -0.0158,  0.0709,  0.0004,  0.1304, -0.0208,  0.1296, -0.1051,  0.1210,\n",
      "         -0.0028,  0.0768]]), tensor([-0.2470])]\n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.638432502746582 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8963504592374801| flobal_FPR: 1.0 \n",
      "[tensor([[-0.0065,  0.0576,  0.0006,  ..., -0.0476,  0.0517,  0.0565],\n",
      "        [-0.0208, -0.0605,  0.0256,  ..., -0.0115, -0.0480, -0.0197],\n",
      "        [-0.0156, -0.0490, -0.0512,  ...,  0.0349, -0.0551, -0.0088],\n",
      "        ...,\n",
      "        [-0.0346,  0.0328,  0.0307,  ..., -0.0452,  0.0501,  0.0152],\n",
      "        [-0.0257, -0.0444,  0.0537,  ...,  0.0579, -0.0459, -0.0312],\n",
      "        [ 0.0167,  0.0148, -0.0180,  ..., -0.0034, -0.0451, -0.0608]]), tensor([ 1.2773e-02,  6.6709e-02,  1.7534e-02,  4.0281e-02,  3.5410e-02,\n",
      "        -1.5159e-02, -4.1926e-02,  4.2039e-02, -5.4396e-02,  1.7297e-02,\n",
      "        -5.3717e-02,  5.7733e-02, -5.3818e-02,  5.3050e-02,  3.2572e-02,\n",
      "        -4.3610e-02, -3.7940e-02,  2.2489e-02,  6.2943e-02, -5.4977e-02,\n",
      "         4.5029e-02, -2.9453e-02,  6.1913e-02, -6.4015e-02,  5.5253e-02,\n",
      "        -2.4498e-02, -2.2278e-03,  3.7424e-02,  4.9820e-02, -5.6650e-02,\n",
      "        -4.2891e-02, -4.5369e-02, -6.8148e-02, -2.7837e-02, -5.0838e-02,\n",
      "         6.0744e-03,  3.2914e-02,  1.8170e-02,  2.0105e-02, -1.2584e-02,\n",
      "         5.9662e-02, -1.6937e-02,  4.7404e-02, -5.5539e-04,  5.4301e-03,\n",
      "         5.4660e-02, -1.1386e-02, -3.3314e-02,  9.1395e-03,  3.5060e-03,\n",
      "        -6.2760e-02,  2.2380e-02,  6.2405e-02, -1.7341e-03, -3.8981e-02,\n",
      "        -5.1966e-02, -6.2009e-02,  2.2632e-02,  6.3956e-02, -2.4293e-02,\n",
      "         2.0956e-05, -3.0164e-02,  3.1897e-02, -6.7223e-02,  4.0985e-02,\n",
      "         1.8045e-02, -3.6329e-02,  3.3907e-02, -6.0831e-02, -1.4394e-02,\n",
      "        -1.4573e-02,  4.3436e-02,  1.8607e-02,  3.5927e-02, -5.1675e-02,\n",
      "        -5.4111e-02,  4.7765e-02,  7.6013e-03,  5.6223e-02,  7.5607e-03,\n",
      "        -8.4332e-03, -6.5326e-02,  2.8057e-03,  2.2947e-05, -5.3026e-02,\n",
      "        -3.5473e-03,  2.1090e-02, -4.4839e-03, -8.9132e-03, -6.3724e-02,\n",
      "         3.3168e-02,  1.9938e-02, -3.8512e-02,  5.6335e-02,  1.1649e-03,\n",
      "         3.5127e-02, -2.8016e-02, -2.7282e-03,  4.5148e-02, -5.0865e-02,\n",
      "         1.1998e-02,  6.5804e-02,  3.2766e-03, -1.1216e-02,  2.9134e-02,\n",
      "         4.9420e-04, -5.6756e-02,  9.5455e-03, -3.6581e-02,  2.9952e-02,\n",
      "         2.0639e-02, -2.1414e-02, -4.3331e-02,  1.5168e-02, -5.5184e-02,\n",
      "         2.2028e-02,  4.4171e-02, -3.9569e-03, -2.0338e-02, -6.3238e-02,\n",
      "        -6.6710e-02,  3.8944e-03,  2.5853e-02,  9.5436e-03,  2.7910e-02,\n",
      "         5.0825e-02,  1.3359e-02, -2.4414e-02, -6.6356e-02, -1.2755e-03,\n",
      "         2.2255e-02, -5.8308e-02,  6.0062e-02,  3.4888e-02, -1.0345e-03,\n",
      "        -3.7189e-02, -6.4129e-02, -2.3091e-02, -6.5900e-02, -1.3209e-02,\n",
      "         1.8712e-03, -5.1324e-02,  8.9264e-03, -3.9204e-02,  6.8789e-03,\n",
      "        -4.8130e-02,  2.2222e-02,  6.6278e-02,  1.6734e-02, -3.3480e-02,\n",
      "         9.8861e-03,  5.4206e-02, -1.9965e-02,  3.9971e-02,  4.7728e-03,\n",
      "         5.9617e-02, -3.9920e-02, -3.9898e-02, -4.7587e-03,  6.3941e-02,\n",
      "         1.7042e-02,  3.1183e-02, -1.5212e-02,  5.4272e-03,  4.1063e-02,\n",
      "        -8.7040e-03,  4.5839e-02, -3.1519e-03,  1.8121e-02,  3.8206e-02,\n",
      "         3.6437e-02, -6.0053e-02,  5.8117e-02,  2.7594e-02,  2.2349e-02,\n",
      "         2.1631e-02,  1.0995e-02, -2.6432e-02,  5.6402e-02, -5.2359e-02,\n",
      "         6.7923e-02, -6.6493e-02,  6.4586e-02,  5.5242e-02,  1.4384e-03,\n",
      "         6.7695e-02,  6.2236e-02, -2.6489e-02,  6.3574e-02, -2.7345e-02,\n",
      "         5.6832e-02, -1.7278e-02, -3.5177e-02,  1.5265e-02,  1.7696e-02,\n",
      "         3.8699e-02, -3.9873e-03, -3.0302e-03, -6.2163e-02,  3.9000e-02]), tensor([[-0.0245,  0.0061,  0.0086,  ...,  0.0437,  0.0574,  0.0554],\n",
      "        [-0.0140, -0.0368,  0.0357,  ...,  0.0377,  0.0155,  0.0340],\n",
      "        [ 0.0645,  0.0511,  0.0433,  ...,  0.0277, -0.0497,  0.0595],\n",
      "        ...,\n",
      "        [ 0.0424, -0.0658, -0.0434,  ...,  0.0006,  0.0594, -0.0196],\n",
      "        [-0.0218, -0.0164,  0.0087,  ..., -0.0058, -0.0132,  0.0354],\n",
      "        [-0.0094,  0.0565,  0.0247,  ...,  0.0552, -0.0490, -0.0548]]), tensor([-0.0398,  0.0319, -0.0420, -0.0366,  0.0005,  0.0007, -0.0670,  0.0299,\n",
      "        -0.0196,  0.0040, -0.0237, -0.0506, -0.0416,  0.0241,  0.0597,  0.0251,\n",
      "        -0.0189,  0.0033, -0.0173,  0.0559, -0.0563,  0.0511,  0.0389,  0.0686,\n",
      "         0.0088,  0.0297,  0.0023,  0.0573,  0.0173,  0.0299, -0.0491, -0.0130,\n",
      "         0.0488,  0.0578, -0.0597, -0.0228,  0.0540,  0.0658,  0.0615, -0.0629,\n",
      "        -0.0057, -0.0045, -0.0299,  0.0331,  0.0276, -0.0179,  0.0114, -0.0311,\n",
      "         0.0049, -0.0634,  0.0119, -0.0594, -0.0177,  0.0135, -0.0650,  0.0576,\n",
      "        -0.0623,  0.0287,  0.0174,  0.0159,  0.0282, -0.0121,  0.0274, -0.0470,\n",
      "         0.0423,  0.0468, -0.0643, -0.0017, -0.0308,  0.0636,  0.0331, -0.0117,\n",
      "        -0.0299,  0.0413,  0.0028,  0.0598,  0.0508, -0.0050, -0.0219, -0.0632,\n",
      "        -0.0533,  0.0207, -0.0272, -0.0518,  0.0095, -0.0233,  0.0562, -0.0463,\n",
      "        -0.0625, -0.0113,  0.0431, -0.0007, -0.0141, -0.0019,  0.0136, -0.0177,\n",
      "        -0.0456,  0.0202, -0.0477,  0.0555]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0015,  0.0884],\n",
      "        [-0.0075, -0.0839, -0.0160,  ...,  0.0452,  0.0601, -0.0552],\n",
      "        [ 0.0738, -0.0704,  0.0593,  ..., -0.0439,  0.0796, -0.0692],\n",
      "        ...,\n",
      "        [-0.0704,  0.0929,  0.0693,  ...,  0.0104, -0.0973, -0.0935],\n",
      "        [-0.0208,  0.0588, -0.0464,  ..., -0.0225,  0.0192,  0.0256],\n",
      "        [-0.0692, -0.0852,  0.0388,  ..., -0.0918, -0.0870,  0.0947]]), tensor([ 3.9470e-02, -2.7241e-02,  8.9693e-03,  7.0538e-02, -4.4865e-02,\n",
      "        -1.5423e-02, -6.9604e-02,  1.1529e-01,  8.1199e-02, -1.0096e-01,\n",
      "        -4.0854e-02, -9.1402e-02,  7.2153e-02, -3.3710e-02, -6.0853e-02,\n",
      "        -1.4961e-02, -6.5178e-02,  7.1124e-02, -9.1741e-02, -6.2331e-02,\n",
      "         8.3638e-02, -3.6568e-02,  4.3236e-02,  1.8849e-02, -7.9582e-02,\n",
      "         8.6424e-02,  1.5878e-02,  7.8698e-02,  5.0815e-02, -2.5556e-02,\n",
      "         7.0267e-02, -3.8141e-02, -5.6545e-02,  6.4738e-02,  1.1103e-04,\n",
      "         3.7464e-02,  3.5558e-03, -5.9957e-02,  5.3203e-02,  1.2942e-02,\n",
      "         2.4566e-02, -1.1333e-02,  4.7295e-02, -5.7271e-02, -1.8620e-02,\n",
      "         3.9501e-02,  4.6246e-02,  1.7239e-02,  7.3851e-02,  1.5875e-02]), tensor([[-7.2173e-03,  3.0856e-02,  9.7902e-02,  1.3149e-01,  1.0268e-01,\n",
      "         -2.7528e-02, -2.2581e-02, -1.7629e-01,  4.1918e-02,  9.0572e-02,\n",
      "          1.0484e-01, -1.5583e-02, -9.7951e-05, -2.5294e-02, -9.0268e-02,\n",
      "         -7.0152e-02, -7.9620e-02, -3.3113e-02, -1.3641e-01, -1.6770e-01,\n",
      "         -3.0547e-02, -7.5062e-02, -1.4083e-01, -9.5137e-03,  1.3365e-01,\n",
      "         -8.8411e-02,  7.5106e-02,  3.1620e-02, -6.0520e-02, -1.2942e-01,\n",
      "          1.0055e-01, -1.3578e-01,  4.7969e-02, -4.7466e-02, -1.0392e-01,\n",
      "          1.2711e-01,  5.2486e-02,  4.1892e-02, -2.6238e-02, -7.2486e-02,\n",
      "         -1.8705e-02,  7.1921e-02,  5.0558e-04,  1.3033e-01, -1.9314e-02,\n",
      "          1.3013e-01, -1.0573e-01,  1.2215e-01, -2.8111e-03,  7.6885e-02]]), tensor([-0.2744])]\n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6439300179481506 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9123543263005397| flobal_FPR: 1.0 \n",
      "[tensor([[-0.0064,  0.0577,  0.0007,  ..., -0.0474,  0.0522,  0.0571],\n",
      "        [-0.0208, -0.0605,  0.0256,  ..., -0.0115, -0.0481, -0.0197],\n",
      "        [-0.0155, -0.0490, -0.0512,  ...,  0.0350, -0.0549, -0.0087],\n",
      "        ...,\n",
      "        [-0.0347,  0.0327,  0.0306,  ..., -0.0452,  0.0499,  0.0149],\n",
      "        [-0.0257, -0.0444,  0.0537,  ...,  0.0579, -0.0459, -0.0312],\n",
      "        [ 0.0167,  0.0148, -0.0180,  ..., -0.0034, -0.0452, -0.0610]]), tensor([ 0.0136,  0.0666,  0.0178,  0.0396,  0.0355, -0.0148, -0.0420,  0.0420,\n",
      "        -0.0540,  0.0170, -0.0538,  0.0579, -0.0548,  0.0520,  0.0327, -0.0441,\n",
      "        -0.0389,  0.0223,  0.0631, -0.0555,  0.0449, -0.0296,  0.0624, -0.0644,\n",
      "         0.0551, -0.0249, -0.0024,  0.0375,  0.0501, -0.0567, -0.0423, -0.0457,\n",
      "        -0.0681, -0.0281, -0.0517,  0.0071,  0.0326,  0.0184,  0.0206, -0.0127,\n",
      "         0.0599, -0.0171,  0.0471, -0.0007,  0.0053,  0.0554, -0.0121, -0.0335,\n",
      "         0.0094,  0.0037, -0.0629,  0.0223,  0.0622, -0.0018, -0.0390, -0.0522,\n",
      "        -0.0616,  0.0229,  0.0631, -0.0247,  0.0002, -0.0304,  0.0312, -0.0681,\n",
      "         0.0400,  0.0182, -0.0361,  0.0339, -0.0610, -0.0141, -0.0146,  0.0435,\n",
      "         0.0175,  0.0359, -0.0518, -0.0545,  0.0473,  0.0069,  0.0564,  0.0075,\n",
      "        -0.0087, -0.0655,  0.0026, -0.0007, -0.0527, -0.0038,  0.0211, -0.0042,\n",
      "        -0.0087, -0.0643,  0.0332,  0.0197, -0.0389,  0.0563,  0.0012,  0.0352,\n",
      "        -0.0277, -0.0023,  0.0451, -0.0511,  0.0112,  0.0664,  0.0031, -0.0112,\n",
      "         0.0293,  0.0012, -0.0556,  0.0094, -0.0366,  0.0296,  0.0203, -0.0219,\n",
      "        -0.0435,  0.0150, -0.0555,  0.0220,  0.0442, -0.0038, -0.0207, -0.0632,\n",
      "        -0.0672,  0.0036,  0.0258,  0.0091,  0.0280,  0.0507,  0.0131, -0.0243,\n",
      "        -0.0663, -0.0010,  0.0222, -0.0584,  0.0600,  0.0344, -0.0010, -0.0373,\n",
      "        -0.0639, -0.0231, -0.0663, -0.0133,  0.0016, -0.0509,  0.0089, -0.0394,\n",
      "         0.0069, -0.0491,  0.0213,  0.0662,  0.0165, -0.0340,  0.0097,  0.0542,\n",
      "        -0.0198,  0.0398,  0.0047,  0.0593, -0.0398, -0.0400, -0.0053,  0.0643,\n",
      "         0.0169,  0.0310, -0.0152,  0.0062,  0.0409, -0.0082,  0.0464, -0.0028,\n",
      "         0.0178,  0.0379,  0.0366, -0.0603,  0.0581,  0.0278,  0.0221,  0.0217,\n",
      "         0.0121, -0.0265,  0.0576, -0.0524,  0.0685, -0.0670,  0.0649,  0.0552,\n",
      "         0.0012,  0.0679,  0.0624, -0.0274,  0.0633, -0.0273,  0.0569, -0.0177,\n",
      "        -0.0351,  0.0151,  0.0179,  0.0382, -0.0041, -0.0034, -0.0622,  0.0388]), tensor([[-0.0248,  0.0061,  0.0086,  ...,  0.0435,  0.0574,  0.0554],\n",
      "        [-0.0136, -0.0368,  0.0359,  ...,  0.0379,  0.0155,  0.0341],\n",
      "        [ 0.0634,  0.0511,  0.0437,  ...,  0.0270, -0.0498,  0.0593],\n",
      "        ...,\n",
      "        [ 0.0428, -0.0657, -0.0437,  ...,  0.0008,  0.0594, -0.0195],\n",
      "        [-0.0227, -0.0164,  0.0094,  ..., -0.0062, -0.0132,  0.0351],\n",
      "        [-0.0104,  0.0565,  0.0247,  ...,  0.0547, -0.0490, -0.0549]]), tensor([-0.0407,  0.0335, -0.0429, -0.0366,  0.0002, -0.0002, -0.0672,  0.0315,\n",
      "        -0.0208,  0.0036, -0.0231, -0.0517, -0.0426,  0.0235,  0.0613,  0.0248,\n",
      "        -0.0189,  0.0033, -0.0178,  0.0554, -0.0560,  0.0421,  0.0390,  0.0678,\n",
      "         0.0084,  0.0295,  0.0025,  0.0570,  0.0175,  0.0308, -0.0487, -0.0115,\n",
      "         0.0507,  0.0566, -0.0606, -0.0220,  0.0547,  0.0629,  0.0584, -0.0633,\n",
      "        -0.0073, -0.0064, -0.0292,  0.0330,  0.0272, -0.0187,  0.0124, -0.0319,\n",
      "         0.0036, -0.0634,  0.0112, -0.0601, -0.0176,  0.0134, -0.0650,  0.0568,\n",
      "        -0.0622,  0.0267,  0.0167,  0.0141,  0.0286, -0.0129,  0.0251, -0.0462,\n",
      "         0.0426,  0.0469, -0.0641, -0.0026, -0.0325,  0.0637,  0.0322, -0.0136,\n",
      "        -0.0295,  0.0420,  0.0042,  0.0596,  0.0506, -0.0026, -0.0214, -0.0636,\n",
      "        -0.0541,  0.0203, -0.0273, -0.0521,  0.0081, -0.0231,  0.0564, -0.0457,\n",
      "        -0.0621, -0.0099,  0.0431, -0.0008, -0.0143, -0.0020,  0.0142, -0.0179,\n",
      "        -0.0455,  0.0202, -0.0468,  0.0531]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0015,  0.0884],\n",
      "        [-0.0075, -0.0839, -0.0158,  ...,  0.0452,  0.0600, -0.0552],\n",
      "        [ 0.0737, -0.0710,  0.0591,  ..., -0.0440,  0.0789, -0.0693],\n",
      "        ...,\n",
      "        [-0.0699,  0.0981,  0.0733,  ...,  0.0120, -0.0968, -0.0925],\n",
      "        [-0.0208,  0.0587, -0.0464,  ..., -0.0225,  0.0193,  0.0256],\n",
      "        [-0.0692, -0.0851,  0.0390,  ..., -0.0917, -0.0870,  0.0947]]), tensor([ 0.0396, -0.0278,  0.0048,  0.0663, -0.0444, -0.0151, -0.0693,  0.1199,\n",
      "         0.0801, -0.1010, -0.0409, -0.0915,  0.0721, -0.0329, -0.0636, -0.0143,\n",
      "        -0.0733,  0.0719, -0.0917, -0.0752,  0.0844, -0.0349,  0.0476,  0.0189,\n",
      "        -0.0796,  0.0892,  0.0148,  0.0784,  0.0519, -0.0276,  0.0698, -0.0354,\n",
      "        -0.0558,  0.0646, -0.0003,  0.0374,  0.0011, -0.0600,  0.0538,  0.0152,\n",
      "         0.0249, -0.0161,  0.0472, -0.0575, -0.0188,  0.0374,  0.0476,  0.0244,\n",
      "         0.0739,  0.0156]), tensor([[-0.0075,  0.0304,  0.0970,  0.1307,  0.1032, -0.0276, -0.0188, -0.1689,\n",
      "          0.0478,  0.0906,  0.1048, -0.0155,  0.0015, -0.0290, -0.0891, -0.0702,\n",
      "         -0.0744, -0.0290, -0.1364, -0.1572, -0.0262, -0.0655, -0.1414, -0.0088,\n",
      "          0.1336, -0.0926,  0.0749,  0.0347, -0.0515, -0.1199,  0.1005, -0.1334,\n",
      "          0.0520, -0.0419, -0.1038,  0.1271,  0.0510,  0.0419, -0.0211, -0.0744,\n",
      "         -0.0067,  0.0703,  0.0028,  0.1303, -0.0192,  0.1302, -0.1061,  0.1285,\n",
      "         -0.0014,  0.0769]]), tensor([-0.3014])]\n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6421428918838501 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.920338216070249| flobal_FPR: 1.0 \n",
      "[tensor([[-0.0065,  0.0576,  0.0007,  ..., -0.0474,  0.0523,  0.0573],\n",
      "        [-0.0207, -0.0604,  0.0257,  ..., -0.0115, -0.0481, -0.0198],\n",
      "        [-0.0156, -0.0490, -0.0512,  ...,  0.0351, -0.0549, -0.0086],\n",
      "        ...,\n",
      "        [-0.0346,  0.0328,  0.0307,  ..., -0.0452,  0.0498,  0.0148],\n",
      "        [-0.0257, -0.0444,  0.0537,  ...,  0.0578, -0.0459, -0.0313],\n",
      "        [ 0.0168,  0.0149, -0.0179,  ..., -0.0035, -0.0453, -0.0611]]), tensor([ 0.0139,  0.0665,  0.0180,  0.0396,  0.0357, -0.0146, -0.0420,  0.0419,\n",
      "        -0.0537,  0.0169, -0.0538,  0.0580, -0.0550,  0.0518,  0.0325, -0.0441,\n",
      "        -0.0390,  0.0223,  0.0632, -0.0557,  0.0449, -0.0296,  0.0628, -0.0645,\n",
      "         0.0551, -0.0249, -0.0024,  0.0376,  0.0503, -0.0568, -0.0419, -0.0458,\n",
      "        -0.0682, -0.0282, -0.0517,  0.0079,  0.0325,  0.0185,  0.0208, -0.0127,\n",
      "         0.0601, -0.0170,  0.0470, -0.0006,  0.0053,  0.0559, -0.0122, -0.0338,\n",
      "         0.0095,  0.0037, -0.0630,  0.0221,  0.0622, -0.0018, -0.0391, -0.0524,\n",
      "        -0.0615,  0.0232,  0.0629, -0.0248,  0.0003, -0.0304,  0.0314, -0.0682,\n",
      "         0.0397,  0.0183, -0.0359,  0.0337, -0.0610, -0.0140, -0.0147,  0.0436,\n",
      "         0.0174,  0.0358, -0.0519, -0.0545,  0.0472,  0.0067,  0.0565,  0.0075,\n",
      "        -0.0087, -0.0656,  0.0025, -0.0009, -0.0526, -0.0038,  0.0212, -0.0041,\n",
      "        -0.0086, -0.0645,  0.0332,  0.0198, -0.0389,  0.0564,  0.0011,  0.0353,\n",
      "        -0.0275, -0.0022,  0.0452, -0.0511,  0.0108,  0.0667,  0.0031, -0.0111,\n",
      "         0.0293,  0.0015, -0.0551,  0.0093, -0.0366,  0.0297,  0.0202, -0.0222,\n",
      "        -0.0436,  0.0150, -0.0555,  0.0220,  0.0440, -0.0037, -0.0210, -0.0632,\n",
      "        -0.0674,  0.0037,  0.0257,  0.0089,  0.0280,  0.0509,  0.0130, -0.0242,\n",
      "        -0.0663, -0.0007,  0.0222, -0.0584,  0.0600,  0.0343, -0.0008, -0.0375,\n",
      "        -0.0637, -0.0229, -0.0664, -0.0134,  0.0015, -0.0509,  0.0088, -0.0396,\n",
      "         0.0068, -0.0492,  0.0212,  0.0664,  0.0164, -0.0342,  0.0097,  0.0540,\n",
      "        -0.0196,  0.0399,  0.0047,  0.0590, -0.0398, -0.0400, -0.0054,  0.0646,\n",
      "         0.0168,  0.0309, -0.0150,  0.0065,  0.0408, -0.0081,  0.0465, -0.0025,\n",
      "         0.0178,  0.0379,  0.0366, -0.0605,  0.0580,  0.0277,  0.0220,  0.0216,\n",
      "         0.0124, -0.0266,  0.0581, -0.0526,  0.0688, -0.0672,  0.0651,  0.0552,\n",
      "         0.0013,  0.0681,  0.0624, -0.0277,  0.0633, -0.0272,  0.0570, -0.0180,\n",
      "        -0.0351,  0.0150,  0.0180,  0.0383, -0.0041, -0.0035, -0.0621,  0.0386]), tensor([[-0.0249,  0.0061,  0.0086,  ...,  0.0435,  0.0575,  0.0554],\n",
      "        [-0.0134, -0.0368,  0.0358,  ...,  0.0380,  0.0155,  0.0341],\n",
      "        [ 0.0632,  0.0511,  0.0438,  ...,  0.0270, -0.0497,  0.0592],\n",
      "        ...,\n",
      "        [ 0.0429, -0.0657, -0.0438,  ...,  0.0010,  0.0594, -0.0194],\n",
      "        [-0.0230, -0.0165,  0.0096,  ..., -0.0063, -0.0132,  0.0348],\n",
      "        [-0.0106,  0.0565,  0.0247,  ...,  0.0546, -0.0490, -0.0550]]), tensor([-4.0767e-02,  3.3861e-02, -4.2513e-02, -3.6611e-02,  1.8690e-04,\n",
      "        -5.0940e-05, -6.7254e-02,  3.2169e-02, -2.1430e-02,  3.4805e-03,\n",
      "        -2.2585e-02, -5.1831e-02, -4.1964e-02,  2.3522e-02,  6.2719e-02,\n",
      "         2.4613e-02, -1.8939e-02,  3.2844e-03, -1.7909e-02,  5.5501e-02,\n",
      "        -5.5604e-02,  3.9308e-02,  3.9051e-02,  6.7623e-02,  8.3002e-03,\n",
      "         2.9373e-02,  2.5487e-03,  5.6976e-02,  1.7709e-02,  3.0814e-02,\n",
      "        -4.8697e-02, -1.1074e-02,  5.1444e-02,  5.6415e-02, -6.0471e-02,\n",
      "        -2.1291e-02,  5.4864e-02,  6.2677e-02,  5.6885e-02, -6.3421e-02,\n",
      "        -7.6599e-03, -6.4698e-03, -2.8960e-02,  3.2979e-02,  2.6935e-02,\n",
      "        -1.9195e-02,  1.2973e-02, -3.2256e-02,  3.5152e-03, -6.3490e-02,\n",
      "         1.1460e-02, -6.0157e-02, -1.7755e-02,  1.4155e-02, -6.4966e-02,\n",
      "         5.6666e-02, -6.2294e-02,  2.6399e-02,  1.6762e-02,  1.3439e-02,\n",
      "         2.8611e-02, -1.2968e-02,  2.3838e-02, -4.5511e-02,  4.2807e-02,\n",
      "         4.6927e-02, -6.4217e-02, -2.7143e-03, -3.2889e-02,  6.3505e-02,\n",
      "         3.1795e-02, -1.3689e-02, -2.9321e-02,  4.3000e-02,  4.4775e-03,\n",
      "         5.9514e-02,  5.1026e-02, -1.8553e-03, -2.1191e-02, -6.3530e-02,\n",
      "        -5.4067e-02,  2.0075e-02, -2.7624e-02, -5.2293e-02,  7.1125e-03,\n",
      "        -2.3463e-02,  5.6337e-02, -4.5624e-02, -6.1667e-02, -8.3636e-03,\n",
      "         4.2997e-02, -7.7129e-04, -1.4472e-02, -1.9623e-03,  1.4529e-02,\n",
      "        -1.7872e-02, -4.5599e-02,  1.9768e-02, -4.6235e-02,  5.2625e-02]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0015,  0.0884],\n",
      "        [-0.0075, -0.0839, -0.0158,  ...,  0.0452,  0.0599, -0.0552],\n",
      "        [ 0.0737, -0.0712,  0.0591,  ..., -0.0438,  0.0787, -0.0694],\n",
      "        ...,\n",
      "        [-0.0699,  0.0996,  0.0745,  ...,  0.0121, -0.0967, -0.0924],\n",
      "        [-0.0208,  0.0587, -0.0464,  ..., -0.0225,  0.0193,  0.0256],\n",
      "        [-0.0692, -0.0852,  0.0389,  ..., -0.0917, -0.0870,  0.0946]]), tensor([ 0.0396, -0.0281,  0.0049,  0.0642, -0.0441, -0.0149, -0.0689,  0.1234,\n",
      "         0.0791, -0.1011, -0.0408, -0.0915,  0.0721, -0.0323, -0.0650, -0.0128,\n",
      "        -0.0742,  0.0725, -0.0919, -0.0771,  0.0849, -0.0333,  0.0515,  0.0191,\n",
      "        -0.0797,  0.0913,  0.0150,  0.0785,  0.0531, -0.0278,  0.0709, -0.0314,\n",
      "        -0.0563,  0.0647, -0.0013,  0.0373,  0.0005, -0.0605,  0.0543,  0.0170,\n",
      "         0.0250, -0.0166,  0.0472, -0.0575, -0.0191,  0.0378,  0.0483,  0.0249,\n",
      "         0.0739,  0.0154]), tensor([[-0.0073,  0.0304,  0.0978,  0.1321,  0.1035, -0.0277, -0.0190, -0.1716,\n",
      "          0.0480,  0.0905,  0.1049, -0.0155,  0.0010, -0.0312, -0.0884, -0.0712,\n",
      "         -0.0766, -0.0298, -0.1364, -0.1588, -0.0270, -0.0658, -0.1448, -0.0095,\n",
      "          0.1336, -0.0956,  0.0756,  0.0360, -0.0529, -0.1196,  0.1009, -0.1349,\n",
      "          0.0526, -0.0423, -0.1035,  0.1271,  0.0509,  0.0417, -0.0222, -0.0772,\n",
      "         -0.0055,  0.0706,  0.0020,  0.1303, -0.0187,  0.1304, -0.1064,  0.1319,\n",
      "         -0.0009,  0.0770]]), tensor([-0.3219])]\n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6329511404037476 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9317314271523477| flobal_FPR: 1.0 \n",
      "[tensor([[-0.0069,  0.0572,  0.0003,  ..., -0.0474,  0.0524,  0.0574],\n",
      "        [-0.0203, -0.0600,  0.0261,  ..., -0.0115, -0.0481, -0.0198],\n",
      "        [-0.0158, -0.0492, -0.0514,  ...,  0.0351, -0.0549, -0.0086],\n",
      "        ...,\n",
      "        [-0.0344,  0.0330,  0.0310,  ..., -0.0453,  0.0497,  0.0148],\n",
      "        [-0.0256, -0.0443,  0.0538,  ...,  0.0578, -0.0459, -0.0313],\n",
      "        [ 0.0171,  0.0153, -0.0175,  ..., -0.0035, -0.0453, -0.0612]]), tensor([ 0.0139,  0.0663,  0.0181,  0.0402,  0.0356, -0.0145, -0.0416,  0.0419,\n",
      "        -0.0535,  0.0170, -0.0536,  0.0579, -0.0547,  0.0522,  0.0323, -0.0436,\n",
      "        -0.0388,  0.0226,  0.0634, -0.0557,  0.0450, -0.0296,  0.0632, -0.0645,\n",
      "         0.0550, -0.0248, -0.0023,  0.0378,  0.0503, -0.0570, -0.0419, -0.0456,\n",
      "        -0.0683, -0.0283, -0.0514,  0.0085,  0.0327,  0.0184,  0.0209, -0.0125,\n",
      "         0.0604, -0.0169,  0.0471, -0.0004,  0.0052,  0.0560, -0.0119, -0.0339,\n",
      "         0.0096,  0.0036, -0.0631,  0.0220,  0.0625, -0.0019, -0.0391, -0.0527,\n",
      "        -0.0614,  0.0235,  0.0632, -0.0244,  0.0003, -0.0303,  0.0321, -0.0677,\n",
      "         0.0400,  0.0184, -0.0359,  0.0336, -0.0609, -0.0141, -0.0150,  0.0438,\n",
      "         0.0179,  0.0357, -0.0519, -0.0544,  0.0476,  0.0069,  0.0566,  0.0074,\n",
      "        -0.0084, -0.0657,  0.0027, -0.0009, -0.0526, -0.0036,  0.0211, -0.0040,\n",
      "        -0.0085, -0.0643,  0.0331,  0.0200, -0.0387,  0.0564,  0.0009,  0.0354,\n",
      "        -0.0274, -0.0022,  0.0453, -0.0509,  0.0107,  0.0668,  0.0033, -0.0109,\n",
      "         0.0293,  0.0016, -0.0550,  0.0093, -0.0366,  0.0301,  0.0202, -0.0223,\n",
      "        -0.0435,  0.0150, -0.0554,  0.0221,  0.0437, -0.0039, -0.0215, -0.0631,\n",
      "        -0.0674,  0.0038,  0.0256,  0.0090,  0.0281,  0.0513,  0.0130, -0.0242,\n",
      "        -0.0663, -0.0005,  0.0221, -0.0584,  0.0600,  0.0344, -0.0008, -0.0377,\n",
      "        -0.0635, -0.0225, -0.0664, -0.0135,  0.0015, -0.0509,  0.0087, -0.0396,\n",
      "         0.0068, -0.0490,  0.0220,  0.0667,  0.0164, -0.0341,  0.0096,  0.0538,\n",
      "        -0.0197,  0.0403,  0.0046,  0.0590, -0.0398, -0.0400, -0.0055,  0.0649,\n",
      "         0.0169,  0.0308, -0.0149,  0.0068,  0.0409, -0.0080,  0.0465, -0.0025,\n",
      "         0.0183,  0.0383,  0.0365, -0.0610,  0.0577,  0.0275,  0.0220,  0.0215,\n",
      "         0.0125, -0.0266,  0.0582, -0.0528,  0.0687, -0.0673,  0.0651,  0.0553,\n",
      "         0.0014,  0.0684,  0.0624, -0.0277,  0.0635, -0.0272,  0.0572, -0.0180,\n",
      "        -0.0351,  0.0149,  0.0179,  0.0390, -0.0040, -0.0033, -0.0619,  0.0387]), tensor([[-0.0248,  0.0062,  0.0086,  ...,  0.0437,  0.0576,  0.0555],\n",
      "        [-0.0134, -0.0369,  0.0354,  ...,  0.0379,  0.0153,  0.0337],\n",
      "        [ 0.0635,  0.0511,  0.0438,  ...,  0.0275, -0.0495,  0.0593],\n",
      "        ...,\n",
      "        [ 0.0430, -0.0655, -0.0433,  ...,  0.0010,  0.0594, -0.0191],\n",
      "        [-0.0229, -0.0167,  0.0094,  ..., -0.0060, -0.0131,  0.0349],\n",
      "        [-0.0106,  0.0565,  0.0247,  ...,  0.0547, -0.0489, -0.0550]]), tensor([-0.0401,  0.0332, -0.0412, -0.0366,  0.0005,  0.0003, -0.0673,  0.0324,\n",
      "        -0.0218,  0.0039, -0.0222, -0.0511, -0.0400,  0.0237,  0.0642,  0.0246,\n",
      "        -0.0192,  0.0033, -0.0168,  0.0565, -0.0553,  0.0407,  0.0392,  0.0678,\n",
      "         0.0081,  0.0290,  0.0026,  0.0567,  0.0179,  0.0305, -0.0486, -0.0113,\n",
      "         0.0514,  0.0571, -0.0601, -0.0211,  0.0549,  0.0643,  0.0566, -0.0635,\n",
      "        -0.0058, -0.0053, -0.0290,  0.0331,  0.0268, -0.0194,  0.0135, -0.0319,\n",
      "         0.0039, -0.0639,  0.0124, -0.0597, -0.0179,  0.0158, -0.0649,  0.0571,\n",
      "        -0.0624,  0.0276,  0.0176,  0.0133,  0.0283, -0.0128,  0.0239, -0.0452,\n",
      "         0.0435,  0.0467, -0.0647, -0.0026, -0.0320,  0.0634,  0.0317, -0.0121,\n",
      "        -0.0296,  0.0445,  0.0039,  0.0594,  0.0518, -0.0019, -0.0212, -0.0631,\n",
      "        -0.0535,  0.0199, -0.0276, -0.0523,  0.0068, -0.0240,  0.0563, -0.0457,\n",
      "        -0.0614, -0.0072,  0.0427, -0.0006, -0.0145, -0.0020,  0.0141, -0.0173,\n",
      "        -0.0458,  0.0204, -0.0455,  0.0530]), tensor([[ 0.0656, -0.0876, -0.0197,  ..., -0.0757, -0.0015,  0.0884],\n",
      "        [-0.0075, -0.0839, -0.0157,  ...,  0.0453,  0.0599, -0.0552],\n",
      "        [ 0.0737, -0.0709,  0.0589,  ..., -0.0434,  0.0786, -0.0693],\n",
      "        ...,\n",
      "        [-0.0705,  0.0983,  0.0734,  ...,  0.0118, -0.0970, -0.0928],\n",
      "        [-0.0208,  0.0587, -0.0464,  ..., -0.0225,  0.0193,  0.0256],\n",
      "        [-0.0692, -0.0853,  0.0387,  ..., -0.0915, -0.0870,  0.0945]]), tensor([ 0.0395, -0.0277,  0.0083,  0.0659, -0.0441, -0.0146, -0.0680,  0.1261,\n",
      "         0.0780, -0.1017, -0.0409, -0.0915,  0.0721, -0.0320, -0.0655, -0.0097,\n",
      "        -0.0699,  0.0730, -0.0922, -0.0720,  0.0853, -0.0314,  0.0557,  0.0191,\n",
      "        -0.0798,  0.0926,  0.0158,  0.0788,  0.0546, -0.0269,  0.0740, -0.0266,\n",
      "        -0.0573,  0.0657, -0.0025,  0.0369,  0.0015, -0.0611,  0.0547,  0.0186,\n",
      "         0.0251, -0.0153,  0.0472, -0.0576, -0.0195,  0.0390,  0.0491,  0.0212,\n",
      "         0.0739,  0.0162]), tensor([[-6.8773e-03,  3.1377e-02,  1.0074e-01,  1.3872e-01,  1.0330e-01,\n",
      "         -2.8009e-02, -2.3168e-02, -1.8530e-01,  4.6064e-02,  9.0209e-02,\n",
      "          1.0482e-01, -1.5535e-02,  3.9598e-05, -3.0611e-02, -8.7980e-02,\n",
      "         -7.3935e-02, -8.6124e-02, -3.5932e-02, -1.3638e-01, -1.6800e-01,\n",
      "         -2.8828e-02, -7.5011e-02, -1.5246e-01, -9.7038e-03,  1.3358e-01,\n",
      "         -9.8307e-02,  7.7901e-02,  3.6761e-02, -6.0510e-02, -1.2700e-01,\n",
      "          1.0242e-01, -1.3942e-01,  5.0749e-02, -4.7498e-02, -1.0344e-01,\n",
      "          1.2713e-01,  5.2482e-02,  4.1135e-02, -2.6540e-02, -8.1187e-02,\n",
      "         -1.1290e-02,  7.2191e-02,  1.6382e-03,  1.3019e-01, -1.7303e-02,\n",
      "          1.3130e-01, -1.0696e-01,  1.3187e-01, -5.8022e-04,  7.7281e-02]]), tensor([-0.3369])]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 162\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39melif\u001b[39;00m selected_training_approach \u001b[39m==\u001b[39m fgsm_attack:\n\u001b[0;32m    161\u001b[0m     epsilon \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 162\u001b[0m     X_adv \u001b[39m=\u001b[39m fgsm_attack(local_model, clients_batched[client]\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mtensors[\u001b[39m0\u001b[39;49m], clients_batched[client]\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mtensors[\u001b[39m1\u001b[39;49m], epsilon, loss, optimizer)\n\u001b[0;32m    163\u001b[0m     train_model(local_model, X_adv, loss, optimizer)\n\u001b[0;32m    164\u001b[0m \u001b[39m# train_model(local_model, train_loader, loss, optimizer)\u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39m# # FGSM attack\u001b[39;00m\n\u001b[0;32m    166\u001b[0m \u001b[39m# epsilon = 1\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    169\u001b[0m \n\u001b[0;32m    170\u001b[0m \u001b[39m# scale the model weights and add to the list\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 21\u001b[0m, in \u001b[0;36mfgsm_attack\u001b[1;34m(model, X, y, epsilon, loss, optimizer)\u001b[0m\n\u001b[0;32m     19\u001b[0m X_adv \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mclone()\n\u001b[0;32m     20\u001b[0m X_adv\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m outputs \u001b[39m=\u001b[39m model(X_adv)\n\u001b[0;32m     22\u001b[0m loss \u001b[39m=\u001b[39m loss(outputs, y)\n\u001b[0;32m     23\u001b[0m \u001b[39m# print(\"adver\" , loss)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mh:\\GIT project\\DW-FedAvg\\federated_utils_fedavg_copy.py:97\u001b[0m, in \u001b[0;36mSimpleMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     95\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu1(x)\n\u001b[0;32m     96\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc2(x)\n\u001b[1;32m---> 97\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu2(x)\n\u001b[0;32m     98\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc3(x)\n\u001b[0;32m     99\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu3(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m, inplace\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\functional.py:1471\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1469\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrelu_(\u001b[39minput\u001b[39m)\n\u001b[0;32m   1470\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1471\u001b[0m     result \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mrelu(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m   1472\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [5, 10, 15]\n",
    "n_round = [200, 5000]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "# for d in range(0,len(dataset)):\n",
    "for d in range(0, 2):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "                print(global_weights)\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [5, 10, 15]\n",
    "n_round = [200, 5000]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "# for d in range(0,len(dataset)):\n",
    "for d in range(0, 2):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "                print(global_weights)\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
