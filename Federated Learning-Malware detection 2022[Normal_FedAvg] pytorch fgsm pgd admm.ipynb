{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain\n",
    "import copy\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33dcfcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_prox(model,global_model, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    for i in range(2):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Add the proximal term\n",
    "            # for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            #     loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_model.parameters())):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_model.parameters()))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "    # temp2 = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    # Update x_hats\n",
    "    # for x_hat, z_weight, param_model in zip(x_hats[int(client[-1])-1],(Z_weights[int(client[-1])-1], local_model.parameters()) ):\n",
    "    #     x_hat = param_model +  z_weight/rho\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "    # print(x_hats[int(client[-1])-1])\n",
    "    # print(delta_x_hats[int(client[-1])-1])\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3ab07f",
   "metadata": {},
   "source": [
    "## non iid fedAVG FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6356e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-non-iid-FGSM  2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model_prox(local_model, global_model,X_adv, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-non-iid-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-non-iid-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-non-iid-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d522b1",
   "metadata": {},
   "source": [
    "## iid fedAVG FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32db8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model_prox(local_model, global_model,X_adv, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5238cac1",
   "metadata": {},
   "source": [
    "## iid fedAVG PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0127a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    # Generate adversarial examples using PGD\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        loss.backward()\n",
    "\n",
    "        grad_sign = X_adv.grad.data.sign()\n",
    "        X_adv = X_adv + alpha * grad_sign\n",
    "        X_adv = torch.min(torch.max(X_adv, X - epsilon), X + epsilon)  # Clip to epsilon neighborhood\n",
    "        X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear the gradients for the next iteration\n",
    "\n",
    "    model.train()  # Set the model back to training mode\n",
    "\n",
    "    # Create a DataLoader for the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv.detach(), y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "                # List of training approaches\n",
    "                training_approaches = [train_model, pgd_attack]\n",
    "\n",
    "                # Randomly choose between normal training and PGD attack\n",
    "                selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                # Apply the selected training approach\n",
    "                if selected_training_approach == train_model:\n",
    "                        train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                elif selected_training_approach == pgd_attack:\n",
    "                    epsilon = 0.01\n",
    "                    alpha = 0.01\n",
    "                    num_iter = 5\n",
    "                    X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                    train_model_prox(local_model, global_model,X_adv, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                    Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb1f186",
   "metadata": {},
   "source": [
    "## non-iid fedAVG PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter):\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = nn.BCELoss()(outputs, y)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = X_adv.grad.data\n",
    "        X_adv = X_adv + alpha * grad.sign()\n",
    "        X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "        X_adv = torch.clamp(X_adv, 0, 0.1)\n",
    "        X_adv.detach_()\n",
    "\n",
    "    return X_adv\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "# for d in range(0,1):\n",
    "for d in range(0, 2):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg non iid pgd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "                    selected_training_approach = random.choice([train_model, pgd_attack])\n",
    "\n",
    "                    # List of training approaches    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.1  # Set your desired value for epsilon\n",
    "                        alpha = 0.01   # Set your desired value for alpha\n",
    "                        num_iter = 10   # Set your desired number of iterations\n",
    "                        X_adv = pgd_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter)\n",
    "                        train_model_prox(local_model, global_model,X_adv, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                        Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-non-iid-pgd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-non-iid-pgd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-non-iid-pgd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
