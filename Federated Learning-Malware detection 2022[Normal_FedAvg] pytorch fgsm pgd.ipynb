{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3ab07f",
   "metadata": {},
   "source": [
    "## non iid fedAVG FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6356e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg-non-iid-FGSM  2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6717052459716797 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6724314111503801| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6537531018257141 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7590051429469546| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6309362053871155 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8221754034086565| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6378189325332642 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8297718271350637| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6149929165840149 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8676570792562741| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.577144205570221 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9054521125683823| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5951080322265625 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9059316967641362| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.5133616924285889 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9384546277738129| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 82.480% | global_loss: 0.424327552318573 | global_f1: 0.6887182516243354 | global_precision: 0.9982876712328768 | global_recall: 0.5256988277727682 | global_auc: 0.9722667380819765| flobal_FPR: 0.4743011722272317 \n",
      "comm_round: 9 | global_acc: 93.750% | global_loss: 0.3259251117706299 | global_f1: 0.9088263821532492 | global_precision: 0.9832109129066107 | global_recall: 0.8449053201082056 | global_auc: 0.9852155113673324| flobal_FPR: 0.1550946798917944 \n",
      "comm_round: 10 | global_acc: 90.492% | global_loss: 0.3456845283508301 | global_f1: 0.8549695740365111 | global_precision: 0.9768250289687138 | global_recall: 0.7601442741208295 | global_auc: 0.9788232713245213| flobal_FPR: 0.23985572587917042 \n",
      "comm_round: 11 | global_acc: 91.622% | global_loss: 0.3734607398509979 | global_f1: 0.8756169792694964 | global_precision: 0.9672846237731734 | global_recall: 0.799819657348963 | global_auc: 0.9759025560887964| flobal_FPR: 0.20018034265103696 \n",
      "comm_round: 12 | global_acc: 95.013% | global_loss: 0.2701566517353058 | global_f1: 0.92970946579194 | global_precision: 0.9678048780487805 | global_recall: 0.8944995491433724 | global_auc: 0.9863309007493384| flobal_FPR: 0.1055004508566276 \n",
      "comm_round: 13 | global_acc: 95.944% | global_loss: 0.17163214087486267 | global_f1: 0.9435707678075854 | global_precision: 0.9686609686609686 | global_recall: 0.9197475202885482 | global_auc: 0.990407366413247| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 14 | global_acc: 96.144% | global_loss: 0.14572174847126007 | global_f1: 0.9465437788018433 | global_precision: 0.9679547596606974 | global_recall: 0.9260595130748422 | global_auc: 0.9909638740146564| flobal_FPR: 0.0739404869251578 \n",
      "comm_round: 15 | global_acc: 96.676% | global_loss: 0.11464057117700577 | global_f1: 0.9542124542124543 | global_precision: 0.9693023255813954 | global_recall: 0.939585211902615 | global_auc: 0.9924657797682895| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 16 | global_acc: 96.941% | global_loss: 0.10136312246322632 | global_f1: 0.958029197080292 | global_precision: 0.9695290858725761 | global_recall: 0.9467989179440938 | global_auc: 0.9931495433741169| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 17 | global_acc: 96.709% | global_loss: 0.10888246446847916 | global_f1: 0.9545245751033532 | global_precision: 0.9728464419475655 | global_recall: 0.9368800721370604 | global_auc: 0.9918124056560546| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 18 | global_acc: 96.975% | global_loss: 0.09754447638988495 | global_f1: 0.9583142464498398 | global_precision: 0.9739292364990689 | global_recall: 0.9431920649233544 | global_auc: 0.9927858191226838| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 19 | global_acc: 97.241% | global_loss: 0.09203941375017166 | global_f1: 0.9620484682213077 | global_precision: 0.9758812615955473 | global_recall: 0.9486023444544635 | global_auc: 0.9932112720329765| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 20 | global_acc: 97.241% | global_loss: 0.0900113433599472 | global_f1: 0.9620484682213077 | global_precision: 0.9758812615955473 | global_recall: 0.9486023444544635 | global_auc: 0.9932639788109255| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 21 | global_acc: 97.274% | global_loss: 0.0847647413611412 | global_f1: 0.9624885635864592 | global_precision: 0.9767873723305478 | global_recall: 0.9486023444544635 | global_auc: 0.9937278934240461| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 22 | global_acc: 96.809% | global_loss: 0.10466155409812927 | global_f1: 0.955719557195572 | global_precision: 0.9782813975448537 | global_recall: 0.9341749323715058 | global_auc: 0.9909225632968043| flobal_FPR: 0.06582506762849413 \n",
      "comm_round: 23 | global_acc: 97.174% | global_loss: 0.0829300805926323 | global_f1: 0.961169483782549 | global_precision: 0.9740740740740741 | global_recall: 0.9486023444544635 | global_auc: 0.9941799371412318| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 24 | global_acc: 97.108% | global_loss: 0.08826886117458344 | global_f1: 0.96 | global_precision: 0.9793621013133208 | global_recall: 0.9413886384129847 | global_auc: 0.9935702479260358| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 25 | global_acc: 97.174% | global_loss: 0.07947763800621033 | global_f1: 0.9610270518110959 | global_precision: 0.9776119402985075 | global_recall: 0.9449954914337241 | global_auc: 0.9945840224388424| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 26 | global_acc: 97.141% | global_loss: 0.08164212852716446 | global_f1: 0.960514233241506 | global_precision: 0.9784845650140318 | global_recall: 0.9431920649233544 | global_auc: 0.9945460355718518| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 27 | global_acc: 97.374% | global_loss: 0.07606112957000732 | global_f1: 0.9639104613978986 | global_precision: 0.9768518518518519 | global_recall: 0.951307484220018 | global_auc: 0.9949638911087464| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 28 | global_acc: 97.241% | global_loss: 0.0796576663851738 | global_f1: 0.961978928080623 | global_precision: 0.9776536312849162 | global_recall: 0.9467989179440938 | global_auc: 0.9947540136686244| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 29 | global_acc: 97.507% | global_loss: 0.07172732800245285 | global_f1: 0.9657690552259243 | global_precision: 0.977818853974122 | global_recall: 0.9540126239855726 | global_auc: 0.9953879195115268| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 30 | global_acc: 97.440% | global_loss: 0.0739244669675827 | global_f1: 0.9647919524462735 | global_precision: 0.9786641929499073 | global_recall: 0.951307484220018 | global_auc: 0.9952597138354342| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 31 | global_acc: 97.307% | global_loss: 0.07616882771253586 | global_f1: 0.9628610729023384 | global_precision: 0.9794776119402985 | global_recall: 0.9467989179440938 | global_auc: 0.9951006438299118| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 32 | global_acc: 97.540% | global_loss: 0.07307185977697372 | global_f1: 0.9661172161172161 | global_precision: 0.9813953488372092 | global_recall: 0.951307484220018 | global_auc: 0.9952758582539052| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 33 | global_acc: 97.573% | global_loss: 0.07166313380002975 | global_f1: 0.9665903890160183 | global_precision: 0.9814126394052045 | global_recall: 0.9522091974752029 | global_auc: 0.9954249567068426| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 34 | global_acc: 97.374% | global_loss: 0.07585681229829788 | global_f1: 0.9637448370812299 | global_precision: 0.9813084112149533 | global_recall: 0.9467989179440938 | global_auc: 0.9952046328782982| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 35 | global_acc: 97.374% | global_loss: 0.07584850490093231 | global_f1: 0.9637448370812299 | global_precision: 0.9813084112149533 | global_recall: 0.9467989179440938 | global_auc: 0.9952559151487351| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 36 | global_acc: 97.074% | global_loss: 0.08199203759431839 | global_f1: 0.9594470046082949 | global_precision: 0.9811498586239397 | global_recall: 0.9386834986474302 | global_auc: 0.9947706329229328| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 37 | global_acc: 97.573% | global_loss: 0.06898613274097443 | global_f1: 0.9665903890160183 | global_precision: 0.9814126394052045 | global_recall: 0.9522091974752029 | global_auc: 0.9956571514313213| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 38 | global_acc: 97.706% | global_loss: 0.06620984524488449 | global_f1: 0.9685075308078502 | global_precision: 0.9805914972273567 | global_recall: 0.9567177637511272 | global_auc: 0.9957988899287794| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 39 | global_acc: 97.473% | global_loss: 0.07219342142343521 | global_f1: 0.9652014652014652 | global_precision: 0.9804651162790697 | global_recall: 0.9504057709648331 | global_auc: 0.9955503133679109| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 40 | global_acc: 97.573% | global_loss: 0.06745467334985733 | global_f1: 0.966742596810934 | global_precision: 0.9769797421731123 | global_recall: 0.9567177637511272 | global_auc: 0.9958788997673779| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 41 | global_acc: 97.706% | global_loss: 0.06621182709932327 | global_f1: 0.968593536640874 | global_precision: 0.9779411764705882 | global_recall: 0.9594229035166817 | global_auc: 0.9959297072019775| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 42 | global_acc: 97.640% | global_loss: 0.06807989627122879 | global_f1: 0.9675650982183646 | global_precision: 0.9805555555555555 | global_recall: 0.9549143372407575 | global_auc: 0.995832840691152| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 43 | global_acc: 97.939% | global_loss: 0.06480839103460312 | global_f1: 0.9717153284671532 | global_precision: 0.9833795013850416 | global_recall: 0.9603246167718665 | global_auc: 0.9958105234067952| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 44 | global_acc: 98.005% | global_loss: 0.06394146382808685 | global_f1: 0.9728260869565218 | global_precision: 0.9772520473157416 | global_recall: 0.9684400360685302 | global_auc: 0.9959938100400239| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 45 | global_acc: 97.739% | global_loss: 0.0655469000339508 | global_f1: 0.9690346083788706 | global_precision: 0.9788408463661453 | global_recall: 0.9594229035166817 | global_auc: 0.9959259085152785| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 46 | global_acc: 97.806% | global_loss: 0.06512674689292908 | global_f1: 0.9699727024567788 | global_precision: 0.9788797061524335 | global_recall: 0.9612263300270514 | global_auc: 0.9959083395892955| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 47 | global_acc: 97.806% | global_loss: 0.06269463896751404 | global_f1: 0.97 | global_precision: 0.9780018331805683 | global_recall: 0.9621280432822362 | global_auc: 0.9960655102514684| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 48 | global_acc: 97.939% | global_loss: 0.062199804931879044 | global_f1: 0.9718693284936479 | global_precision: 0.9780821917808219 | global_recall: 0.9657348963029756 | global_auc: 0.996130087925352| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 49 | global_acc: 97.972% | global_loss: 0.06373582780361176 | global_f1: 0.9723104857013164 | global_precision: 0.9789762340036563 | global_recall: 0.9657348963029756 | global_auc: 0.9961751973299031| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 50 | global_acc: 97.839% | global_loss: 0.06409120559692383 | global_f1: 0.9704411095952706 | global_precision: 0.9788990825688073 | global_recall: 0.9621280432822362 | global_auc: 0.9960940004017111| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 51 | global_acc: 98.039% | global_loss: 0.06164649873971939 | global_f1: 0.9733152419719584 | global_precision: 0.9764065335753176 | global_recall: 0.9702434625788999 | global_auc: 0.996268739989867| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 52 | global_acc: 97.939% | global_loss: 0.06230204552412033 | global_f1: 0.9718693284936479 | global_precision: 0.9780821917808219 | global_recall: 0.9657348963029756 | global_auc: 0.9962165080477552| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 53 | global_acc: 97.872% | global_loss: 0.06402431428432465 | global_f1: 0.9709090909090908 | global_precision: 0.9789184234647113 | global_recall: 0.9630297565374211 | global_auc: 0.9961913417483741| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 54 | global_acc: 97.739% | global_loss: 0.06485801190137863 | global_f1: 0.968978102189781 | global_precision: 0.9806094182825484 | global_recall: 0.957619477006312 | global_auc: 0.9961243898953035| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 55 | global_acc: 98.105% | global_loss: 0.06267024576663971 | global_f1: 0.9742198100407056 | global_precision: 0.9773139745916516 | global_recall: 0.9711451758340848 | global_auc: 0.9963152739019302| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 56 | global_acc: 98.138% | global_loss: 0.06199798360466957 | global_f1: 0.9746835443037976 | global_precision: 0.9773345421577516 | global_recall: 0.9720468890892696 | global_auc: 0.9963228712753284| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 57 | global_acc: 97.972% | global_loss: 0.0626007467508316 | global_f1: 0.9723356009070295 | global_precision: 0.9781021897810219 | global_recall: 0.9666366095581606 | global_auc: 0.9962521207355587| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 58 | global_acc: 98.005% | global_loss: 0.06380251049995422 | global_f1: 0.9727520435967303 | global_precision: 0.979871912168344 | global_recall: 0.9657348963029756 | global_auc: 0.996191341748374| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 59 | global_acc: 98.072% | global_loss: 0.062275759875774384 | global_f1: 0.9736363636363635 | global_precision: 0.9816681943171403 | global_recall: 0.9657348963029756 | global_auc: 0.9962468975413475| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 60 | global_acc: 98.238% | global_loss: 0.06109001114964485 | global_f1: 0.9761153672825597 | global_precision: 0.9756756756756757 | global_recall: 0.9765554553651938 | global_auc: 0.9964952366842974| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 61 | global_acc: 98.205% | global_loss: 0.06323367357254028 | global_f1: 0.975609756097561 | global_precision: 0.9773755656108597 | global_recall: 0.9738503155996393 | global_auc: 0.9963428143804982| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 62 | global_acc: 98.138% | global_loss: 0.06261710077524185 | global_f1: 0.974660633484163 | global_precision: 0.9782016348773842 | global_recall: 0.9711451758340848 | global_auc: 0.9964235364728529| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 63 | global_acc: 98.105% | global_loss: 0.0622759684920311 | global_f1: 0.9741261915569679 | global_precision: 0.9808043875685558 | global_recall: 0.9675383228133454 | global_auc: 0.9963057771851827| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 64 | global_acc: 98.305% | global_loss: 0.061071790754795074 | global_f1: 0.9769959404600811 | global_precision: 0.9774368231046932 | global_recall: 0.9765554553651938 | global_auc: 0.9965004598785085| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 65 | global_acc: 98.271% | global_loss: 0.060840070247650146 | global_f1: 0.9765342960288809 | global_precision: 0.9774164408310749 | global_recall: 0.975653742110009 | global_auc: 0.9964843154600376| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 66 | global_acc: 98.172% | global_loss: 0.06165853515267372 | global_f1: 0.9751243781094527 | global_precision: 0.9782214156079855 | global_recall: 0.9720468890892696 | global_auc: 0.9964724445641031| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 67 | global_acc: 98.205% | global_loss: 0.061685629189014435 | global_f1: 0.9755656108597286 | global_precision: 0.9791099000908265 | global_recall: 0.9720468890892696 | global_auc: 0.9964795671016637| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 68 | global_acc: 98.172% | global_loss: 0.06238142400979996 | global_f1: 0.9750566893424036 | global_precision: 0.9808394160583942 | global_recall: 0.9693417493237151 | global_auc: 0.9964373067121369| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 69 | global_acc: 98.138% | global_loss: 0.061847083270549774 | global_f1: 0.974568574023615 | global_precision: 0.9817017383348582 | global_recall: 0.9675383228133454 | global_auc: 0.9964529762947705| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 70 | global_acc: 98.138% | global_loss: 0.062202274799346924 | global_f1: 0.9745454545454545 | global_precision: 0.9825847846012832 | global_recall: 0.9666366095581606 | global_auc: 0.996437306712137| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 71 | global_acc: 98.205% | global_loss: 0.06183077394962311 | global_f1: 0.9754768392370572 | global_precision: 0.9826166514181153 | global_recall: 0.9684400360685302 | global_auc: 0.9964320835179258| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 72 | global_acc: 98.205% | global_loss: 0.06154569610953331 | global_f1: 0.9755213055303718 | global_precision: 0.9808568824065633 | global_recall: 0.9702434625788999 | global_auc: 0.9963722542024158| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 73 | global_acc: 98.172% | global_loss: 0.06200485676527023 | global_f1: 0.9750340444847935 | global_precision: 0.9817184643510055 | global_recall: 0.9684400360685302 | global_auc: 0.9963717793665784| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 74 | global_acc: 98.238% | global_loss: 0.06353767961263657 | global_f1: 0.9759200363471149 | global_precision: 0.9835164835164835 | global_recall: 0.9684400360685302 | global_auc: 0.996332367992076| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 75 | global_acc: 98.271% | global_loss: 0.06236840784549713 | global_f1: 0.9763851044504995 | global_precision: 0.9835315645013724 | global_recall: 0.9693417493237151 | global_auc: 0.996369880023229| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 76 | global_acc: 98.172% | global_loss: 0.06344395875930786 | global_f1: 0.9749886311959981 | global_precision: 0.9834862385321101 | global_recall: 0.9666366095581606 | global_auc: 0.9963466130671973| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 77 | global_acc: 98.238% | global_loss: 0.06377630680799484 | global_f1: 0.9759200363471149 | global_precision: 0.9835164835164835 | global_recall: 0.9684400360685302 | global_auc: 0.9962867837516876| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 78 | global_acc: 98.238% | global_loss: 0.06287066638469696 | global_f1: 0.9759200363471149 | global_precision: 0.9835164835164835 | global_recall: 0.9684400360685302 | global_auc: 0.9963755780532775| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 79 | global_acc: 98.238% | global_loss: 0.06400617957115173 | global_f1: 0.9759200363471149 | global_precision: 0.9835164835164835 | global_recall: 0.9684400360685302 | global_auc: 0.9962364511529251| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 80 | global_acc: 98.338% | global_loss: 0.06356547027826309 | global_f1: 0.9772933696639419 | global_precision: 0.9844464775846294 | global_recall: 0.9702434625788999 | global_auc: 0.9963233461111658| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 81 | global_acc: 98.305% | global_loss: 0.06254516541957855 | global_f1: 0.9768497503404449 | global_precision: 0.9835466179159049 | global_recall: 0.9702434625788999 | global_auc: 0.9964415802346733| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 82 | global_acc: 98.338% | global_loss: 0.062246475368738174 | global_f1: 0.9773139745916515 | global_precision: 0.9835616438356164 | global_recall: 0.9711451758340848 | global_auc: 0.9964581994889817| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 83 | global_acc: 98.404% | global_loss: 0.06232959032058716 | global_f1: 0.9782608695652174 | global_precision: 0.9827115559599636 | global_recall: 0.9738503155996393 | global_auc: 0.996433508025438| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 84 | global_acc: 98.305% | global_loss: 0.06287503987550735 | global_f1: 0.9769959404600811 | global_precision: 0.9774368231046932 | global_recall: 0.9765554553651938 | global_auc: 0.9965450944472223| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 85 | global_acc: 98.438% | global_loss: 0.062056418508291245 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9965280003570766| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 86 | global_acc: 98.438% | global_loss: 0.0625229999423027 | global_f1: 0.9787234042553192 | global_precision: 0.9827272727272728 | global_recall: 0.9747520288548241 | global_auc: 0.9964876393108992| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 87 | global_acc: 98.371% | global_loss: 0.06270232796669006 | global_f1: 0.9778781038374718 | global_precision: 0.9792043399638336 | global_recall: 0.9765554553651938 | global_auc: 0.9965256261778898| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 88 | global_acc: 98.438% | global_loss: 0.06386847048997879 | global_f1: 0.9787618617261635 | global_precision: 0.9809782608695652 | global_recall: 0.9765554553651938 | global_auc: 0.9963860244417| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 89 | global_acc: 98.504% | global_loss: 0.06297758966684341 | global_f1: 0.9796287913082843 | global_precision: 0.9836363636363636 | global_recall: 0.975653742110009 | global_auc: 0.9964282848312267| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 90 | global_acc: 98.537% | global_loss: 0.06319475919008255 | global_f1: 0.9800904977375565 | global_precision: 0.9836512261580381 | global_recall: 0.9765554553651938 | global_auc: 0.9964230616370156| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 91 | global_acc: 98.371% | global_loss: 0.06324778497219086 | global_f1: 0.9777576032682705 | global_precision: 0.9844606946983546 | global_recall: 0.9711451758340848 | global_auc: 0.9963945714867728| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 92 | global_acc: 98.404% | global_loss: 0.06238415837287903 | global_f1: 0.9783393501805054 | global_precision: 0.979223125564589 | global_recall: 0.9774571686203787 | global_auc: 0.9965759587766521| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 93 | global_acc: 98.305% | global_loss: 0.06487759202718735 | global_f1: 0.9770166741775576 | global_precision: 0.9765765765765766 | global_recall: 0.9774571686203787 | global_auc: 0.9963784270683018| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 94 | global_acc: 98.371% | global_loss: 0.064479298889637 | global_f1: 0.9778980604420388 | global_precision: 0.9783393501805054 | global_recall: 0.9774571686203787 | global_auc: 0.9964031185318455| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 95 | global_acc: 98.471% | global_loss: 0.06328675895929337 | global_f1: 0.9792231255645889 | global_precision: 0.9809954751131221 | global_recall: 0.9774571686203787 | global_auc: 0.9964691207132413| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 96 | global_acc: 98.471% | global_loss: 0.06341595947742462 | global_f1: 0.9791477787851314 | global_precision: 0.9845031905195989 | global_recall: 0.9738503155996393 | global_auc: 0.9964325583537631| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 97 | global_acc: 98.471% | global_loss: 0.06457104533910751 | global_f1: 0.9791477787851314 | global_precision: 0.9845031905195989 | global_recall: 0.9738503155996393 | global_auc: 0.9962298034512018| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 98 | global_acc: 98.504% | global_loss: 0.0644419714808464 | global_f1: 0.9796103307657453 | global_precision: 0.9845173041894353 | global_recall: 0.9747520288548241 | global_auc: 0.9962298034512018| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 99 | global_acc: 98.404% | global_loss: 0.06416098773479462 | global_f1: 0.9783393501805054 | global_precision: 0.979223125564589 | global_recall: 0.9774571686203787 | global_auc: 0.9965099565952561| flobal_FPR: 0.02254283137962128 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-non-iid-FGSM  2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-non-iid-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-non-iid-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-non-iid-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d522b1",
   "metadata": {},
   "source": [
    "## iid fedAVG FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f32db8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6646864414215088 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.5728918119783037| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6614248156547546 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.627435017528565| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6578173637390137 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6880340419308535| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6545289754867554 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7372837775660009| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6508768796920776 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7818050979325173| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6478559374809265 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8114792513358319| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6440765261650085 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8383995943002607| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6426448822021484 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8508908632563007| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.6385051012039185 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8711024880923042| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.6338786482810974 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8863508913380922| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.6286982297897339 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8983761089197437| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.6258054375648499 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9071691189563488| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.6225525140762329 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9145611258547639| flobal_FPR: 1.0 \n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.6171506643295288 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9213686098373639| flobal_FPR: 1.0 \n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.6220712065696716 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9194538343231285| flobal_FPR: 1.0 \n",
      "comm_round: 15 | global_acc: 63.132% | global_loss: 0.612547755241394 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9293824142648281| flobal_FPR: 1.0 \n",
      "comm_round: 16 | global_acc: 63.132% | global_loss: 0.6074575185775757 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9347015253151603| flobal_FPR: 1.0 \n",
      "comm_round: 17 | global_acc: 63.132% | global_loss: 0.6017714142799377 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9393235773562185| flobal_FPR: 1.0 \n",
      "comm_round: 18 | global_acc: 63.132% | global_loss: 0.5928383469581604 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9443141020070837| flobal_FPR: 1.0 \n",
      "comm_round: 19 | global_acc: 63.132% | global_loss: 0.5800694227218628 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.950075760057854| flobal_FPR: 1.0 \n",
      "comm_round: 20 | global_acc: 63.132% | global_loss: 0.575211763381958 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9534034096062138| flobal_FPR: 1.0 \n",
      "comm_round: 21 | global_acc: 63.132% | global_loss: 0.5627079010009766 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9581014353812529| flobal_FPR: 1.0 \n",
      "comm_round: 22 | global_acc: 63.132% | global_loss: 0.5561227202415466 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9607092338001444| flobal_FPR: 1.0 \n",
      "comm_round: 23 | global_acc: 63.132% | global_loss: 0.548652172088623 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9629096230705639| flobal_FPR: 1.0 \n",
      "comm_round: 24 | global_acc: 63.132% | global_loss: 0.5360390543937683 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9656499006880847| flobal_FPR: 1.0 \n",
      "comm_round: 25 | global_acc: 63.132% | global_loss: 0.5404398441314697 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9650796228473911| flobal_FPR: 1.0 \n",
      "comm_round: 26 | global_acc: 63.132% | global_loss: 0.529468834400177 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9674267363915611| flobal_FPR: 1.0 \n",
      "comm_round: 27 | global_acc: 65.259% | global_loss: 0.5131948590278625 | global_f1: 0.10912190963341858 | global_precision: 1.0 | global_recall: 0.057709648331830475 | global_auc: 0.9697106967693594| flobal_FPR: 0.9422903516681695 \n",
      "comm_round: 28 | global_acc: 65.559% | global_loss: 0.5015296936035156 | global_f1: 0.12351945854483924 | global_precision: 1.0 | global_recall: 0.06582506762849413 | global_auc: 0.9713208650939154| flobal_FPR: 0.9341749323715058 \n",
      "comm_round: 29 | global_acc: 73.371% | global_loss: 0.47827985882759094 | global_f1: 0.4347212420606916 | global_precision: 1.0 | global_recall: 0.2777276825969342 | global_auc: 0.9739229654827586| flobal_FPR: 0.7222723174030659 \n",
      "comm_round: 30 | global_acc: 81.250% | global_loss: 0.46046358346939087 | global_f1: 0.6590084643288996 | global_precision: 1.0 | global_recall: 0.4914337240757439 | global_auc: 0.9758394029224247| flobal_FPR: 0.5085662759242561 \n",
      "comm_round: 31 | global_acc: 85.738% | global_loss: 0.4360763132572174 | global_f1: 0.7612687813021702 | global_precision: 0.9941860465116279 | global_recall: 0.6167718665464382 | global_auc: 0.9780412167003563| flobal_FPR: 0.38322813345356177 \n",
      "comm_round: 32 | global_acc: 88.231% | global_loss: 0.42288938164711 | global_f1: 0.8115015974440895 | global_precision: 0.9908972691807543 | global_recall: 0.6871055004508566 | global_auc: 0.9788688555649098| flobal_FPR: 0.3128944995491434 \n",
      "comm_round: 33 | global_acc: 89.694% | global_loss: 0.4148291349411011 | global_f1: 0.8390446521287642 | global_precision: 0.988984088127295 | global_recall: 0.7285843101893598 | global_auc: 0.9788574595048127| flobal_FPR: 0.27141568981064024 \n",
      "comm_round: 34 | global_acc: 90.991% | global_loss: 0.3929588496685028 | global_f1: 0.8625063419583968 | global_precision: 0.9860788863109049 | global_recall: 0.7664562669071235 | global_auc: 0.9801737044460304| flobal_FPR: 0.23354373309287646 \n",
      "comm_round: 35 | global_acc: 92.753% | global_loss: 0.3714297413825989 | global_f1: 0.8926108374384237 | global_precision: 0.9837133550488599 | global_recall: 0.8169522091974752 | global_auc: 0.9811718093762034| flobal_FPR: 0.18304779080252478 \n",
      "comm_round: 36 | global_acc: 93.418% | global_loss: 0.3556671142578125 | global_f1: 0.9038834951456312 | global_precision: 0.9789695057833859 | global_recall: 0.8394950405770965 | global_auc: 0.9815792185246754| flobal_FPR: 0.16050495942290352 \n",
      "comm_round: 37 | global_acc: 93.949% | global_loss: 0.3337648808956146 | global_f1: 0.9124157844080847 | global_precision: 0.978328173374613 | global_recall: 0.8548241659152389 | global_auc: 0.9825882446791083| flobal_FPR: 0.14517583408476104 \n",
      "comm_round: 38 | global_acc: 94.249% | global_loss: 0.31299862265586853 | global_f1: 0.9175011921793037 | global_precision: 0.9736842105263158 | global_recall: 0.8674481514878268 | global_auc: 0.9832354459254573| flobal_FPR: 0.13255184851217314 \n",
      "comm_round: 39 | global_acc: 94.282% | global_loss: 0.28688567876815796 | global_f1: 0.9185606060606061 | global_precision: 0.967098703888335 | global_recall: 0.8746618575293057 | global_auc: 0.9841395333598293| flobal_FPR: 0.12533814247069433 \n",
      "comm_round: 40 | global_acc: 94.215% | global_loss: 0.2743839621543884 | global_f1: 0.9179245283018868 | global_precision: 0.9624134520276953 | global_recall: 0.8773669972948602 | global_auc: 0.9839401023081294| flobal_FPR: 0.12263300270513977 \n",
      "comm_round: 41 | global_acc: 94.116% | global_loss: 0.2558332681655884 | global_f1: 0.9168623767026772 | global_precision: 0.9568627450980393 | global_recall: 0.8800721370604148 | global_auc: 0.9843043013954001| flobal_FPR: 0.11992786293958521 \n",
      "comm_round: 42 | global_acc: 94.382% | global_loss: 0.22825099527835846 | global_f1: 0.9209911173445535 | global_precision: 0.9563106796116505 | global_recall: 0.8881875563570785 | global_auc: 0.9855787607829284| flobal_FPR: 0.11181244364292155 \n",
      "comm_round: 43 | global_acc: 94.415% | global_loss: 0.22062207758426666 | global_f1: 0.9214218896164641 | global_precision: 0.9572400388726919 | global_recall: 0.8881875563570785 | global_auc: 0.9853071546839469| flobal_FPR: 0.11181244364292155 \n",
      "comm_round: 44 | global_acc: 94.382% | global_loss: 0.20803844928741455 | global_f1: 0.9210649229332089 | global_precision: 0.9554263565891473 | global_recall: 0.8890892696122633 | global_auc: 0.9856618570544698| flobal_FPR: 0.1109107303877367 \n",
      "comm_round: 45 | global_acc: 94.315% | global_loss: 0.20104122161865234 | global_f1: 0.9204281060958585 | global_precision: 0.9509615384615384 | global_recall: 0.8917944093778178 | global_auc: 0.9855341262142145| flobal_FPR: 0.10820559062218214 \n",
      "comm_round: 46 | global_acc: 94.382% | global_loss: 0.1849347949028015 | global_f1: 0.9215048769159313 | global_precision: 0.9501915708812261 | global_recall: 0.8944995491433724 | global_auc: 0.9865887366090358| flobal_FPR: 0.1055004508566276 \n",
      "comm_round: 47 | global_acc: 94.515% | global_loss: 0.18466992676258087 | global_f1: 0.9235757295044003 | global_precision: 0.9495238095238095 | global_recall: 0.8990081154192967 | global_auc: 0.9859078220182327| flobal_FPR: 0.10099188458070334 \n",
      "comm_round: 48 | global_acc: 94.581% | global_loss: 0.17009596526622772 | global_f1: 0.9245020842982863 | global_precision: 0.9504761904761905 | global_recall: 0.8999098286744815 | global_auc: 0.9873503732921934| flobal_FPR: 0.10009017132551848 \n",
      "comm_round: 49 | global_acc: 94.681% | global_loss: 0.16642707586288452 | global_f1: 0.9259944495837187 | global_precision: 0.9506172839506173 | global_recall: 0.9026149684400361 | global_auc: 0.9872891194691714| flobal_FPR: 0.09738503155996393 \n",
      "comm_round: 50 | global_acc: 95.047% | global_loss: 0.15832802653312683 | global_f1: 0.9314942528735632 | global_precision: 0.950281425891182 | global_recall: 0.9134355275022543 | global_auc: 0.9878574979665156| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 51 | global_acc: 95.180% | global_loss: 0.15128007531166077 | global_f1: 0.9331489165514061 | global_precision: 0.9547169811320755 | global_recall: 0.9125338142470695 | global_auc: 0.9883826664026579| flobal_FPR: 0.08746618575293057 \n",
      "comm_round: 52 | global_acc: 95.246% | global_loss: 0.14848507940769196 | global_f1: 0.9341923607915325 | global_precision: 0.9539473684210527 | global_recall: 0.915238954012624 | global_auc: 0.9884083075378763| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 53 | global_acc: 95.213% | global_loss: 0.14828795194625854 | global_f1: 0.9336405529953916 | global_precision: 0.9547596606974552 | global_recall: 0.9134355275022543 | global_auc: 0.9881723141266986| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 54 | global_acc: 95.412% | global_loss: 0.14344698190689087 | global_f1: 0.93646408839779 | global_precision: 0.9567262464722484 | global_recall: 0.9170423805229937 | global_auc: 0.9886428764415423| flobal_FPR: 0.0829576194770063 \n",
      "comm_round: 55 | global_acc: 95.512% | global_loss: 0.13656938076019287 | global_f1: 0.9378739070409573 | global_precision: 0.9577067669172933 | global_recall: 0.9188458070333634 | global_auc: 0.9893499070034012| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 56 | global_acc: 95.778% | global_loss: 0.12881018221378326 | global_f1: 0.9416091954022987 | global_precision: 0.9606003752345216 | global_recall: 0.9233543733092876 | global_auc: 0.9901599769419717| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 57 | global_acc: 95.678% | global_loss: 0.12731191515922546 | global_f1: 0.9400921658986174 | global_precision: 0.9613572101790764 | global_recall: 0.9197475202885482 | global_auc: 0.990240224198489| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 58 | global_acc: 96.011% | global_loss: 0.11885733157396317 | global_f1: 0.9449541284403671 | global_precision: 0.96171802054155 | global_recall: 0.9287646528403968 | global_auc: 0.9911694779322419| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 59 | global_acc: 96.110% | global_loss: 0.1168103814125061 | global_f1: 0.9462068965517243 | global_precision: 0.9652908067542214 | global_recall: 0.9278629395852119 | global_auc: 0.9913480162070968| flobal_FPR: 0.0721370604147881 \n",
      "comm_round: 60 | global_acc: 96.177% | global_loss: 0.11412413418292999 | global_f1: 0.9472234970169803 | global_precision: 0.9644859813084112 | global_recall: 0.9305680793507665 | global_auc: 0.9915303531686508| flobal_FPR: 0.06943192064923355 \n",
      "comm_round: 61 | global_acc: 96.044% | global_loss: 0.11553559452295303 | global_f1: 0.945287356321839 | global_precision: 0.9643527204502814 | global_recall: 0.9269612263300271 | global_auc: 0.9912929352499606| flobal_FPR: 0.07303877366997295 \n",
      "comm_round: 62 | global_acc: 96.177% | global_loss: 0.11152362823486328 | global_f1: 0.9472718936267767 | global_precision: 0.9636194029850746 | global_recall: 0.9314697926059513 | global_auc: 0.9916804012932628| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 63 | global_acc: 96.210% | global_loss: 0.11000809818506241 | global_f1: 0.9477543538038496 | global_precision: 0.9636533084808947 | global_recall: 0.9323715058611362 | global_auc: 0.9918209527011274| flobal_FPR: 0.06762849413886383 \n",
      "comm_round: 64 | global_acc: 96.543% | global_loss: 0.10401739925146103 | global_f1: 0.9524245196706312 | global_precision: 0.9665738161559888 | global_recall: 0.9386834986474302 | global_auc: 0.9922957885385075| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 65 | global_acc: 96.509% | global_loss: 0.10284790396690369 | global_f1: 0.9519450800915331 | global_precision: 0.966542750929368 | global_recall: 0.9377817853922452 | global_auc: 0.9924610314099158| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 66 | global_acc: 96.476% | global_loss: 0.10214677453041077 | global_f1: 0.9514652014652014 | global_precision: 0.9665116279069768 | global_recall: 0.9368800721370604 | global_auc: 0.9924425128122579| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 67 | global_acc: 96.609% | global_loss: 0.0992070883512497 | global_f1: 0.953382084095064 | global_precision: 0.9666357738646896 | global_recall: 0.9404869251577999 | global_auc: 0.9927713366296438| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 68 | global_acc: 96.576% | global_loss: 0.09882853925228119 | global_f1: 0.9528604118993135 | global_precision: 0.9674721189591078 | global_recall: 0.9386834986474302 | global_auc: 0.9928029132128295| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 69 | global_acc: 96.842% | global_loss: 0.09600114077329636 | global_f1: 0.9566408032861706 | global_precision: 0.9685767097966729 | global_recall: 0.9449954914337241 | global_auc: 0.9929705302634246| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 70 | global_acc: 96.842% | global_loss: 0.09681932628154755 | global_f1: 0.9565217391304348 | global_precision: 0.9711895910780669 | global_recall: 0.9422903516681695 | global_auc: 0.992937291754808| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 71 | global_acc: 97.008% | global_loss: 0.09464230388402939 | global_f1: 0.9587912087912088 | global_precision: 0.9739534883720931 | global_recall: 0.9440937781785392 | global_auc: 0.9930768934909977| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 72 | global_acc: 96.975% | global_loss: 0.09302949160337448 | global_f1: 0.9583524027459955 | global_precision: 0.9730483271375465 | global_recall: 0.9440937781785392 | global_auc: 0.9931685368076123| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 73 | global_acc: 97.041% | global_loss: 0.09086668491363525 | global_f1: 0.959304983996342 | global_precision: 0.9730983302411874 | global_recall: 0.9458972046889089 | global_auc: 0.993325707469785| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 74 | global_acc: 97.241% | global_loss: 0.08899908512830734 | global_f1: 0.9621523027815777 | global_precision: 0.9732472324723247 | global_recall: 0.951307484220018 | global_auc: 0.9935184908197614| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 75 | global_acc: 97.041% | global_loss: 0.09037981927394867 | global_f1: 0.9592677345537758 | global_precision: 0.9739776951672863 | global_recall: 0.9449954914337241 | global_auc: 0.9934923748487055| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 76 | global_acc: 97.008% | global_loss: 0.08837617933750153 | global_f1: 0.9588665447897624 | global_precision: 0.9721964782205746 | global_recall: 0.9458972046889089 | global_auc: 0.9936362501074316| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 77 | global_acc: 97.041% | global_loss: 0.08742565661668777 | global_f1: 0.959304983996342 | global_precision: 0.9730983302411874 | global_recall: 0.9458972046889089 | global_auc: 0.9937160225281116| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 78 | global_acc: 97.241% | global_loss: 0.08613121509552002 | global_f1: 0.9621523027815777 | global_precision: 0.9732472324723247 | global_recall: 0.951307484220018 | global_auc: 0.9938043419938642| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 79 | global_acc: 97.241% | global_loss: 0.08507894724607468 | global_f1: 0.9622212107419208 | global_precision: 0.9715073529411765 | global_recall: 0.9531109107303878 | global_auc: 0.9939396702075175| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 80 | global_acc: 97.241% | global_loss: 0.08364560455083847 | global_f1: 0.9622555707139608 | global_precision: 0.9706422018348624 | global_recall: 0.9540126239855726 | global_auc: 0.9940892434962922| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 81 | global_acc: 97.008% | global_loss: 0.08588000386953354 | global_f1: 0.9588665447897624 | global_precision: 0.9721964782205746 | global_recall: 0.9458972046889089 | global_auc: 0.9939890531346052| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 82 | global_acc: 97.008% | global_loss: 0.08449437469244003 | global_f1: 0.958941605839416 | global_precision: 0.9704524469067405 | global_recall: 0.9477006311992786 | global_auc: 0.9940389108975299| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 83 | global_acc: 97.374% | global_loss: 0.08180610090494156 | global_f1: 0.9641398093508853 | global_precision: 0.9707495429616088 | global_recall: 0.957619477006312 | global_auc: 0.9942597095619118| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 84 | global_acc: 97.473% | global_loss: 0.08104400336742401 | global_f1: 0.965391621129326 | global_precision: 0.9751609935602575 | global_recall: 0.9558160504959423 | global_auc: 0.9943214382207711| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 85 | global_acc: 97.407% | global_loss: 0.08099319785833359 | global_f1: 0.96448087431694 | global_precision: 0.9742410303587856 | global_recall: 0.9549143372407575 | global_auc: 0.9943504032068513| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 86 | global_acc: 97.540% | global_loss: 0.07914190739393234 | global_f1: 0.9664246823956443 | global_precision: 0.9726027397260274 | global_recall: 0.9603246167718665 | global_auc: 0.9945104228840485| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 87 | global_acc: 97.673% | global_loss: 0.07768186926841736 | global_f1: 0.9681528662420382 | global_precision: 0.9770431588613406 | global_recall: 0.9594229035166817 | global_auc: 0.9946096635740609| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 88 | global_acc: 97.606% | global_loss: 0.07750577479600906 | global_f1: 0.9672131147540984 | global_precision: 0.9770009199632015 | global_recall: 0.957619477006312 | global_auc: 0.9946106132457356| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 89 | global_acc: 97.640% | global_loss: 0.07721319794654846 | global_f1: 0.9676832043695949 | global_precision: 0.9770220588235294 | global_recall: 0.9585211902614968 | global_auc: 0.99470225656235| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 90 | global_acc: 97.640% | global_loss: 0.07744384557008743 | global_f1: 0.9676832043695949 | global_precision: 0.9770220588235294 | global_recall: 0.9585211902614968 | global_auc: 0.9947350202351293| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 91 | global_acc: 97.673% | global_loss: 0.07677838951349258 | global_f1: 0.9681818181818183 | global_precision: 0.9761686526122824 | global_recall: 0.9603246167718665 | global_auc: 0.9947915256997775| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 92 | global_acc: 97.573% | global_loss: 0.07816413789987564 | global_f1: 0.966742596810934 | global_precision: 0.9769797421731123 | global_recall: 0.9567177637511272 | global_auc: 0.994763510385372| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 93 | global_acc: 97.673% | global_loss: 0.07513047009706497 | global_f1: 0.9681818181818183 | global_precision: 0.9761686526122824 | global_recall: 0.9603246167718665 | global_auc: 0.995052210574499| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 94 | global_acc: 97.706% | global_loss: 0.0743674784898758 | global_f1: 0.9685649202733485 | global_precision: 0.9788213627992634 | global_recall: 0.9585211902614968 | global_auc: 0.9950151733791835| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 95 | global_acc: 97.706% | global_loss: 0.07335972040891647 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.9950906722773268| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 96 | global_acc: 97.706% | global_loss: 0.07343058288097382 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.9950821252322541| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 97 | global_acc: 97.673% | global_loss: 0.07279272377490997 | global_f1: 0.9681818181818183 | global_precision: 0.9761686526122824 | global_recall: 0.9603246167718665 | global_auc: 0.9951661711754705| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 98 | global_acc: 97.706% | global_loss: 0.07373203337192535 | global_f1: 0.968593536640874 | global_precision: 0.9779411764705882 | global_recall: 0.9594229035166817 | global_auc: 0.9951633221604462| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 99 | global_acc: 97.706% | global_loss: 0.07175858318805695 | global_f1: 0.9686506133575646 | global_precision: 0.9761904761904762 | global_recall: 0.9612263300270514 | global_auc: 0.9952563899845726| flobal_FPR: 0.0387736699729486 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5238cac1",
   "metadata": {},
   "source": [
    "## iid fedAVG PGD\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 8,
=======
   "execution_count": 11,
>>>>>>> Stashed changes
   "id": "0127a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg-FGSM 2017|\n",
<<<<<<< Updated upstream
      "|=======================|\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 173\u001b[0m\n\u001b[0;32m    171\u001b[0m     alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m    172\u001b[0m     num_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[1;32m--> 173\u001b[0m     X_adv \u001b[38;5;241m=\u001b[39m\u001b[43mpgd_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocal_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclients_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclients_batched\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensors\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     train_model(local_model, X_adv, loss, optimizer)\n\u001b[0;32m    176\u001b[0m \u001b[38;5;66;03m# train_model(local_model, train_loader, loss, optimizer)\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# # FGSM attack\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;66;03m# epsilon = 1\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    181\u001b[0m \n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# scale the model weights and add to the list\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 24\u001b[0m, in \u001b[0;36mpgd_attack\u001b[1;34m(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m     21\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(outputs, y)\n\u001b[0;32m     22\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 24\u001b[0m grad_sign \u001b[38;5;241m=\u001b[39m \u001b[43mX_adv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39msign()\n\u001b[0;32m     25\u001b[0m X_adv \u001b[38;5;241m=\u001b[39m X_adv \u001b[38;5;241m+\u001b[39m alpha \u001b[38;5;241m*\u001b[39m grad_sign\n\u001b[0;32m     26\u001b[0m X_adv \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmin(torch\u001b[38;5;241m.\u001b[39mmax(X_adv, X \u001b[38;5;241m-\u001b[39m epsilon), X \u001b[38;5;241m+\u001b[39m epsilon)  \u001b[38;5;66;03m# Clip to epsilon neighborhood\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
=======
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6781193614006042 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6957183102871759| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6678715944290161 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8018944525403955| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6581866145133972 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.859070147973092| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6480657458305359 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8899719894339531| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.640242338180542 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9018839586683893| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6302680373191833 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9169141748469012| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6174482107162476 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.930404498404789| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.6019104719161987 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9401070564878956| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5831567645072937 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9477120272593759| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.5645759105682373 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9531460485823537| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.5379728078842163 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9587101749247742| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.5141770243644714 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9635587236602626| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 66.090% | global_loss: 0.482965886592865 | global_f1: 0.14858096828046746 | global_precision: 1.0 | global_recall: 0.08025247971145176 | global_auc: 0.9676969179830303| flobal_FPR: 0.9197475202885482 \n",
      "comm_round: 13 | global_acc: 76.596% | global_loss: 0.45587024092674255 | global_f1: 0.5356200527704486 | global_precision: 0.9975429975429976 | global_recall: 0.36609558160504957 | global_auc: 0.9718265652607253| flobal_FPR: 0.6339044183949504 \n",
      "comm_round: 14 | global_acc: 85.140% | global_loss: 0.4252158999443054 | global_f1: 0.749579831932773 | global_precision: 0.9896449704142012 | global_recall: 0.6032461677186655 | global_auc: 0.9746347444029912| flobal_FPR: 0.39675383228133454 \n",
      "comm_round: 15 | global_acc: 90.525% | global_loss: 0.39337149262428284 | global_f1: 0.85510930350788 | global_precision: 0.9801864801864801 | global_recall: 0.7583408476104598 | global_auc: 0.977020319649989| flobal_FPR: 0.24165915238954014 \n",
      "comm_round: 16 | global_acc: 92.620% | global_loss: 0.36347970366477966 | global_f1: 0.8920233463035019 | global_precision: 0.9683210137275607 | global_recall: 0.8268710550045085 | global_auc: 0.9802924134053754| flobal_FPR: 0.17312894499549145 \n",
      "comm_round: 17 | global_acc: 93.684% | global_loss: 0.3367662727832794 | global_f1: 0.9097815764482431 | global_precision: 0.9608826479438315 | global_recall: 0.8638412984670875 | global_auc: 0.9814633585803548| flobal_FPR: 0.13615870153291254 \n",
      "comm_round: 18 | global_acc: 94.149% | global_loss: 0.3071238398551941 | global_f1: 0.9177570093457944 | global_precision: 0.9524733268671193 | global_recall: 0.8854824165915239 | global_auc: 0.9825345882294844| flobal_FPR: 0.1145175834084761 \n",
      "comm_round: 19 | global_acc: 94.249% | global_loss: 0.2800164520740509 | global_f1: 0.9200184928340267 | global_precision: 0.9440227703984819 | global_recall: 0.8972046889089269 | global_auc: 0.9833631767657127| flobal_FPR: 0.10279531109107304 \n",
      "comm_round: 20 | global_acc: 94.282% | global_loss: 0.2578074336051941 | global_f1: 0.920955882352941 | global_precision: 0.9390815370196813 | global_recall: 0.9035166816952209 | global_auc: 0.9837582401824129| flobal_FPR: 0.09648331830477908 \n",
      "comm_round: 21 | global_acc: 94.415% | global_loss: 0.23171846568584442 | global_f1: 0.9226519337016575 | global_precision: 0.9426152398871119 | global_recall: 0.9035166816952209 | global_auc: 0.9847824610836418| flobal_FPR: 0.09648331830477908 \n",
      "comm_round: 22 | global_acc: 94.315% | global_loss: 0.21013426780700684 | global_f1: 0.9213793103448276 | global_precision: 0.9399624765478424 | global_recall: 0.9035166816952209 | global_auc: 0.9853973734930492| flobal_FPR: 0.09648331830477908 \n",
      "comm_round: 23 | global_acc: 94.315% | global_loss: 0.1919361650943756 | global_f1: 0.921451538814883 | global_precision: 0.9391385767790262 | global_recall: 0.9044183949504058 | global_auc: 0.9860801874272016| flobal_FPR: 0.09558160504959423 \n",
      "comm_round: 24 | global_acc: 94.481% | global_loss: 0.1770622432231903 | global_f1: 0.923783287419651 | global_precision: 0.941066417212348 | global_recall: 0.9071235347159603 | global_auc: 0.9866053558633442| flobal_FPR: 0.09287646528403967 \n",
      "comm_round: 25 | global_acc: 94.714% | global_loss: 0.16413424909114838 | global_f1: 0.9272976680384087 | global_precision: 0.9406307977736549 | global_recall: 0.9143372407574392 | global_auc: 0.9872031741826057| flobal_FPR: 0.08566275924256087 \n",
      "comm_round: 26 | global_acc: 94.847% | global_loss: 0.15463115274906158 | global_f1: 0.9289967934035731 | global_precision: 0.9441340782122905 | global_recall: 0.9143372407574392 | global_auc: 0.9876153316894517| flobal_FPR: 0.08566275924256087 \n",
      "comm_round: 27 | global_acc: 94.880% | global_loss: 0.14728611707687378 | global_f1: 0.9294871794871795 | global_precision: 0.9441860465116279 | global_recall: 0.915238954012624 | global_auc: 0.987847051578093| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 28 | global_acc: 94.880% | global_loss: 0.1404051035642624 | global_f1: 0.9294871794871795 | global_precision: 0.9441860465116279 | global_recall: 0.915238954012624 | global_auc: 0.9882805766976213| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 29 | global_acc: 95.312% | global_loss: 0.1328512728214264 | global_f1: 0.9354099862574438 | global_precision: 0.9506517690875232 | global_recall: 0.9206492335437331 | global_auc: 0.9891139135922232| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 30 | global_acc: 95.346% | global_loss: 0.12800346314907074 | global_f1: 0.9358386801099908 | global_precision: 0.9515377446411929 | global_recall: 0.9206492335437331 | global_auc: 0.9894676662910715| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 31 | global_acc: 95.379% | global_loss: 0.12235099822282791 | global_f1: 0.9364426154549611 | global_precision: 0.9499072356215214 | global_recall: 0.9233543733092876 | global_auc: 0.9901243642541682| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 32 | global_acc: 95.745% | global_loss: 0.11854556947946548 | global_f1: 0.9417652411282985 | global_precision: 0.9504132231404959 | global_recall: 0.933273219116321 | global_auc: 0.9906082219724586| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 33 | global_acc: 96.011% | global_loss: 0.11449389904737473 | global_f1: 0.9455040871934605 | global_precision: 0.9524245196706312 | global_recall: 0.9386834986474302 | global_auc: 0.9910882810040498| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 34 | global_acc: 96.077% | global_loss: 0.11035672575235367 | global_f1: 0.9464123524069028 | global_precision: 0.9533394327538883 | global_recall: 0.939585211902615 | global_auc: 0.9915445982437722| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 35 | global_acc: 96.243% | global_loss: 0.10849253833293915 | global_f1: 0.9485193621867881 | global_precision: 0.9585635359116023 | global_recall: 0.9386834986474302 | global_auc: 0.9916148739477045| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 36 | global_acc: 96.243% | global_loss: 0.10547599196434021 | global_f1: 0.9485662266727357 | global_precision: 0.9577205882352942 | global_recall: 0.939585211902615 | global_auc: 0.9919605544373172| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 37 | global_acc: 96.376% | global_loss: 0.10248371213674545 | global_f1: 0.9505220154334999 | global_precision: 0.9570383912248629 | global_recall: 0.9440937781785392 | global_auc: 0.9922350095513229| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 38 | global_acc: 96.509% | global_loss: 0.09985833615064621 | global_f1: 0.9523377212891512 | global_precision: 0.9588665447897623 | global_recall: 0.9458972046889089 | global_auc: 0.9925189613820763| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 39 | global_acc: 96.543% | global_loss: 0.09778707474470139 | global_f1: 0.9528130671506352 | global_precision: 0.958904109589041 | global_recall: 0.9467989179440938 | global_auc: 0.992733587180572| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 40 | global_acc: 96.709% | global_loss: 0.09630746394395828 | global_f1: 0.9551427276846398 | global_precision: 0.9599271402550091 | global_recall: 0.9504057709648331 | global_auc: 0.9928774624392982| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 41 | global_acc: 96.742% | global_loss: 0.09634853899478912 | global_f1: 0.9554950045413261 | global_precision: 0.9624885635864593 | global_recall: 0.9486023444544635 | global_auc: 0.9928560948266161| flobal_FPR: 0.05139765554553652 \n",
      "comm_round: 42 | global_acc: 97.008% | global_loss: 0.09299751371145248 | global_f1: 0.959202175883953 | global_precision: 0.9644484958979034 | global_recall: 0.9540126239855726 | global_auc: 0.9932749000351854| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 43 | global_acc: 97.108% | global_loss: 0.0911208838224411 | global_f1: 0.9605799728137744 | global_precision: 0.9653916211293261 | global_recall: 0.9558160504959423 | global_auc: 0.9934990225504288| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 44 | global_acc: 97.108% | global_loss: 0.09014249593019485 | global_f1: 0.9605442176870748 | global_precision: 0.9662408759124088 | global_recall: 0.9549143372407575 | global_auc: 0.9935583770301013| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 45 | global_acc: 97.108% | global_loss: 0.08993327617645264 | global_f1: 0.9605442176870748 | global_precision: 0.9662408759124088 | global_recall: 0.9549143372407575 | global_auc: 0.9935778452994338| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 46 | global_acc: 97.141% | global_loss: 0.08854517340660095 | global_f1: 0.9610154125113328 | global_precision: 0.9662716499544212 | global_recall: 0.9558160504959423 | global_auc: 0.9937730028285972| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 47 | global_acc: 97.174% | global_loss: 0.08837039768695831 | global_f1: 0.9614512471655328 | global_precision: 0.9671532846715328 | global_recall: 0.9558160504959423 | global_auc: 0.993786773067881| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 48 | global_acc: 97.174% | global_loss: 0.08748707920312881 | global_f1: 0.9614162505674082 | global_precision: 0.9680073126142597 | global_recall: 0.9549143372407575 | global_auc: 0.9939410947150297| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 49 | global_acc: 97.141% | global_loss: 0.08543971925973892 | global_f1: 0.9609800362976407 | global_precision: 0.9671232876712329 | global_recall: 0.9549143372407575 | global_auc: 0.9941861100071179| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 50 | global_acc: 97.141% | global_loss: 0.08536562323570251 | global_f1: 0.9609800362976407 | global_precision: 0.9671232876712329 | global_recall: 0.9549143372407575 | global_auc: 0.9941766132903702| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 51 | global_acc: 97.141% | global_loss: 0.08493708074092865 | global_f1: 0.9609800362976407 | global_precision: 0.9671232876712329 | global_recall: 0.9549143372407575 | global_auc: 0.9942117511423363| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 52 | global_acc: 97.108% | global_loss: 0.08418236672878265 | global_f1: 0.9605442176870748 | global_precision: 0.9662408759124088 | global_recall: 0.9549143372407575 | global_auc: 0.9943100421606741| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 53 | global_acc: 97.207% | global_loss: 0.08349285274744034 | global_f1: 0.9619220308250226 | global_precision: 0.96718322698268 | global_recall: 0.9567177637511272 | global_auc: 0.994367497296997| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 54 | global_acc: 97.340% | global_loss: 0.08202432096004486 | global_f1: 0.9637352674524025 | global_precision: 0.9690063810391978 | global_recall: 0.9585211902614968 | global_auc: 0.9945956559168582| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 55 | global_acc: 97.340% | global_loss: 0.0821370929479599 | global_f1: 0.9637352674524025 | global_precision: 0.9690063810391978 | global_recall: 0.9585211902614968 | global_auc: 0.9945465104076894| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 56 | global_acc: 97.374% | global_loss: 0.08157788962125778 | global_f1: 0.9641723356009071 | global_precision: 0.9698905109489051 | global_recall: 0.9585211902614968 | global_auc: 0.9946343550376047| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 57 | global_acc: 97.407% | global_loss: 0.08207488805055618 | global_f1: 0.9646739130434783 | global_precision: 0.9690627843494085 | global_recall: 0.9603246167718665 | global_auc: 0.9945484097510389| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 58 | global_acc: 97.407% | global_loss: 0.08184684813022614 | global_f1: 0.9646739130434783 | global_precision: 0.9690627843494085 | global_recall: 0.9603246167718665 | global_auc: 0.9945479349152014| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 59 | global_acc: 97.440% | global_loss: 0.08085169643163681 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9946896734126593| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 60 | global_acc: 97.407% | global_loss: 0.08079615235328674 | global_f1: 0.9646739130434783 | global_precision: 0.9690627843494085 | global_recall: 0.9603246167718665 | global_auc: 0.9946927598456023| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 61 | global_acc: 97.374% | global_loss: 0.08062499761581421 | global_f1: 0.964204802899864 | global_precision: 0.9690346083788707 | global_recall: 0.9594229035166817 | global_auc: 0.994723624175032| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 62 | global_acc: 97.440% | global_loss: 0.07987358421087265 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9948285628950931| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 63 | global_acc: 97.340% | global_loss: 0.08067774772644043 | global_f1: 0.9637681159420289 | global_precision: 0.9681528662420382 | global_recall: 0.9594229035166817 | global_auc: 0.9947343079813732| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 64 | global_acc: 97.407% | global_loss: 0.07966332882642746 | global_f1: 0.9646739130434783 | global_precision: 0.9690627843494085 | global_recall: 0.9603246167718665 | global_auc: 0.9948618014037097| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 65 | global_acc: 97.407% | global_loss: 0.079810231924057 | global_f1: 0.9646739130434783 | global_precision: 0.9690627843494085 | global_recall: 0.9603246167718665 | global_auc: 0.9948565782094986| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 66 | global_acc: 97.407% | global_loss: 0.07912617176771164 | global_f1: 0.9646739130434783 | global_precision: 0.9690627843494085 | global_recall: 0.9603246167718665 | global_auc: 0.9949515453769746| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 67 | global_acc: 97.540% | global_loss: 0.07761991769075394 | global_f1: 0.9664551223934724 | global_precision: 0.9717411121239745 | global_recall: 0.9612263300270514 | global_auc: 0.9951300836518294| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 68 | global_acc: 97.573% | global_loss: 0.07752478867769241 | global_f1: 0.9669234254644313 | global_precision: 0.9717668488160291 | global_recall: 0.9621280432822362 | global_auc: 0.9951011186657492| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 69 | global_acc: 97.573% | global_loss: 0.0773848295211792 | global_f1: 0.9669234254644313 | global_precision: 0.9717668488160291 | global_recall: 0.9621280432822362 | global_auc: 0.9951229611142688| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 70 | global_acc: 97.573% | global_loss: 0.07735461741685867 | global_f1: 0.9668934240362811 | global_precision: 0.9726277372262774 | global_recall: 0.9612263300270514 | global_auc: 0.9951291339801548| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 71 | global_acc: 97.606% | global_loss: 0.07693246752023697 | global_f1: 0.9673617407071623 | global_precision: 0.9726526891522334 | global_recall: 0.9621280432822362 | global_auc: 0.9951823155939412| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 72 | global_acc: 97.640% | global_loss: 0.07716254144906998 | global_f1: 0.9678296329859537 | global_precision: 0.9726775956284153 | global_recall: 0.9630297565374211 | global_auc: 0.995181840758104| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 73 | global_acc: 97.606% | global_loss: 0.07716989517211914 | global_f1: 0.967391304347826 | global_precision: 0.9717925386715196 | global_recall: 0.9630297565374211 | global_auc: 0.9951865891164777| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 74 | global_acc: 97.673% | global_loss: 0.07602207362651825 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.995335687569415| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 75 | global_acc: 97.706% | global_loss: 0.07569421827793121 | global_f1: 0.9687074829931972 | global_precision: 0.9744525547445255 | global_recall: 0.9630297565374211 | global_auc: 0.995371775093056| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 76 | global_acc: 97.739% | global_loss: 0.0752299576997757 | global_f1: 0.969147005444646 | global_precision: 0.9753424657534246 | global_recall: 0.9630297565374211 | global_auc: 0.995415459990095| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 77 | global_acc: 97.606% | global_loss: 0.07617110759019852 | global_f1: 0.9673617407071623 | global_precision: 0.9726526891522334 | global_recall: 0.9621280432822362 | global_auc: 0.995303636150392| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 78 | global_acc: 97.773% | global_loss: 0.07503221184015274 | global_f1: 0.9696145124716553 | global_precision: 0.9753649635036497 | global_recall: 0.9639314697926059 | global_auc: 0.9954482236628741| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 79 | global_acc: 97.773% | global_loss: 0.07543614506721497 | global_f1: 0.9696145124716553 | global_precision: 0.9753649635036497 | global_recall: 0.9639314697926059 | global_auc: 0.9953888691832016| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 80 | global_acc: 97.806% | global_loss: 0.07521238923072815 | global_f1: 0.9700544464609802 | global_precision: 0.9762557077625571 | global_recall: 0.9639314697926059 | global_auc: 0.9954225825276556| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 81 | global_acc: 97.773% | global_loss: 0.07479643076658249 | global_f1: 0.9696420480289986 | global_precision: 0.9744990892531876 | global_recall: 0.9648331830477908 | global_auc: 0.995492858231588| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 82 | global_acc: 97.806% | global_loss: 0.07502226531505585 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9954477488270366| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 83 | global_acc: 97.739% | global_loss: 0.07571104913949966 | global_f1: 0.9692028985507246 | global_precision: 0.9736123748862603 | global_recall: 0.9648331830477908 | global_auc: 0.995406912945022| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 84 | global_acc: 97.806% | global_loss: 0.07454117387533188 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9955474643528865| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 85 | global_acc: 97.806% | global_loss: 0.07436690479516983 | global_f1: 0.970081595648232 | global_precision: 0.97538742023701 | global_recall: 0.9648331830477908 | global_auc: 0.9955645584430324| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 86 | global_acc: 97.906% | global_loss: 0.07418753206729889 | global_f1: 0.9714544630720434 | global_precision: 0.97632058287796 | global_recall: 0.9666366095581606 | global_auc: 0.9955939982649499| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 87 | global_acc: 97.906% | global_loss: 0.07479578256607056 | global_f1: 0.9714544630720434 | global_precision: 0.97632058287796 | global_recall: 0.9666366095581606 | global_auc: 0.9955417663228381| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 88 | global_acc: 97.906% | global_loss: 0.07479394972324371 | global_f1: 0.9714544630720434 | global_precision: 0.97632058287796 | global_recall: 0.9666366095581606 | global_auc: 0.9955590978309024| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 89 | global_acc: 97.906% | global_loss: 0.07437226921319962 | global_f1: 0.9714544630720434 | global_precision: 0.97632058287796 | global_recall: 0.9666366095581606 | global_auc: 0.9956072936683965| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 90 | global_acc: 97.972% | global_loss: 0.07368214428424835 | global_f1: 0.9723856948845632 | global_precision: 0.9763636363636363 | global_recall: 0.9684400360685302 | global_auc: 0.9956723461781176| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 91 | global_acc: 97.906% | global_loss: 0.07397379726171494 | global_f1: 0.9714544630720434 | global_precision: 0.97632058287796 | global_recall: 0.9666366095581606 | global_auc: 0.9956338844752898| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 92 | global_acc: 97.939% | global_loss: 0.07447320967912674 | global_f1: 0.9718948322756119 | global_precision: 0.9772105742935278 | global_recall: 0.9666366095581606 | global_auc: 0.9956144162059571| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 93 | global_acc: 97.939% | global_loss: 0.07423444837331772 | global_f1: 0.9718948322756119 | global_precision: 0.9772105742935278 | global_recall: 0.9666366095581606 | global_auc: 0.9956462302070616| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 94 | global_acc: 97.939% | global_loss: 0.07401395589113235 | global_f1: 0.9718948322756119 | global_precision: 0.9772105742935278 | global_recall: 0.9666366095581606 | global_auc: 0.9956694971630934| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 95 | global_acc: 97.972% | global_loss: 0.07375159859657288 | global_f1: 0.9723606705935659 | global_precision: 0.9772313296903461 | global_recall: 0.9675383228133454 | global_auc: 0.9956894402682633| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 96 | global_acc: 97.972% | global_loss: 0.07357006520032883 | global_f1: 0.9723606705935659 | global_precision: 0.9772313296903461 | global_recall: 0.9675383228133454 | global_auc: 0.9957003614925231| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 97 | global_acc: 98.005% | global_loss: 0.07409259676933289 | global_f1: 0.9728260869565218 | global_precision: 0.9772520473157416 | global_recall: 0.9684400360685302 | global_auc: 0.9956737706856298| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 98 | global_acc: 98.072% | global_loss: 0.07417875528335571 | global_f1: 0.9737556561085974 | global_precision: 0.9772933696639419 | global_recall: 0.9702434625788999 | global_auc: 0.9956770945364914| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 99 | global_acc: 98.072% | global_loss: 0.07396624982357025 | global_f1: 0.9737556561085974 | global_precision: 0.9772933696639419 | global_recall: 0.9702434625788999 | global_auc: 0.9957008363283604| flobal_FPR: 0.029756537421100092 \n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "\n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation using PGD\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            perturbation = torch.clamp(X_adv - X, -epsilon, epsilon)\n",
    "            X_adv = torch.clamp(X + perturbation, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model, pgd_attack]\n",
<<<<<<< Updated upstream
    "    \n",
    "                    # Randomly choose between normal training and PGD attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "    \n",
=======
    "\n",
    "                    # Randomly choose between normal training and PGD attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
>>>>>>> Stashed changes
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.01\n",
    "                        alpha = 0.01\n",
    "                        num_iter = 5\n",
    "                        X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                        train_model(local_model, X_adv, loss, optimizer)\n",
<<<<<<< Updated upstream
    "    \n",
=======
    "\n",
>>>>>>> Stashed changes
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-PGD-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-PGD-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-PGD-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb1f186",
   "metadata": {},
   "source": [
    "## non-iid fedAVG PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "153b54df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedAvg non iid pgd 2017|\n",
      "|=======================|\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "pgd_attack() missing 2 required positional arguments: 'loss_fn' and 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 171\u001b[0m\n\u001b[0;32m    169\u001b[0m     alpha \u001b[39m=\u001b[39m \u001b[39m0.01\u001b[39m   \u001b[39m# Set your desired value for alpha\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     num_iter \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m   \u001b[39m# Set your desired number of iterations\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m     X_adv \u001b[39m=\u001b[39m pgd_attack(local_model, clients_batched[client]\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mtensors[\u001b[39m0\u001b[39;49m], clients_batched[client]\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49mtensors[\u001b[39m1\u001b[39;49m], epsilon, alpha, num_iter)\n\u001b[0;32m    172\u001b[0m     train_model(local_model, DataLoader(TensorDataset(X_adv, clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m1\u001b[39m])),\n\u001b[0;32m    173\u001b[0m                 loss, optimizer)\n\u001b[0;32m    174\u001b[0m \u001b[39m# train_model(local_model, train_loader, loss, optimizer)\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39m# # FGSM attack\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[39m# epsilon = 1\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    179\u001b[0m \n\u001b[0;32m    180\u001b[0m \u001b[39m# scale the model weights and add to the list\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: pgd_attack() missing 2 required positional arguments: 'loss_fn' and 'optimizer'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        # print(\"adver\" , loss)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "# for d in range(0,1):\n",
    "for d in range(0, 2):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg non iid pgd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "                    selected_training_approach = random.choice([train_model, pgd_attack])\n",
    "\n",
    "                    # List of training approaches    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model:\n",
    "                        train_model(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.1  # Set your desired value for epsilon\n",
    "                        alpha = 0.01   # Set your desired value for alpha\n",
    "                        num_iter = 10   # Set your desired number of iterations\n",
    "                        X_adv = pgd_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter)\n",
    "                        train_model(local_model, DataLoader(TensorDataset(X_adv, clients_batched[client].dataset.tensors[1])),\n",
    "                                    loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedAvg-non-iid-pgd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-non-iid-pgd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-non-iid-pgd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
