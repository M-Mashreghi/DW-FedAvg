{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f200b28",
   "metadata": {},
   "source": [
    "## fedADMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedADMM 2022|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6550667881965637 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8728237680028073| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6200079321861267 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9168529210238789| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.5447004437446594 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9440097322353229| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 84.674% | global_loss: 0.37596794962882996 | global_f1: 0.7388101983002833 | global_precision: 0.9939024390243902 | global_recall: 0.587917042380523 | global_auc: 0.9752572541857966| flobal_FPR: 0.412082957619477 \n",
      "comm_round: 4 | global_acc: 94.614% | global_loss: 0.1638631820678711 | global_f1: 0.9282550930026573 | global_precision: 0.9120974760661444 | global_recall: 0.9449954914337241 | global_auc: 0.9861452399369228| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 5 | global_acc: 94.814% | global_loss: 0.15104158222675323 | global_f1: 0.9301700984780662 | global_precision: 0.9235555555555556 | global_recall: 0.9368800721370604 | global_auc: 0.9890151477380482| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 6 | global_acc: 96.144% | global_loss: 0.14919251203536987 | global_f1: 0.9481680071492404 | global_precision: 0.9397697077059345 | global_recall: 0.9567177637511272 | global_auc: 0.9928321156168284| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 7 | global_acc: 97.108% | global_loss: 0.1755663901567459 | global_f1: 0.9609690444145357 | global_precision: 0.95625 | global_recall: 0.9657348963029756 | global_auc: 0.9945287040637875| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 8 | global_acc: 97.241% | global_loss: 0.20013292133808136 | global_f1: 0.9626966292134831 | global_precision: 0.9596774193548387 | global_recall: 0.9657348963029756 | global_auc: 0.9951514512645117| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 9 | global_acc: 97.739% | global_loss: 0.23039428889751434 | global_f1: 0.9694244604316546 | global_precision: 0.9668161434977578 | global_recall: 0.9720468890892696 | global_auc: 0.9951191624275698| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 10 | global_acc: 97.806% | global_loss: 0.23767158389091492 | global_f1: 0.9702970297029703 | global_precision: 0.9685534591194969 | global_recall: 0.9720468890892696 | global_auc: 0.9947041559056996| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 11 | global_acc: 98.005% | global_loss: 0.24361154437065125 | global_f1: 0.9728506787330317 | global_precision: 0.9763851044504995 | global_recall: 0.9693417493237151 | global_auc: 0.9947962740581512| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 12 | global_acc: 97.872% | global_loss: 0.24852080643177032 | global_f1: 0.9711451758340848 | global_precision: 0.9711451758340848 | global_recall: 0.9711451758340848 | global_auc: 0.994922105555057| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 13 | global_acc: 97.939% | global_loss: 0.2586628794670105 | global_f1: 0.9718181818181818 | global_precision: 0.9798350137488543 | global_recall: 0.9639314697926059 | global_auc: 0.9951372061893902| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 14 | global_acc: 97.872% | global_loss: 0.26533499360084534 | global_f1: 0.9711191335740074 | global_precision: 0.971996386630533 | global_recall: 0.9702434625788999 | global_auc: 0.9951519261003489| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 15 | global_acc: 98.072% | global_loss: 0.2751050889492035 | global_f1: 0.9736842105263159 | global_precision: 0.9799086757990868 | global_recall: 0.9675383228133454 | global_auc: 0.9953907685265511| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 16 | global_acc: 98.005% | global_loss: 0.31001004576683044 | global_f1: 0.972972972972973 | global_precision: 0.9720972097209721 | global_recall: 0.9738503155996393 | global_auc: 0.9950925716206764| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 17 | global_acc: 98.338% | global_loss: 0.32251277565956116 | global_f1: 0.9773960216998192 | global_precision: 0.9800543970988214 | global_recall: 0.9747520288548241 | global_auc: 0.995080700724742| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 18 | global_acc: 97.773% | global_loss: 0.35581547021865845 | global_f1: 0.9693924166285974 | global_precision: 0.9824074074074074 | global_recall: 0.9567177637511272 | global_auc: 0.9950745278588561| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 19 | global_acc: 97.939% | global_loss: 0.4514455497264862 | global_f1: 0.9722966934763183 | global_precision: 0.9636846767050488 | global_recall: 0.9810640216411182 | global_auc: 0.9941070498401939| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 20 | global_acc: 98.172% | global_loss: 0.4150800406932831 | global_f1: 0.9750566893424036 | global_precision: 0.9808394160583942 | global_recall: 0.9693417493237151 | global_auc: 0.9945783244087938| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 21 | global_acc: 98.105% | global_loss: 0.5372880697250366 | global_f1: 0.9744279946164199 | global_precision: 0.9696428571428571 | global_recall: 0.9792605951307484 | global_auc: 0.9935002096400223| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 22 | global_acc: 97.939% | global_loss: 0.5545604825019836 | global_f1: 0.9722966934763183 | global_precision: 0.9636846767050488 | global_recall: 0.9810640216411182 | global_auc: 0.9935021089833718| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 23 | global_acc: 98.039% | global_loss: 0.5108946561813354 | global_f1: 0.9731207289293848 | global_precision: 0.9834254143646409 | global_recall: 0.9630297565374211 | global_auc: 0.9941877719325487| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 24 | global_acc: 98.205% | global_loss: 0.6329824328422546 | global_f1: 0.9757630161579892 | global_precision: 0.9714030384271671 | global_recall: 0.9801623083859333 | global_auc: 0.9931853934798393| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 25 | global_acc: 98.338% | global_loss: 0.5682545304298401 | global_f1: 0.9773755656108598 | global_precision: 0.9809264305177112 | global_recall: 0.9738503155996393 | global_auc: 0.9939142664902177| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 26 | global_acc: 98.504% | global_loss: 0.7069018483161926 | global_f1: 0.9797388563710041 | global_precision: 0.9784172661870504 | global_recall: 0.9810640216411182 | global_auc: 0.9927319252551412| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 27 | global_acc: 98.371% | global_loss: 0.677253246307373 | global_f1: 0.9779179810725552 | global_precision: 0.9774774774774775 | global_recall: 0.9783588818755635 | global_auc: 0.992886009484371| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 28 | global_acc: 98.471% | global_loss: 0.7509725689888 | global_f1: 0.9792605951307484 | global_precision: 0.9792605951307484 | global_recall: 0.9792605951307484 | global_auc: 0.9924384767076403| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 29 | global_acc: 98.404% | global_loss: 0.8189578056335449 | global_f1: 0.9784172661870504 | global_precision: 0.9757847533632287 | global_recall: 0.9810640216411182 | global_auc: 0.9918817316883121| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 30 | global_acc: 98.338% | global_loss: 0.8444922566413879 | global_f1: 0.9774368231046933 | global_precision: 0.978319783197832 | global_recall: 0.9765554553651938 | global_auc: 0.991604427559282| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 31 | global_acc: 98.404% | global_loss: 0.8837248682975769 | global_f1: 0.9783783783783783 | global_precision: 0.9774977497749775 | global_recall: 0.9792605951307484 | global_auc: 0.9912117383217687| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 32 | global_acc: 98.438% | global_loss: 0.8704513311386108 | global_f1: 0.9787810383747179 | global_precision: 0.9801084990958409 | global_recall: 0.9774571686203787 | global_auc: 0.9911865720223876| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 33 | global_acc: 98.404% | global_loss: 0.9114887714385986 | global_f1: 0.9783978397839784 | global_precision: 0.9766397124887691 | global_recall: 0.9801623083859333 | global_auc: 0.9909377580436004| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 34 | global_acc: 98.404% | global_loss: 0.9532151818275452 | global_f1: 0.9784366576819408 | global_precision: 0.9749328558639212 | global_recall: 0.981965734896303 | global_auc: 0.9907046136474468| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 35 | global_acc: 98.371% | global_loss: 0.9348495006561279 | global_f1: 0.9779577147998201 | global_precision: 0.9757630161579892 | global_recall: 0.9801623083859333 | global_auc: 0.9906134451666698| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 36 | global_acc: 98.371% | global_loss: 1.0171170234680176 | global_f1: 0.9779775280898877 | global_precision: 0.974910394265233 | global_recall: 0.9810640216411182 | global_auc: 0.9896777811491122| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 37 | global_acc: 98.371% | global_loss: 0.9531753063201904 | global_f1: 0.9778781038374718 | global_precision: 0.9792043399638336 | global_recall: 0.9765554553651938 | global_auc: 0.989949862083931| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 38 | global_acc: 98.138% | global_loss: 0.9921026825904846 | global_f1: 0.974820143884892 | global_precision: 0.9721973094170404 | global_recall: 0.9774571686203787 | global_auc: 0.9897658631969463| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 39 | global_acc: 98.305% | global_loss: 1.0094207525253296 | global_f1: 0.9769751693002258 | global_precision: 0.9783001808318263 | global_recall: 0.975653742110009 | global_auc: 0.9895512373984504| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 40 | global_acc: 98.271% | global_loss: 1.012895941734314 | global_f1: 0.9765342960288809 | global_precision: 0.9774164408310749 | global_recall: 0.975653742110009 | global_auc: 0.9896091673706109| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 41 | global_acc: 98.205% | global_loss: 1.0438004732131958 | global_f1: 0.975653742110009 | global_precision: 0.975653742110009 | global_recall: 0.975653742110009 | global_auc: 0.9894819113661929| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 42 | global_acc: 98.172% | global_loss: 1.0328433513641357 | global_f1: 0.975191700496166 | global_precision: 0.9756317689530686 | global_recall: 0.9747520288548241 | global_auc: 0.9896162899081713| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 43 | global_acc: 98.172% | global_loss: 1.1256699562072754 | global_f1: 0.975191700496166 | global_precision: 0.9756317689530686 | global_recall: 0.9747520288548241 | global_auc: 0.988853703553339| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 44 | global_acc: 98.072% | global_loss: 1.0985379219055176 | global_f1: 0.9738973897389739 | global_precision: 0.97214734950584 | global_recall: 0.975653742110009 | global_auc: 0.9892708468364775| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 45 | global_acc: 98.238% | global_loss: 1.135475993156433 | global_f1: 0.9760938204781237 | global_precision: 0.9765342960288809 | global_recall: 0.975653742110009 | global_auc: 0.9890301050669258| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 46 | global_acc: 98.271% | global_loss: 1.1201921701431274 | global_f1: 0.9765342960288809 | global_precision: 0.9774164408310749 | global_recall: 0.975653742110009 | global_auc: 0.9892157658793413| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 47 | global_acc: 98.172% | global_loss: 1.2536734342575073 | global_f1: 0.9752808988764045 | global_precision: 0.9722222222222222 | global_recall: 0.9783588818755635 | global_auc: 0.987663290109027| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 48 | global_acc: 98.238% | global_loss: 1.227055311203003 | global_f1: 0.9761583445793971 | global_precision: 0.973967684021544 | global_recall: 0.9783588818755635 | global_auc: 0.9881091609603269| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 49 | global_acc: 98.205% | global_loss: 1.2165982723236084 | global_f1: 0.9756975697569756 | global_precision: 0.9739442946990117 | global_recall: 0.9774571686203787 | global_auc: 0.9879018951173105| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 50 | global_acc: 98.238% | global_loss: 1.2804005146026611 | global_f1: 0.9761368752814047 | global_precision: 0.9748201438848921 | global_recall: 0.9774571686203787 | global_auc: 0.9878292452341914| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 51 | global_acc: 98.305% | global_loss: 1.1958037614822388 | global_f1: 0.9770166741775576 | global_precision: 0.9765765765765766 | global_recall: 0.9774571686203787 | global_auc: 0.9881711270371051| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 52 | global_acc: 98.438% | global_loss: 1.1249182224273682 | global_f1: 0.9788001804239964 | global_precision: 0.9792418772563177 | global_recall: 0.9783588818755635 | global_auc: 0.9887259727130837| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 53 | global_acc: 98.271% | global_loss: 1.2551161050796509 | global_f1: 0.9765976597659766 | global_precision: 0.9748427672955975 | global_recall: 0.9783588818755635 | global_auc: 0.9876708874824252| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 54 | global_acc: 98.271% | global_loss: 1.317234992980957 | global_f1: 0.9765765765765766 | global_precision: 0.9756975697569757 | global_recall: 0.9774571686203787 | global_auc: 0.987219081183158| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 55 | global_acc: 98.172% | global_loss: 1.4140607118606567 | global_f1: 0.9752808988764045 | global_precision: 0.9722222222222222 | global_recall: 0.9783588818755635 | global_auc: 0.9865051655016569| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 56 | global_acc: 98.205% | global_loss: 1.4461729526519775 | global_f1: 0.9756975697569756 | global_precision: 0.9739442946990117 | global_recall: 0.9774571686203787 | global_auc: 0.9859951918123107| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 57 | global_acc: 95.180% | global_loss: 1.8950549364089966 | global_f1: 0.9375807145931984 | global_precision: 0.8970345963756178 | global_recall: 0.981965734896303 | global_auc: 0.9830723398153174| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 58 | global_acc: 97.906% | global_loss: 1.797590732574463 | global_f1: 0.9718372820742065 | global_precision: 0.9636524822695035 | global_recall: 0.9801623083859333 | global_auc: 0.9835284196371209| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 59 | global_acc: 97.806% | global_loss: 1.7165474891662598 | global_f1: 0.9704035874439463 | global_precision: 0.9652096342551294 | global_recall: 0.975653742110009 | global_auc: 0.9837869677505744| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 60 | global_acc: 98.005% | global_loss: 1.5386216640472412 | global_f1: 0.972972972972973 | global_precision: 0.9720972097209721 | global_recall: 0.9738503155996393 | global_auc: 0.984128849553488| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 61 | global_acc: 98.271% | global_loss: 1.515084147453308 | global_f1: 0.9766606822262118 | global_precision: 0.9722966934763181 | global_recall: 0.9810640216411182 | global_auc: 0.9855281907662474| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 62 | global_acc: 98.238% | global_loss: 1.4982528686523438 | global_f1: 0.9762011674898968 | global_precision: 0.9722719141323792 | global_recall: 0.9801623083859333 | global_auc: 0.985653310009397| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 63 | global_acc: 98.271% | global_loss: 1.4665671586990356 | global_f1: 0.9766606822262118 | global_precision: 0.9722966934763181 | global_recall: 0.9810640216411182 | global_auc: 0.985716938011606| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 64 | global_acc: 98.238% | global_loss: 1.4382952451705933 | global_f1: 0.9762011674898968 | global_precision: 0.9722719141323792 | global_recall: 0.9801623083859333 | global_auc: 0.9859766732146529| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 65 | global_acc: 98.338% | global_loss: 1.4006792306900024 | global_f1: 0.977538185085355 | global_precision: 0.9740376007162042 | global_recall: 0.9810640216411182 | global_auc: 0.986228573626383| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 66 | global_acc: 98.271% | global_loss: 1.4280683994293213 | global_f1: 0.9765765765765766 | global_precision: 0.9756975697569757 | global_recall: 0.9774571686203787 | global_auc: 0.9855820846337902| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 67 | global_acc: 98.138% | global_loss: 1.583463191986084 | global_f1: 0.9748653500897666 | global_precision: 0.9705093833780161 | global_recall: 0.9792605951307484 | global_auc: 0.9843024020520506| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 68 | global_acc: 98.172% | global_loss: 1.4780056476593018 | global_f1: 0.975236380009005 | global_precision: 0.9739208633093526 | global_recall: 0.9765554553651938 | global_auc: 0.9847347400819851| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 69 | global_acc: 98.039% | global_loss: 1.5676652193069458 | global_f1: 0.9734831460674157 | global_precision: 0.9704301075268817 | global_recall: 0.9765554553651938 | global_auc: 0.9841891537048354| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 70 | global_acc: 98.105% | global_loss: 1.6328911781311035 | global_f1: 0.9742895805142084 | global_precision: 0.9747292418772563 | global_recall: 0.9738503155996393 | global_auc: 0.9830440870829933| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 71 | global_acc: 97.972% | global_loss: 1.6200847625732422 | global_f1: 0.9725348941918055 | global_precision: 0.9712230215827338 | global_recall: 0.9738503155996393 | global_auc: 0.9835072894423575| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 72 | global_acc: 98.039% | global_loss: 1.674782395362854 | global_f1: 0.9733152419719584 | global_precision: 0.9764065335753176 | global_recall: 0.9702434625788999 | global_auc: 0.9820284132268371| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 73 | global_acc: 97.939% | global_loss: 1.5569896697998047 | global_f1: 0.9722719141323792 | global_precision: 0.9645075421472937 | global_recall: 0.9801623083859333 | global_auc: 0.9844826022523363| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 74 | global_acc: 98.238% | global_loss: 1.581250786781311 | global_f1: 0.9762011674898968 | global_precision: 0.9722719141323792 | global_recall: 0.9801623083859333 | global_auc: 0.9839476996815276| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 75 | global_acc: 98.172% | global_loss: 1.6093803644180298 | global_f1: 0.9753252579632121 | global_precision: 0.9705357142857143 | global_recall: 0.9801623083859333 | global_auc: 0.9837029218073583| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 76 | global_acc: 98.305% | global_loss: 1.522903323173523 | global_f1: 0.9770992366412213 | global_precision: 0.9731663685152058 | global_recall: 0.9810640216411182 | global_auc: 0.9847060125138237| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 77 | global_acc: 98.238% | global_loss: 1.5780895948410034 | global_f1: 0.9762438368444645 | global_precision: 0.9705882352941176 | global_recall: 0.981965734896303 | global_auc: 0.9844685945951337| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 78 | global_acc: 98.338% | global_loss: 1.5121119022369385 | global_f1: 0.9774977497749775 | global_precision: 0.9757412398921833 | global_recall: 0.9792605951307484 | global_auc: 0.9844550617737683| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 79 | global_acc: 98.371% | global_loss: 1.536832571029663 | global_f1: 0.9779775280898877 | global_precision: 0.974910394265233 | global_recall: 0.9810640216411182 | global_auc: 0.9842639403492228| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 80 | global_acc: 98.105% | global_loss: 1.6007075309753418 | global_f1: 0.9744050291872474 | global_precision: 0.9704830053667263 | global_recall: 0.9783588818755635 | global_auc: 0.9839723911450714| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 81 | global_acc: 98.238% | global_loss: 1.6126623153686523 | global_f1: 0.9761583445793971 | global_precision: 0.973967684021544 | global_recall: 0.9783588818755635 | global_auc: 0.9832632238219441| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 82 | global_acc: 98.039% | global_loss: 1.7848398685455322 | global_f1: 0.9734831460674157 | global_precision: 0.9704301075268817 | global_recall: 0.9765554553651938 | global_auc: 0.9814956474172967| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 83 | global_acc: 98.205% | global_loss: 1.5847502946853638 | global_f1: 0.9756317689530687 | global_precision: 0.976513098464318 | global_recall: 0.9747520288548241 | global_auc: 0.9829220542727866| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 84 | global_acc: 98.205% | global_loss: 1.5987701416015625 | global_f1: 0.9755434782608696 | global_precision: 0.9799818016378526 | global_recall: 0.9711451758340848 | global_auc: 0.9831851133266951| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 85 | global_acc: 98.205% | global_loss: 1.602827548980713 | global_f1: 0.9755434782608696 | global_precision: 0.9799818016378526 | global_recall: 0.9711451758340848 | global_auc: 0.9827299831765662| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 86 | global_acc: 98.105% | global_loss: 1.685718297958374 | global_f1: 0.9742198100407056 | global_precision: 0.9773139745916516 | global_recall: 0.9711451758340848 | global_auc: 0.9819697710009206| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 87 | global_acc: 98.005% | global_loss: 1.725478172302246 | global_f1: 0.97289972899729 | global_precision: 0.9746606334841629 | global_recall: 0.9711451758340848 | global_auc: 0.9808660150969306| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 88 | global_acc: 94.714% | global_loss: 2.333421468734741 | global_f1: 0.9312581063553826 | global_precision: 0.8945182724252492 | global_recall: 0.9711451758340848 | global_auc: 0.9777539410187414| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 89 | global_acc: 97.573% | global_loss: 2.100032329559326 | global_f1: 0.9671022983325823 | global_precision: 0.9666666666666667 | global_recall: 0.9675383228133454 | global_auc: 0.9777145296442388| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 90 | global_acc: 97.606% | global_loss: 2.1819286346435547 | global_f1: 0.9674796747967479 | global_precision: 0.9692307692307692 | global_recall: 0.9657348963029756 | global_auc: 0.9772629607628902| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 91 | global_acc: 97.640% | global_loss: 2.2441251277923584 | global_f1: 0.9681471511888738 | global_precision: 0.9633928571428572 | global_recall: 0.9729486023444545 | global_auc: 0.9765913054709161| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 92 | global_acc: 97.706% | global_loss: 2.2541708946228027 | global_f1: 0.9688487584650113 | global_precision: 0.9701627486437613 | global_recall: 0.9675383228133454 | global_auc: 0.9751758198396859| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 93 | global_acc: 97.307% | global_loss: 2.6601624488830566 | global_f1: 0.9631650750341064 | global_precision: 0.9715596330275229 | global_recall: 0.9549143372407575 | global_auc: 0.969475178194019| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 94 | global_acc: 96.376% | global_loss: 3.6236987113952637 | global_f1: 0.9497464269248502 | global_precision: 0.9716981132075472 | global_recall: 0.9287646528403968 | global_auc: 0.9564459202342269| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 95 | global_acc: 96.277% | global_loss: 3.7015380859375 | global_f1: 0.9481481481481482 | global_precision: 0.9743101807802094 | global_recall: 0.9233543733092876 | global_auc: 0.9549527989435852| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 96 | global_acc: 96.277% | global_loss: 3.7082858085632324 | global_f1: 0.948196114708603 | global_precision: 0.97340930674264 | global_recall: 0.9242560865644724 | global_auc: 0.9551059335011403| flobal_FPR: 0.0757439134355275 \n",
      "comm_round: 97 | global_acc: 91.955% | global_loss: 8.015168190002441 | global_f1: 0.893859649122807 | global_precision: 0.8701964133219471 | global_recall: 0.9188458070333634 | global_auc: 0.9193232544678491| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 98 | global_acc: 95.545% | global_loss: 4.454787254333496 | global_f1: 0.9404973357015985 | global_precision: 0.926509186351706 | global_recall: 0.9549143372407575 | global_auc: 0.9552927814031495| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 99 | global_acc: 96.177% | global_loss: 3.8231382369995117 | global_f1: 0.9483610237988326 | global_precision: 0.9445438282647585 | global_recall: 0.9522091974752029 | global_auc: 0.9596857251526717| flobal_FPR: 0.047790802524797116 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain\n",
    "import copy\n",
    "\n",
    "def train_model_prox(model,global_model, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    for i in range(2):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Add the proximal term\n",
    "            # for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            #     loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_model.parameters())):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_model.parameters()))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "    # temp2 = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    # Update x_hats\n",
    "    # for x_hat, z_weight, param_model in zip(x_hats[int(client[-1])-1],(Z_weights[int(client[-1])-1], local_model.parameters()) ):\n",
    "    #     x_hat = param_model +  z_weight/rho\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "    # print(x_hats[int(client[-1])-1])\n",
    "    # print(delta_x_hats[int(client[-1])-1])\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n",
    "\n",
    "\n",
    "\n",
    "# def update_avg_with_delta(avg_grad, delta_x_hats):\n",
    "#     '''Update avg_grad by adding the corresponding elements from delta_x_hats'''\n",
    "#     for avg_layer, delta_layers in zip(avg_grad, zip(*delta_x_hats)):\n",
    "#         avg_layer += delta_layers\n",
    "#     return avg_grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM 2022|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                    Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "   \n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5f58d3",
   "metadata": {},
   "source": [
    "## fedadmm nonidd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f343ccec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedADMM-nonidd 2022|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.5069310665130615 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9400272840672159| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 94.914% | global_loss: 0.15528902411460876 | global_f1: 0.929912963811269 | global_precision: 0.9450651769087524 | global_recall: 0.915238954012624 | global_auc: 0.9874339443995723| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 2 | global_acc: 96.310% | global_loss: 0.1520184725522995 | global_f1: 0.9502910882221226 | global_precision: 0.943950177935943 | global_recall: 0.9567177637511272 | global_auc: 0.9928765127676235| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 3 | global_acc: 97.340% | global_loss: 0.17797482013702393 | global_f1: 0.9640287769784173 | global_precision: 0.9614349775784753 | global_recall: 0.9666366095581606 | global_auc: 0.9951842149372909| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 4 | global_acc: 97.507% | global_loss: 0.1797630488872528 | global_f1: 0.9665626393223361 | global_precision: 0.9559082892416225 | global_recall: 0.9774571686203787 | global_auc: 0.9957632772409759| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 5 | global_acc: 97.540% | global_loss: 0.18787680566310883 | global_f1: 0.9665158371040724 | global_precision: 0.9700272479564033 | global_recall: 0.9630297565374211 | global_auc: 0.9957844074357391| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 6 | global_acc: 97.739% | global_loss: 0.2425762563943863 | global_f1: 0.9690346083788706 | global_precision: 0.9788408463661453 | global_recall: 0.9594229035166817 | global_auc: 0.9957720617039673| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 7 | global_acc: 97.906% | global_loss: 0.25302553176879883 | global_f1: 0.9715318572074108 | global_precision: 0.9737318840579711 | global_recall: 0.9693417493237151 | global_auc: 0.9954007400791361| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 8 | global_acc: 97.972% | global_loss: 0.3150160014629364 | global_f1: 0.9723606705935659 | global_precision: 0.9772313296903461 | global_recall: 0.9675383228133454 | global_auc: 0.9950954206357008| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 9 | global_acc: 97.906% | global_loss: 0.3350538909435272 | global_f1: 0.9715832205683356 | global_precision: 0.9720216606498195 | global_recall: 0.9711451758340848 | global_auc: 0.9951922871465263| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 10 | global_acc: 93.783% | global_loss: 0.694635272026062 | global_f1: 0.9208633093525179 | global_precision: 0.8676236044657097 | global_recall: 0.9810640216411182 | global_auc: 0.9932464098849425| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 11 | global_acc: 98.072% | global_loss: 0.4979532063007355 | global_f1: 0.9736363636363635 | global_precision: 0.9816681943171403 | global_recall: 0.9657348963029756 | global_auc: 0.9945526832735752| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 12 | global_acc: 98.072% | global_loss: 0.6767432689666748 | global_f1: 0.9740143369175627 | global_precision: 0.9679430097951914 | global_recall: 0.9801623083859333 | global_auc: 0.9928477851994618| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 13 | global_acc: 98.271% | global_loss: 0.7087454199790955 | global_f1: 0.9766187050359711 | global_precision: 0.9739910313901345 | global_recall: 0.9792605951307484 | global_auc: 0.9928518213040797| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 14 | global_acc: 98.305% | global_loss: 0.7289590835571289 | global_f1: 0.9769126301493889 | global_precision: 0.980909090909091 | global_recall: 0.9729486023444545 | global_auc: 0.9931787457781157| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 15 | global_acc: 98.338% | global_loss: 0.803904116153717 | global_f1: 0.9775784753363229 | global_precision: 0.9723461195361285 | global_recall: 0.9828674481514879 | global_auc: 0.9925346309647097| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 16 | global_acc: 98.305% | global_loss: 0.8201513886451721 | global_f1: 0.9771197846567968 | global_precision: 0.9723214285714286 | global_recall: 0.981965734896303 | global_auc: 0.9925923235189515| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 17 | global_acc: 98.404% | global_loss: 0.8960158228874207 | global_f1: 0.9784366576819408 | global_precision: 0.9749328558639212 | global_recall: 0.981965734896303 | global_auc: 0.9919135456894166| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 18 | global_acc: 98.105% | global_loss: 0.9454540610313416 | global_f1: 0.9745649263721553 | global_precision: 0.9646643109540636 | global_recall: 0.9846708746618575 | global_auc: 0.9917039056672132| flobal_FPR: 0.015329125338142471 \n",
      "comm_round: 19 | global_acc: 98.205% | global_loss: 0.9959613680839539 | global_f1: 0.9758497316636852 | global_precision: 0.9680567879325643 | global_recall: 0.9837691614066727 | global_auc: 0.9906305392568154| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 20 | global_acc: 98.338% | global_loss: 0.9646854996681213 | global_f1: 0.9774774774774774 | global_precision: 0.9765976597659766 | global_recall: 0.9783588818755635 | global_auc: 0.9907354779768764| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 21 | global_acc: 98.105% | global_loss: 0.9908761382102966 | global_f1: 0.9745194456861869 | global_precision: 0.9663120567375887 | global_recall: 0.9828674481514879 | global_auc: 0.9904610228628707| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 22 | global_acc: 98.271% | global_loss: 1.0433158874511719 | global_f1: 0.9767025089605735 | global_precision: 0.9706144256455922 | global_recall: 0.9828674481514879 | global_auc: 0.990240936452245| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 23 | global_acc: 98.371% | global_loss: 0.985927164554596 | global_f1: 0.978036754818467 | global_precision: 0.9723707664884136 | global_recall: 0.9837691614066727 | global_auc: 0.9907259812601289| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 24 | global_acc: 98.271% | global_loss: 1.0175046920776367 | global_f1: 0.9767025089605735 | global_precision: 0.9706144256455922 | global_recall: 0.9828674481514879 | global_auc: 0.9904700447437811| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 25 | global_acc: 98.404% | global_loss: 0.9349468946456909 | global_f1: 0.9783783783783783 | global_precision: 0.9774977497749775 | global_recall: 0.9792605951307484 | global_auc: 0.9906091716441333| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 26 | global_acc: 98.271% | global_loss: 1.0023044347763062 | global_f1: 0.9766816143497757 | global_precision: 0.9714540588760036 | global_recall: 0.981965734896303 | global_auc: 0.9903938335918816| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 27 | global_acc: 98.404% | global_loss: 1.0207096338272095 | global_f1: 0.9783393501805054 | global_precision: 0.979223125564589 | global_recall: 0.9774571686203787 | global_auc: 0.9902354758401151| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 28 | global_acc: 98.305% | global_loss: 1.154349446296692 | global_f1: 0.9769751693002258 | global_precision: 0.9783001808318263 | global_recall: 0.975653742110009 | global_auc: 0.9891445405037343| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 29 | global_acc: 98.305% | global_loss: 1.2273738384246826 | global_f1: 0.9770992366412213 | global_precision: 0.9731663685152058 | global_recall: 0.9810640216411182 | global_auc: 0.9886165230525678| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 30 | global_acc: 98.338% | global_loss: 1.2690563201904297 | global_f1: 0.9774571686203787 | global_precision: 0.9774571686203787 | global_recall: 0.9774571686203787 | global_auc: 0.9875082562081224| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 31 | global_acc: 98.172% | global_loss: 1.2694041728973389 | global_f1: 0.975191700496166 | global_precision: 0.9756317689530686 | global_recall: 0.9747520288548241 | global_auc: 0.9869223087847954| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 32 | global_acc: 98.072% | global_loss: 1.509616494178772 | global_f1: 0.9739442946990116 | global_precision: 0.9704565801253358 | global_recall: 0.9774571686203787 | global_auc: 0.9856051141719029| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 33 | global_acc: 98.138% | global_loss: 1.4228440523147583 | global_f1: 0.9747747747747747 | global_precision: 0.9738973897389739 | global_recall: 0.975653742110009 | global_auc: 0.986350369018671| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 34 | global_acc: 97.972% | global_loss: 1.621656894683838 | global_f1: 0.9726334679228353 | global_precision: 0.9678571428571429 | global_recall: 0.9774571686203787 | global_auc: 0.9846666011393211| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 35 | global_acc: 98.105% | global_loss: 1.4142051935195923 | global_f1: 0.974473802060009 | global_precision: 0.9679715302491103 | global_recall: 0.9810640216411182 | global_auc: 0.9861105769207941| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 36 | global_acc: 98.172% | global_loss: 1.3640954494476318 | global_f1: 0.9752586594691857 | global_precision: 0.9730700179533214 | global_recall: 0.9774571686203787 | global_auc: 0.9865972836541086| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 37 | global_acc: 98.271% | global_loss: 1.2441956996917725 | global_f1: 0.976513098464318 | global_precision: 0.9782805429864253 | global_recall: 0.9747520288548241 | global_auc: 0.9867126687625922| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 38 | global_acc: 98.138% | global_loss: 1.4892828464508057 | global_f1: 0.9748653500897666 | global_precision: 0.9705093833780161 | global_recall: 0.9792605951307484 | global_auc: 0.9849823669711788| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 39 | global_acc: 98.172% | global_loss: 1.514345645904541 | global_f1: 0.975191700496166 | global_precision: 0.9756317689530686 | global_recall: 0.9747520288548241 | global_auc: 0.984171822196771| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 40 | global_acc: 97.972% | global_loss: 1.519321084022522 | global_f1: 0.9724106739032112 | global_precision: 0.9754990925589837 | global_recall: 0.9693417493237151 | global_auc: 0.9829187304219249| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 41 | global_acc: 97.806% | global_loss: 1.5688714981079102 | global_f1: 0.9702702702702702 | global_precision: 0.9693969396939695 | global_recall: 0.9711451758340848 | global_auc: 0.9830702030540491| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 42 | global_acc: 98.039% | global_loss: 1.5723377466201782 | global_f1: 0.973339358337099 | global_precision: 0.9755434782608695 | global_recall: 0.9711451758340848 | global_auc: 0.9834690651574485| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 43 | global_acc: 98.039% | global_loss: 1.7802703380584717 | global_f1: 0.9732910819375283 | global_precision: 0.9772727272727273 | global_recall: 0.9693417493237151 | global_auc: 0.9801988707454116| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 44 | global_acc: 98.005% | global_loss: 1.781930685043335 | global_f1: 0.9728506787330317 | global_precision: 0.9763851044504995 | global_recall: 0.9693417493237151 | global_auc: 0.9794797318696993| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 45 | global_acc: 97.773% | global_loss: 1.8305732011795044 | global_f1: 0.9695592912312585 | global_precision: 0.9771062271062271 | global_recall: 0.9621280432822362 | global_auc: 0.979703616967024| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 46 | global_acc: 97.540% | global_loss: 2.0595908164978027 | global_f1: 0.9666666666666666 | global_precision: 0.9657965796579658 | global_recall: 0.9675383228133454 | global_auc: 0.9774391248585583| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 47 | global_acc: 97.640% | global_loss: 1.9263439178466797 | global_f1: 0.9676537585421411 | global_precision: 0.9779005524861878 | global_recall: 0.957619477006312 | global_auc: 0.9790443074068219| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 48 | global_acc: 97.673% | global_loss: 1.9897160530090332 | global_f1: 0.968094804010939 | global_precision: 0.9788018433179724 | global_recall: 0.957619477006312 | global_auc: 0.9779305799502467| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 49 | global_acc: 97.706% | global_loss: 1.9006620645523071 | global_f1: 0.9686790739900136 | global_precision: 0.9753199268738574 | global_recall: 0.9621280432822362 | global_auc: 0.9793641093432972| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 50 | global_acc: 97.374% | global_loss: 2.021571159362793 | global_f1: 0.9640091116173121 | global_precision: 0.9742173112338858 | global_recall: 0.9540126239855726 | global_auc: 0.9760407333174739| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 51 | global_acc: 97.374% | global_loss: 2.066326379776001 | global_f1: 0.9641723356009071 | global_precision: 0.9698905109489051 | global_recall: 0.9585211902614968 | global_auc: 0.9757541698896148| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 52 | global_acc: 97.407% | global_loss: 2.1647768020629883 | global_f1: 0.9644484958979033 | global_precision: 0.9751152073732718 | global_recall: 0.9540126239855726 | global_auc: 0.9738676471077036| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 53 | global_acc: 96.809% | global_loss: 2.554807186126709 | global_f1: 0.9570277529095791 | global_precision: 0.9502222222222222 | global_recall: 0.9639314697926059 | global_auc: 0.9729089535520332| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 54 | global_acc: 96.875% | global_loss: 2.7881975173950195 | global_f1: 0.957619477006312 | global_precision: 0.957619477006312 | global_recall: 0.957619477006312 | global_auc: 0.9695221869419195| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 55 | global_acc: 97.041% | global_loss: 2.746382713317871 | global_f1: 0.9599640125955916 | global_precision: 0.9578096947935368 | global_recall: 0.9621280432822362 | global_auc: 0.970849353107397| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 56 | global_acc: 97.008% | global_loss: 2.817349910736084 | global_f1: 0.9595323741007195 | global_precision: 0.95695067264574 | global_recall: 0.9621280432822362 | global_auc: 0.9704300730629903| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 57 | global_acc: 96.941% | global_loss: 2.788062572479248 | global_f1: 0.9586330935251798 | global_precision: 0.9560538116591928 | global_recall: 0.9612263300270514 | global_auc: 0.970284773296752| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 58 | global_acc: 96.875% | global_loss: 2.9040000438690186 | global_f1: 0.9578853046594982 | global_precision: 0.9519145146927872 | global_recall: 0.9639314697926059 | global_auc: 0.9692686246047585| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 59 | global_acc: 96.975% | global_loss: 2.8777382373809814 | global_f1: 0.9590643274853802 | global_precision: 0.9569120287253142 | global_recall: 0.9612263300270514 | global_auc: 0.9689789747439567| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 60 | global_acc: 97.008% | global_loss: 2.7836430072784424 | global_f1: 0.9594229035166817 | global_precision: 0.9594229035166817 | global_recall: 0.9594229035166817 | global_auc: 0.9698009155784617| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 61 | global_acc: 96.443% | global_loss: 3.3493165969848633 | global_f1: 0.9525498891352551 | global_precision: 0.93717277486911 | global_recall: 0.9684400360685302 | global_auc: 0.9674027571817734| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 62 | global_acc: 96.476% | global_loss: 3.1951615810394287 | global_f1: 0.9529307282415632 | global_precision: 0.9387576552930884 | global_recall: 0.9675383228133454 | global_auc: 0.9679557035144024| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 63 | global_acc: 96.576% | global_loss: 3.210397958755493 | global_f1: 0.954079358002675 | global_precision: 0.9435626102292769 | global_recall: 0.9648331830477908 | global_auc: 0.9669519005541809| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 64 | global_acc: 96.410% | global_loss: 3.3910112380981445 | global_f1: 0.9516994633273703 | global_precision: 0.9440993788819876 | global_recall: 0.9594229035166817 | global_auc: 0.965369984961949| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 65 | global_acc: 96.243% | global_loss: 3.452284097671509 | global_f1: 0.9495310406431443 | global_precision: 0.9407079646017699 | global_recall: 0.9585211902614968 | global_auc: 0.9643450518069641| flobal_FPR: 0.04147880973850315 \n",
      "comm_round: 66 | global_acc: 96.410% | global_loss: 3.217409133911133 | global_f1: 0.9506849315068492 | global_precision: 0.9629972247918593 | global_recall: 0.9386834986474302 | global_auc: 0.961854063004068| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 67 | global_acc: 96.210% | global_loss: 3.4945220947265625 | global_f1: 0.9478976234003657 | global_precision: 0.9610750695088045 | global_recall: 0.9350766456266907 | global_auc: 0.9572775952033984| flobal_FPR: 0.06492335437330929 \n",
      "comm_round: 68 | global_acc: 95.778% | global_loss: 3.9494521617889404 | global_f1: 0.9414476717381283 | global_precision: 0.9632075471698113 | global_recall: 0.9206492335437331 | global_auc: 0.9506218212708412| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 69 | global_acc: 95.944% | global_loss: 3.6968090534210205 | global_f1: 0.9442413162705666 | global_precision: 0.9573679332715477 | global_recall: 0.9314697926059513 | global_auc: 0.955951141291677| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 70 | global_acc: 96.077% | global_loss: 3.689692497253418 | global_f1: 0.9463148316651502 | global_precision: 0.9550045913682277 | global_recall: 0.9377817853922452 | global_auc: 0.9563412189320848| flobal_FPR: 0.062218214607754736 \n",
      "comm_round: 71 | global_acc: 96.144% | global_loss: 3.733058214187622 | global_f1: 0.9472247497725205 | global_precision: 0.9559228650137741 | global_recall: 0.9386834986474302 | global_auc: 0.9564829574295428| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 72 | global_acc: 96.144% | global_loss: 3.672431230545044 | global_f1: 0.9476534296028881 | global_precision: 0.948509485094851 | global_recall: 0.9467989179440938 | global_auc: 0.9589100808123111| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 73 | global_acc: 96.277% | global_loss: 3.6025755405426025 | global_f1: 0.9495040577096482 | global_precision: 0.9495040577096483 | global_recall: 0.9495040577096483 | global_auc: 0.9599001135332487| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 74 | global_acc: 96.709% | global_loss: 3.0425333976745605 | global_f1: 0.9556650246305419 | global_precision: 0.949288256227758 | global_recall: 0.9621280432822362 | global_auc: 0.9676271171149354| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 75 | global_acc: 96.277% | global_loss: 3.459933280944824 | global_f1: 0.9503105590062113 | global_precision: 0.9353711790393013 | global_recall: 0.9657348963029756 | global_auc: 0.9658552671877515| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 76 | global_acc: 95.844% | global_loss: 3.806957483291626 | global_f1: 0.9434644957033016 | global_precision: 0.9464609800362976 | global_recall: 0.9404869251577999 | global_auc: 0.9590107460098357| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 77 | global_acc: 95.878% | global_loss: 3.842271566390991 | global_f1: 0.9435336976320583 | global_precision: 0.953081876724931 | global_recall: 0.9341749323715058 | global_auc: 0.9552481468344356| flobal_FPR: 0.06582506762849413 \n",
      "comm_round: 78 | global_acc: 92.985% | global_loss: 4.278501033782959 | global_f1: 0.9080610021786493 | global_precision: 0.8785834738617201 | global_recall: 0.939585211902615 | global_auc: 0.9527922958835057| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 79 | global_acc: 95.645% | global_loss: 4.131179332733154 | global_f1: 0.9401005944215821 | global_precision: 0.9536178107606679 | global_recall: 0.9269612263300271 | global_auc: 0.952439730274251| flobal_FPR: 0.07303877366997295 \n",
      "comm_round: 80 | global_acc: 95.213% | global_loss: 4.475547790527344 | global_f1: 0.9340054995417049 | global_precision: 0.9496738117427772 | global_recall: 0.9188458070333634 | global_auc: 0.9500679252665373| flobal_FPR: 0.0811541929666366 \n",
      "comm_round: 81 | global_acc: 95.479% | global_loss: 4.383431434631348 | global_f1: 0.9380692167577412 | global_precision: 0.9475620975160993 | global_recall: 0.9287646528403968 | global_auc: 0.9495332601136472| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 82 | global_acc: 95.146% | global_loss: 4.513558387756348 | global_f1: 0.9332113449222323 | global_precision: 0.947075208913649 | global_recall: 0.9197475202885482 | global_auc: 0.947856614771858| flobal_FPR: 0.08025247971145176 \n",
      "comm_round: 83 | global_acc: 95.312% | global_loss: 4.539229869842529 | global_f1: 0.9354099862574438 | global_precision: 0.9506517690875232 | global_recall: 0.9206492335437331 | global_auc: 0.9458126839098553| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 84 | global_acc: 95.047% | global_loss: 4.517868995666504 | global_f1: 0.9313047487321346 | global_precision: 0.9528301886792453 | global_recall: 0.9107303877366997 | global_auc: 0.9458278786566514| flobal_FPR: 0.08926961226330027 \n",
      "comm_round: 85 | global_acc: 95.312% | global_loss: 4.421515941619873 | global_f1: 0.9357630979498862 | global_precision: 0.9456721915285451 | global_recall: 0.9260595130748422 | global_auc: 0.9498034417051164| flobal_FPR: 0.0739404869251578 \n",
      "comm_round: 86 | global_acc: 95.479% | global_loss: 4.364871025085449 | global_f1: 0.9384615384615385 | global_precision: 0.9418710263396912 | global_recall: 0.9350766456266907 | global_auc: 0.9514957566295393| flobal_FPR: 0.06492335437330929 \n",
      "comm_round: 87 | global_acc: 95.412% | global_loss: 4.308588981628418 | global_f1: 0.9382273948075202 | global_precision: 0.9315555555555556 | global_recall: 0.9449954914337241 | global_auc: 0.9544525593889054| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 88 | global_acc: 96.077% | global_loss: 3.8891422748565674 | global_f1: 0.9467989179440938 | global_precision: 0.9467989179440938 | global_recall: 0.9467989179440938 | global_auc: 0.9583967832721033| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 89 | global_acc: 95.977% | global_loss: 4.022606372833252 | global_f1: 0.9456668163448586 | global_precision: 0.9418604651162791 | global_recall: 0.9495040577096483 | global_auc: 0.9574250317309048| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 90 | global_acc: 95.844% | global_loss: 4.125049114227295 | global_f1: 0.9441215914170765 | global_precision: 0.9361702127659575 | global_recall: 0.9522091974752029 | global_auc: 0.9573727997887931| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 91 | global_acc: 95.844% | global_loss: 4.1316962242126465 | global_f1: 0.9442213297634984 | global_precision: 0.9346289752650176 | global_recall: 0.9540126239855726 | global_auc: 0.9579309693156335| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 92 | global_acc: 95.645% | global_loss: 4.355053424835205 | global_f1: 0.9418553040390589 | global_precision: 0.9274475524475524 | global_recall: 0.9567177637511272 | global_auc: 0.9564710865336081| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 93 | global_acc: 95.445% | global_loss: 4.554521083831787 | global_f1: 0.939407341884122 | global_precision: 0.921875 | global_recall: 0.957619477006312 | global_auc: 0.9550795801121658| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 94 | global_acc: 95.180% | global_loss: 4.820478916168213 | global_f1: 0.9345963013080739 | global_precision: 0.9350180505415162 | global_recall: 0.9341749323715058 | global_auc: 0.948130120214189| flobal_FPR: 0.06582506762849413 \n",
      "comm_round: 95 | global_acc: 95.279% | global_loss: 4.674100399017334 | global_f1: 0.9358047016274864 | global_precision: 0.9383499546690843 | global_recall: 0.933273219116321 | global_auc: 0.9496018738921486| flobal_FPR: 0.06672678088367899 \n",
      "comm_round: 96 | global_acc: 95.113% | global_loss: 4.83883810043335 | global_f1: 0.9333937471681015 | global_precision: 0.9380692167577414 | global_recall: 0.9287646528403968 | global_auc: 0.9472939343045625| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 97 | global_acc: 90.492% | global_loss: 9.382712364196777 | global_f1: 0.8813278008298755 | global_precision: 0.8162951575710992 | global_recall: 0.957619477006312 | global_auc: 0.9168569571284967| flobal_FPR: 0.04238052299368801 \n",
      "comm_round: 98 | global_acc: 95.146% | global_loss: 4.853725433349609 | global_f1: 0.935226264418811 | global_precision: 0.9205240174672489 | global_recall: 0.9504057709648331 | global_auc: 0.9512298485606064| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 99 | global_acc: 94.980% | global_loss: 4.937022686004639 | global_f1: 0.9326193663543062 | global_precision: 0.9231448763250883 | global_recall: 0.9422903516681695 | global_auc: 0.9492215303864072| flobal_FPR: 0.057709648331830475 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain\n",
    "import copy\n",
    "\n",
    "def train_model_prox(model,global_model, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    for i in range(2):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Add the proximal term\n",
    "            # for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            #     loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_model.parameters())):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_model.parameters()))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "    # temp2 = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    # Update x_hats\n",
    "    # for x_hat, z_weight, param_model in zip(x_hats[int(client[-1])-1],(Z_weights[int(client[-1])-1], local_model.parameters()) ):\n",
    "    #     x_hat = param_model +  z_weight/rho\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "    # print(x_hats[int(client[-1])-1])\n",
    "    # print(delta_x_hats[int(client[-1])-1])\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n",
    "\n",
    "\n",
    "\n",
    "# def update_avg_with_delta(avg_grad, delta_x_hats):\n",
    "#     '''Update avg_grad by adding the corresponding elements from delta_x_hats'''\n",
    "#     for avg_layer, delta_layers in zip(avg_grad, zip(*delta_x_hats)):\n",
    "#         avg_layer += delta_layers\n",
    "#     return avg_grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM-nonidd 2022|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                    Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "   \n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-nonidd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-nonidd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-nonidd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
