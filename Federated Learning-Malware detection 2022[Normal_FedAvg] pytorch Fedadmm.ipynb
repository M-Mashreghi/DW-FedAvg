{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da6c9299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 5\n",
      "No. of Rounds: 200\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedADMM 2022|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6382462382316589 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8215818586119314| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.5355787873268127 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.924402098584467| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 94.747% | global_loss: 0.29206186532974243 | global_f1: 0.9283121597096189 | global_precision: 0.9342465753424658 | global_recall: 0.9224526600541028 | global_auc: 0.9802591748967588| flobal_FPR: 0.0775473399458972 \n",
      "comm_round: 3 | global_acc: 94.481% | global_loss: 0.15951257944107056 | global_f1: 0.9256272401433693 | global_precision: 0.9198575244879786 | global_recall: 0.9314697926059513 | global_auc: 0.9880160931362006| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 4 | global_acc: 96.875% | global_loss: 0.15444843471050262 | global_f1: 0.9577717879604671 | global_precision: 0.954341987466428 | global_recall: 0.9612263300270514 | global_auc: 0.9939641242531426| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 5 | global_acc: 97.307% | global_loss: 0.20427988469600677 | global_f1: 0.9636608344549126 | global_precision: 0.9589285714285715 | global_recall: 0.9684400360685302 | global_auc: 0.9952331230285411| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 6 | global_acc: 97.773% | global_loss: 0.2010296732187271 | global_f1: 0.9696145124716553 | global_precision: 0.9753649635036497 | global_recall: 0.9639314697926059 | global_auc: 0.9953888691832016| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 7 | global_acc: 97.806% | global_loss: 0.21223574876785278 | global_f1: 0.9701897018970189 | global_precision: 0.971945701357466 | global_recall: 0.9684400360685302 | global_auc: 0.9954263812143547| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 8 | global_acc: 97.872% | global_loss: 0.22034718096256256 | global_f1: 0.9712488769092542 | global_precision: 0.9677708146821844 | global_recall: 0.9747520288548241 | global_auc: 0.9955871131453078| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 9 | global_acc: 97.806% | global_loss: 0.26274359226226807 | global_f1: 0.9703504043126685 | global_precision: 0.9668755595344674 | global_recall: 0.9738503155996393 | global_auc: 0.9954679293501254| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 10 | global_acc: 98.039% | global_loss: 0.30562177300453186 | global_f1: 0.9735307312696275 | global_precision: 0.96875 | global_recall: 0.9783588818755635 | global_auc: 0.9953651273913326| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 11 | global_acc: 98.271% | global_loss: 0.34014102816581726 | global_f1: 0.9763205828779599 | global_precision: 0.9862005519779209 | global_recall: 0.9666366095581606 | global_auc: 0.9952903407469452| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 12 | global_acc: 92.553% | global_loss: 0.9674366116523743 | global_f1: 0.9071310116086235 | global_precision: 0.8396009209516501 | global_recall: 0.9864743011722272 | global_auc: 0.9936146450768307| flobal_FPR: 0.013525698827772768 \n",
      "comm_round: 13 | global_acc: 97.540% | global_loss: 0.4491068720817566 | global_f1: 0.9658671586715867 | global_precision: 0.9886685552407932 | global_recall: 0.9440937781785392 | global_auc: 0.9952578144920847| flobal_FPR: 0.05590622182146077 \n",
      "comm_round: 14 | global_acc: 98.271% | global_loss: 0.47042229771614075 | global_f1: 0.9766606822262118 | global_precision: 0.9722966934763181 | global_recall: 0.9810640216411182 | global_auc: 0.994685399890123| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 15 | global_acc: 98.371% | global_loss: 0.5184009671211243 | global_f1: 0.9778980604420388 | global_precision: 0.9783393501805054 | global_recall: 0.9774571686203787 | global_auc: 0.9944643638078227| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 16 | global_acc: 98.537% | global_loss: 0.6049844622612 | global_f1: 0.9801623083859333 | global_precision: 0.9801623083859333 | global_recall: 0.9801623083859333 | global_auc: 0.9936134579872374| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 17 | global_acc: 93.717% | global_loss: 1.4888781309127808 | global_f1: 0.9205548549810844 | global_precision: 0.8622047244094488 | global_recall: 0.9873760144274121 | global_auc: 0.9904515261461232| flobal_FPR: 0.012623985572587917 \n",
      "comm_round: 18 | global_acc: 98.305% | global_loss: 0.6286863684654236 | global_f1: 0.9769126301493889 | global_precision: 0.980909090909091 | global_recall: 0.9729486023444545 | global_auc: 0.9936611789888941| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 19 | global_acc: 94.282% | global_loss: 1.2537349462509155 | global_f1: 0.9271186440677966 | global_precision: 0.8745003996802558 | global_recall: 0.9864743011722272 | global_auc: 0.992009699946486| flobal_FPR: 0.013525698827772768 \n",
      "comm_round: 20 | global_acc: 98.072% | global_loss: 0.6913182735443115 | global_f1: 0.9736603088101726 | global_precision: 0.9807868252516011 | global_recall: 0.9666366095581606 | global_auc: 0.9925578979207414| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 21 | global_acc: 97.872% | global_loss: 1.4488627910614014 | global_f1: 0.971403038427167 | global_precision: 0.9627989371124889 | global_recall: 0.9801623083859333 | global_auc: 0.9863622399146057| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 22 | global_acc: 98.471% | global_loss: 0.8630509972572327 | global_f1: 0.9792043399638336 | global_precision: 0.9818676337262012 | global_recall: 0.9765554553651938 | global_auc: 0.9910899429294808| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 23 | global_acc: 98.471% | global_loss: 0.8884170055389404 | global_f1: 0.9793351302785266 | global_precision: 0.9758281110116384 | global_recall: 0.9828674481514879 | global_auc: 0.9910336748827513| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 24 | global_acc: 98.604% | global_loss: 0.9184567928314209 | global_f1: 0.9810469314079423 | global_precision: 0.98193315266486 | global_recall: 0.9801623083859333 | global_auc: 0.9907568455895586| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 25 | global_acc: 98.504% | global_loss: 1.0019608736038208 | global_f1: 0.979702300405954 | global_precision: 0.98014440433213 | global_recall: 0.9792605951307484 | global_auc: 0.989764438689434| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 26 | global_acc: 95.213% | global_loss: 1.1558021306991577 | global_f1: 0.937984496124031 | global_precision: 0.8977741137675186 | global_recall: 0.981965734896303 | global_auc: 0.9893962034975458| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 27 | global_acc: 98.404% | global_loss: 1.0704272985458374 | global_f1: 0.9782805429864253 | global_precision: 0.9818346957311535 | global_recall: 0.9747520288548241 | global_auc: 0.9887744059684965| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 28 | global_acc: 98.404% | global_loss: 1.1196445226669312 | global_f1: 0.9783393501805054 | global_precision: 0.979223125564589 | global_recall: 0.9774571686203787 | global_auc: 0.9883762561188533| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 29 | global_acc: 98.238% | global_loss: 1.1020654439926147 | global_f1: 0.9758761948111061 | global_precision: 0.9852941176470589 | global_recall: 0.9666366095581606 | global_auc: 0.9881196073487494| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 30 | global_acc: 98.271% | global_loss: 1.2592136859893799 | global_f1: 0.976491862567812 | global_precision: 0.9791477787851315 | global_recall: 0.9738503155996393 | global_auc: 0.986120548473379| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 31 | global_acc: 98.271% | global_loss: 1.296849012374878 | global_f1: 0.9764492753623188 | global_precision: 0.9808917197452229 | global_recall: 0.9720468890892696 | global_auc: 0.984788633949528| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 32 | global_acc: 97.839% | global_loss: 1.5547058582305908 | global_f1: 0.9708650829224562 | global_precision: 0.9652406417112299 | global_recall: 0.9765554553651938 | global_auc: 0.986004925946977| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 33 | global_acc: 97.739% | global_loss: 1.667952299118042 | global_f1: 0.9695067264573991 | global_precision: 0.9643175735950045 | global_recall: 0.9747520288548241 | global_auc: 0.9851172203489948| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 34 | global_acc: 97.673% | global_loss: 1.7409532070159912 | global_f1: 0.9685816876122083 | global_precision: 0.9642537980339589 | global_recall: 0.9729486023444545 | global_auc: 0.9839446132485847| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 35 | global_acc: 97.640% | global_loss: 1.9524914026260376 | global_f1: 0.9681471511888738 | global_precision: 0.9633928571428572 | global_recall: 0.9729486023444545 | global_auc: 0.9807069450914082| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 36 | global_acc: 97.606% | global_loss: 1.8975677490234375 | global_f1: 0.9676258992805755 | global_precision: 0.9650224215246637 | global_recall: 0.9702434625788999 | global_auc: 0.9818605587583233| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 37 | global_acc: 97.839% | global_loss: 1.691786766052246 | global_f1: 0.9708389412292509 | global_precision: 0.9660714285714286 | global_recall: 0.975653742110009 | global_auc: 0.982967638513175| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 38 | global_acc: 97.606% | global_loss: 2.0592939853668213 | global_f1: 0.9680284191829485 | global_precision: 0.9536307961504812 | global_recall: 0.9828674481514879 | global_auc: 0.9807188159873426| flobal_FPR: 0.017132551848512173 \n",
      "comm_round: 39 | global_acc: 97.673% | global_loss: 1.9111785888671875 | global_f1: 0.9688057040998218 | global_precision: 0.9577092511013215 | global_recall: 0.9801623083859333 | global_auc: 0.9819797425535057| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 40 | global_acc: 97.606% | global_loss: 1.9328787326812744 | global_f1: 0.9679144385026739 | global_precision: 0.9568281938325991 | global_recall: 0.9792605951307484 | global_auc: 0.9820975018411761| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 41 | global_acc: 97.939% | global_loss: 1.8462765216827393 | global_f1: 0.9723707664884135 | global_precision: 0.9612334801762115 | global_recall: 0.9837691614066727 | global_auc: 0.9833228157195353| flobal_FPR: 0.016230838593327322 \n",
      "comm_round: 42 | global_acc: 97.340% | global_loss: 2.2697207927703857 | global_f1: 0.96386630532972 | global_precision: 0.9656108597285068 | global_recall: 0.9621280432822362 | global_auc: 0.9764630997948234| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 43 | global_acc: 97.274% | global_loss: 2.4043290615081787 | global_f1: 0.9631294964028776 | global_precision: 0.9605381165919282 | global_recall: 0.9657348963029756 | global_auc: 0.9754723548201298| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 44 | global_acc: 97.241% | global_loss: 2.2643847465515137 | global_f1: 0.9627969520394442 | global_precision: 0.9572192513368984 | global_recall: 0.9684400360685302 | global_auc: 0.9764170407185975| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 45 | global_acc: 97.473% | global_loss: 2.145566701889038 | global_f1: 0.9661921708185053 | global_precision: 0.9534679543459175 | global_recall: 0.9792605951307484 | global_auc: 0.979905184779992| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 46 | global_acc: 97.606% | global_loss: 2.212212324142456 | global_f1: 0.9679715302491102 | global_precision: 0.9552238805970149 | global_recall: 0.9810640216411182 | global_auc: 0.9798534276737174| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 47 | global_acc: 97.640% | global_loss: 2.214061737060547 | global_f1: 0.9682610639248993 | global_precision: 0.9601063829787234 | global_recall: 0.9765554553651938 | global_auc: 0.978946016388484| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 48 | global_acc: 97.573% | global_loss: 2.1731550693511963 | global_f1: 0.9673961589995533 | global_precision: 0.9584070796460177 | global_recall: 0.9765554553651938 | global_auc: 0.9796008150082313| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 49 | global_acc: 97.540% | global_loss: 2.2497379779815674 | global_f1: 0.9668755595344672 | global_precision: 0.96 | global_recall: 0.9738503155996393 | global_auc: 0.9781922144966432| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 50 | global_acc: 97.673% | global_loss: 2.0666663646698 | global_f1: 0.9687220732797139 | global_precision: 0.9601417183348095 | global_recall: 0.9774571686203787 | global_auc: 0.9801936475512004| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 51 | global_acc: 97.507% | global_loss: 2.1782500743865967 | global_f1: 0.966651845264562 | global_precision: 0.9535087719298245 | global_recall: 0.9801623083859333 | global_auc: 0.979354375208631| flobal_FPR: 0.019837691614066726 \n",
      "comm_round: 52 | global_acc: 97.573% | global_loss: 2.1464438438415527 | global_f1: 0.9674543022737404 | global_precision: 0.9567901234567902 | global_recall: 0.9783588818755635 | global_auc: 0.979672277801757| flobal_FPR: 0.02164111812443643 \n",
      "comm_round: 53 | global_acc: 97.540% | global_loss: 2.2675108909606934 | global_f1: 0.9671403197158082 | global_precision: 0.952755905511811 | global_recall: 0.981965734896303 | global_auc: 0.9790333861825621| flobal_FPR: 0.018034265103697024 \n",
      "comm_round: 54 | global_acc: 97.673% | global_loss: 2.0309059619903564 | global_f1: 0.968833481745325 | global_precision: 0.9569041336851363 | global_recall: 0.9810640216411182 | global_auc: 0.980836575275013| flobal_FPR: 0.018935978358881875 \n",
      "comm_round: 55 | global_acc: 97.640% | global_loss: 2.1436073780059814 | global_f1: 0.9683459652251449 | global_precision: 0.9576719576719577 | global_recall: 0.9792605951307484 | global_auc: 0.9799647766775832| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 56 | global_acc: 97.839% | global_loss: 2.030993700027466 | global_f1: 0.9709432275368797 | global_precision: 0.9627659574468085 | global_recall: 0.9792605951307484 | global_auc: 0.980251340105442| flobal_FPR: 0.020739404869251576 \n",
      "comm_round: 57 | global_acc: 97.507% | global_loss: 2.1697216033935547 | global_f1: 0.9663526244952894 | global_precision: 0.9616071428571429 | global_recall: 0.9711451758340848 | global_auc: 0.9794578894211798| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 58 | global_acc: 94.914% | global_loss: 2.4366273880004883 | global_f1: 0.9335649153278333 | global_precision: 0.9003350083752094 | global_recall: 0.9693417493237151 | global_auc: 0.9750803778363726| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 59 | global_acc: 97.374% | global_loss: 2.5164377689361572 | global_f1: 0.964334085778781 | global_precision: 0.9656419529837251 | global_recall: 0.9630297565374211 | global_auc: 0.973181034486852| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 60 | global_acc: 97.540% | global_loss: 2.322035074234009 | global_f1: 0.9667266187050361 | global_precision: 0.9641255605381166 | global_recall: 0.9693417493237151 | global_auc: 0.9748980408748186| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 61 | global_acc: 97.573% | global_loss: 2.3702125549316406 | global_f1: 0.9669832654907281 | global_precision: 0.97005444646098 | global_recall: 0.9639314697926059 | global_auc: 0.9758149488767996| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 62 | global_acc: 97.706% | global_loss: 2.2698216438293457 | global_f1: 0.9689887640449438 | global_precision: 0.9659498207885304 | global_recall: 0.9720468890892696 | global_auc: 0.9763754925828269| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 63 | global_acc: 97.307% | global_loss: 2.6044044494628906 | global_f1: 0.9635299414678072 | global_precision: 0.9622302158273381 | global_recall: 0.9648331830477908 | global_auc: 0.9730841679760264| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 64 | global_acc: 96.908% | global_loss: 3.0404279232025146 | global_f1: 0.9579375848032563 | global_precision: 0.9609800362976406 | global_recall: 0.9549143372407575 | global_auc: 0.9671760230694243| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 65 | global_acc: 96.277% | global_loss: 3.628981351852417 | global_f1: 0.9508771929824562 | global_precision: 0.9257045260461144 | global_recall: 0.9774571686203787 | global_auc: 0.9665345198531239| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 66 | global_acc: 96.310% | global_loss: 3.6901817321777344 | global_f1: 0.9506447309915518 | global_precision: 0.9377192982456141 | global_recall: 0.9639314697926059 | global_auc: 0.963168408601936| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 67 | global_acc: 95.911% | global_loss: 4.073612689971924 | global_f1: 0.9456473707467963 | global_precision: 0.9272097053726169 | global_recall: 0.9648331830477908 | global_auc: 0.9615924284576715| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 68 | global_acc: 94.814% | global_loss: 5.128903388977051 | global_f1: 0.9302949061662198 | global_precision: 0.9220549158547388 | global_recall: 0.9386834986474302 | global_auc: 0.9487338739814178| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 69 | global_acc: 94.548% | global_loss: 5.419126510620117 | global_f1: 0.9268510258697592 | global_precision: 0.9170344218887908 | global_recall: 0.9368800721370604 | global_auc: 0.9441187070600017| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 70 | global_acc: 94.648% | global_loss: 5.328099250793457 | global_f1: 0.9275101305718145 | global_precision: 0.9262589928057554 | global_recall: 0.9287646528403968 | global_auc: 0.943185417221631| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 71 | global_acc: 94.315% | global_loss: 5.62638521194458 | global_f1: 0.922237380627558 | global_precision: 0.9302752293577982 | global_recall: 0.9143372407574392 | global_auc: 0.9384565271171624| flobal_FPR: 0.08566275924256087 \n",
      "comm_round: 72 | global_acc: 92.819% | global_loss: 7.180850982666016 | global_f1: 0.9000925069380203 | global_precision: 0.9240265906932573 | global_recall: 0.8773669972948602 | global_auc: 0.917619780901248| flobal_FPR: 0.12263300270513977 \n",
      "comm_round: 73 | global_acc: 93.451% | global_loss: 6.518448352813721 | global_f1: 0.9065021357380162 | global_precision: 0.9569138276553106 | global_recall: 0.8611361587015329 | global_auc: 0.9210089216905486| flobal_FPR: 0.1388638412984671 \n",
      "comm_round: 74 | global_acc: 94.781% | global_loss: 5.219414710998535 | global_f1: 0.927213722763097 | global_precision: 0.9541984732824428 | global_recall: 0.9017132551848512 | global_auc: 0.9382183969447163| flobal_FPR: 0.09828674481514878 \n",
      "comm_round: 75 | global_acc: 94.481% | global_loss: 5.518617153167725 | global_f1: 0.9245454545454544 | global_precision: 0.9321723189734189 | global_recall: 0.9170423805229937 | global_auc: 0.9390196824202952| flobal_FPR: 0.0829576194770063 \n",
      "comm_round: 76 | global_acc: 94.947% | global_loss: 5.053191661834717 | global_f1: 0.9302112029384757 | global_precision: 0.9476145930776426 | global_recall: 0.9134355275022543 | global_auc: 0.9419503692086053| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 77 | global_acc: 94.847% | global_loss: 5.114229202270508 | global_f1: 0.9294492489758762 | global_precision: 0.9384191176470589 | global_recall: 0.9206492335437331 | global_auc: 0.9433611064814617| flobal_FPR: 0.0793507664562669 \n",
      "comm_round: 78 | global_acc: 94.747% | global_loss: 5.213361740112305 | global_f1: 0.9302120141342756 | global_precision: 0.9116883116883117 | global_recall: 0.9495040577096483 | global_auc: 0.9487096573537114| flobal_FPR: 0.05049594229035167 \n",
      "comm_round: 79 | global_acc: 94.215% | global_loss: 5.784574508666992 | global_f1: 0.9222520107238605 | global_precision: 0.9140832595217007 | global_recall: 0.9305680793507665 | global_auc: 0.9397442819081373| flobal_FPR: 0.06943192064923355 \n",
      "comm_round: 80 | global_acc: 94.082% | global_loss: 5.912452220916748 | global_f1: 0.9198919891989198 | global_precision: 0.9182389937106918 | global_recall: 0.9215509467989179 | global_auc: 0.9372022482527228| flobal_FPR: 0.07844905320108206 \n",
      "comm_round: 81 | global_acc: 93.617% | global_loss: 6.382978916168213 | global_f1: 0.9096895578551271 | global_precision: 0.9508357915437562 | global_recall: 0.8719567177637512 | global_auc: 0.9227798219460576| flobal_FPR: 0.12804328223624886 \n",
      "comm_round: 82 | global_acc: 93.750% | global_loss: 6.25 | global_f1: 0.9128822984244671 | global_precision: 0.9389895138226882 | global_recall: 0.8881875563570785 | global_auc: 0.9272133641596759| flobal_FPR: 0.11181244364292155 \n",
      "comm_round: 83 | global_acc: 89.827% | global_loss: 9.861052513122559 | global_f1: 0.8650793650793651 | global_precision: 0.8464193270060397 | global_recall: 0.8845807033363391 | global_auc: 0.9007353307777668| flobal_FPR: 0.11541929666366095 \n",
      "comm_round: 84 | global_acc: 90.758% | global_loss: 9.242025375366211 | global_f1: 0.883779264214047 | global_precision: 0.8238503507404521 | global_recall: 0.9531109107303878 | global_auc: 0.9170381070004573| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 85 | global_acc: 93.251% | global_loss: 6.7486701011657715 | global_f1: 0.9084348218313035 | global_precision: 0.9088447653429603 | global_recall: 0.9080252479711451 | global_auc: 0.9274196803310176| flobal_FPR: 0.09197475202885483 \n",
      "comm_round: 86 | global_acc: 92.287% | global_loss: 7.712766170501709 | global_f1: 0.9008547008547008 | global_precision: 0.8562144597887896 | global_recall: 0.9504057709648331 | global_auc: 0.9285994099689885| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 87 | global_acc: 86.835% | global_loss: 13.16489315032959 | global_f1: 0.8433544303797469 | global_precision: 0.7512332628611699 | global_recall: 0.9612263300270514 | global_auc: 0.8876695104584968| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 88 | global_acc: 85.007% | global_loss: 14.993350982666016 | global_f1: 0.8262042389210019 | global_precision: 0.721399730820996 | global_recall: 0.9666366095581606 | global_auc: 0.8743135654425873| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 89 | global_acc: 83.378% | global_loss: 16.62234115600586 | global_f1: 0.8108925869894099 | global_precision: 0.698371335504886 | global_recall: 0.9666366095581606 | global_auc: 0.8614120383230508| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 90 | global_acc: 82.048% | global_loss: 17.95212745666504 | global_f1: 0.7988077496274217 | global_precision: 0.6806349206349206 | global_recall: 0.9666366095581606 | global_auc: 0.8508801794499596| flobal_FPR: 0.033363390441839495 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 184\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[39m# fit local model with client's data\u001b[39;00m\n\u001b[0;32m    180\u001b[0m train_loader \u001b[39m=\u001b[39m DataLoader(TensorDataset(torch\u001b[39m.\u001b[39mtensor(clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m0\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32),\n\u001b[0;32m    181\u001b[0m                                         torch\u001b[39m.\u001b[39mtensor(clients_batched[client]\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39mtensors[\u001b[39m1\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)),\n\u001b[0;32m    182\u001b[0m                           batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 184\u001b[0m train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n\u001b[0;32m    185\u001b[0m Z_weights,x_hats \u001b[39m=\u001b[39m update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n\u001b[0;32m    188\u001b[0m \u001b[39m# scale the model weights and add to the list\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mtrain_model_prox\u001b[1;34m(model, global_model, train_loader, loss_fn, optimizer, client, rho, Z_weights, x_hats, mu)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39m# Add the proximal term\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[39m# for param, param_global in zip(model.parameters(), global_model.parameters()):\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39m#     loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m z_weight, (param, param_global) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(Z_weights[\u001b[39mint\u001b[39m(client[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], \u001b[39mzip\u001b[39m(model\u001b[39m.\u001b[39mparameters(), global_model\u001b[39m.\u001b[39mparameters())):\n\u001b[1;32m---> 29\u001b[0m     loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (mu \u001b[39m/\u001b[39m \u001b[39m2\u001b[39m) \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39;49mnorm(param \u001b[39m-\u001b[39;49m param_global, p\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m \u001b[39m+\u001b[39m torch\u001b[39m.\u001b[39msum(z_weight \u001b[39m*\u001b[39m (param \u001b[39m-\u001b[39m param_global))\n\u001b[0;32m     31\u001b[0m \u001b[39m# Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\u001b[39;00m\n\u001b[0;32m     33\u001b[0m loss\u001b[39m.\u001b[39mbackward(retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)  \u001b[39m# Set retain_graph=True to retain the computation graph\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\functional.py:1611\u001b[0m, in \u001b[0;36mnorm\u001b[1;34m(input, p, dim, keepdim, out, dtype)\u001b[0m\n\u001b[0;32m   1609\u001b[0m _p \u001b[39m=\u001b[39m \u001b[39m2.0\u001b[39m \u001b[39mif\u001b[39;00m p \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m p\n\u001b[0;32m   1610\u001b[0m \u001b[39mif\u001b[39;00m out \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1611\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49mvector_norm(\u001b[39minput\u001b[39;49m, _p, _dim, keepdim, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m   1612\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1613\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mvector_norm(\u001b[39minput\u001b[39m, _p, _dim, keepdim, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from itertools import chain\n",
    "import copy\n",
    "\n",
    "def train_model_prox(model,global_model, train_loader, loss_fn, optimizer,client ,rho,Z_weights,x_hats,mu=0.01):\n",
    "    model.train()\n",
    "    for i in range(2):\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            # Add the proximal term\n",
    "            # for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            #     loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "\n",
    "\n",
    "            for z_weight, (param, param_global) in zip(Z_weights[int(client[-1])-1], zip(model.parameters(), global_model.parameters())):\n",
    "                loss += (mu / 2) * torch.norm(param - param_global, p=2)**2 + torch.sum(z_weight * (param - param_global))\n",
    "\n",
    "            # Now 'inner_product' contains the inner product between model.parameters() and global_model.parameters()\n",
    "\n",
    "            loss.backward(retain_graph=True)  # Set retain_graph=True to retain the computation graph\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "def update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho):\n",
    "    # Update Z_weight\n",
    "    for i, (z_weight, (param, param_global)) in enumerate(zip(Z_weights[int(client[-1])-1], zip(local_model.parameters(), global_model.parameters()))):\n",
    "        Z_weights[int(client[-1])-1][i] += rho * (param - param_global)\n",
    "    temp = copy.copy(x_hats[int(client[-1])-1])\n",
    "    # temp2 = copy.copy(x_hats[int(client[-1])-1])\n",
    "\n",
    "    # Update x_hats\n",
    "    # for x_hat, z_weight, param_model in zip(x_hats[int(client[-1])-1],(Z_weights[int(client[-1])-1], local_model.parameters()) ):\n",
    "    #     x_hat = param_model +  z_weight/rho\n",
    "    for i, (x_hat, z_weight, param_model) in enumerate(zip(x_hats[int(client[-1])-1], Z_weights[int(client[-1])-1], local_model.parameters())):\n",
    "        x_hats[int(client[-1])-1][i] = param_model + z_weight / rho\n",
    "    # print(x_hats[int(client[-1])-1])\n",
    "    # print(delta_x_hats[int(client[-1])-1])\n",
    "\n",
    "\n",
    "    return Z_weights,x_hats\n",
    "\n",
    "\n",
    "\n",
    "# def update_avg_with_delta(avg_grad, delta_x_hats):\n",
    "#     '''Update avg_grad by adding the corresponding elements from delta_x_hats'''\n",
    "#     for avg_layer, delta_layers in zip(avg_grad, zip(*delta_x_hats)):\n",
    "#         avg_layer += delta_layers\n",
    "#     return avg_grad\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "mu = 0.01\n",
    "rho =0.01\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "            Z_weights=[]\n",
    "            x_hats = []\n",
    "            for i in range(number_of_clients):\n",
    "                Z_weight = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                Z_weights.append(Z_weight)\n",
    "                x_hat = [torch.zeros_like(param) for param in global_model.parameters()]\n",
    "                x_hats.append(x_hat)\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedADMM 2022|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    "\n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "                scaled_x_hats_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    train_model_prox(local_model, global_model,train_loader, loss, optimizer,client,rho,Z_weights,x_hats,mu)\n",
    "                    Z_weights,x_hats = update_z_parameter(local_model,global_model,Z_weights,x_hats,client,rho)\n",
    "   \n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    # scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_x_hats = scale_model_weights(x_hats[int(client[-1])-1], scaling_factor)\n",
    "                    # print(local_model.state_dict().values())\n",
    "                    scaled_x_hats_list.append(scaled_x_hats)\n",
    "                    # scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                # ..\n",
    "  \n",
    "                average_weights = sum_scaled_weights(scaled_x_hats_list)\n",
    "                # print(len(average_weights), \"asdsa\")\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedADMM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedADMM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedADMM-all-std-results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f343ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((all_avg[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2893319",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_avg =[]\n",
    "\n",
    "all_std =[]\n",
    "\n",
    "n_clients = [5,10,15]\n",
    "n_round = [10,20]\n",
    "\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd' ]\n",
    "\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d==1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d==2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d==3:\n",
    "        use_data = Tuandromd_data\n",
    "        \n",
    "        \n",
    "    print('===================================================================================================')\n",
    "    print('Working with:',dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round: #number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients: #number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            # from sklearn.utils import shuffle\n",
    "            # use_data = shuffle(use_data)\n",
    "            # use_data\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "\n",
    "            features = np.array(use_data.iloc[:,range(0,use_data.shape[1]-1)]) #feature set\n",
    "\n",
    "            labels = use_data.iloc[:,-1] #labels --> B : Benign and S\n",
    "\n",
    "\n",
    "            #Do feature scaling \n",
    "\n",
    "\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "\n",
    "            #binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "\n",
    "            #split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=100)\n",
    "\n",
    "\n",
    "\n",
    "            #create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            #process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "\n",
    "                #process and batch the test set  \n",
    "            test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))\n",
    "\n",
    "            #==============================================\n",
    "            # Traditional FedAvg 2017\n",
    "            #==============================================\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            all_results=list()\n",
    "\n",
    "            #create optimizer\n",
    "            lr = 0.01 \n",
    "            loss='binary_crossentropy'\n",
    "            metrics = ['accuracy']\n",
    "            optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=lr, \n",
    "                            decay=lr / comms_round, \n",
    "                            momentum=0.9\n",
    "                           )\n",
    "\n",
    "            #initialize global model\n",
    "            smlp_global = SimpleMLP()\n",
    "            global_model = smlp_global.build(X.shape[1],1)\n",
    "            #-----------------------------------------------\n",
    "\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedAvg 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            #commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = global_model.get_weights()\n",
    "\n",
    "                #initial list to collect local model weights after scalling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                #randomize client data - using keys\n",
    "                client_names= list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                #loop through each client and create new local model\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP()\n",
    "                    local_model = smlp_local.build(X.shape[1],1)\n",
    "                    local_model.compile(loss=loss, \n",
    "                                  optimizer=optimizer, \n",
    "                                  metrics=metrics)\n",
    "\n",
    "                    #set local model weight to the weight of the global model\n",
    "                    local_model.set_weights(global_weights)\n",
    "\n",
    "                    #fit local model with client's data\n",
    "                    local_model.fit(clients_batched[client], epochs=32, verbose=0)\n",
    "\n",
    "                    #scale the model weights and add to list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    #clear session to free memory after each communication round\n",
    "                    keras.backend.clear_session()\n",
    "\n",
    "                #to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                #update global model \n",
    "                global_model.set_weights(average_weights)\n",
    "\n",
    "                #test global model and print out metrics after each communications round\n",
    "                for(X_test, Y_test) in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test, Y_test, global_model, comm_round)\n",
    "                    all_results.append([global_acc,global_loss.numpy(),global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = 'results/round-'+str(r)+'/'+str(cl)+'-clients/FedAvg-'+dataset[d]+'-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "            \n",
    "            \n",
    "            all_avg.append(np.concatenate(([dataset[d],r,cl],np.mean(all_results,axis=0)))) #Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d],r,cl],np.std(all_results,axis=0)))) #Storing std values sfor each dataset\n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "            \n",
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv(f'FedAvg-results.csv')     \n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset', 'num of round', 'num of cliends','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6104c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_AVG = pd.DataFrame(all_avg, columns = ['Dataset','Rounds','no-clients','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedAvg-all-avg-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns = ['Dataset','Rounds','no-clients','global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedAvg-all-std-results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0c3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "# make a little extra space between the subplots\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "s1 = np.array(all_results) #FedAvg\n",
    "\n",
    "t = range(0,s1.shape[0])\n",
    "\n",
    "ax1.plot(t, s1[:,0],label='Acc of FedAvg')\n",
    "ax1.set_xlim(0,s1.shape[0])\n",
    "ax1.set_xlabel('Rounds')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0.98,1)\n",
    "ax1.grid(True)\n",
    "ax1.legend()\n",
    "\n",
    "\n",
    "ax2.plot(t, s1[:,1],label='Error of FedAvg')\n",
    "ax2.set_xlim(0, s1.shape[0])\n",
    "ax2.set_xlabel('Rounds')\n",
    "ax2.set_ylabel('error')\n",
    "ax2.grid(True)\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3861d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b54df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
