{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b69fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173907af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from federated_utils_fedavg_copy import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4bf76d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#declear path to your data\n",
    "drebin_data_path = r'data\\drebin.csv'\n",
    "malgenome_data_path = r'data\\malgenome.csv'\n",
    "kronodroid_data_path = r'data\\kronodroid.csv'\n",
    "TUANDROMD_data_path=r'data\\TUANDROMD.csv'\n",
    "\n",
    "\n",
    "\n",
    "Drebin_data = pd.read_csv(drebin_data_path, header = None)\n",
    "\n",
    "Malgenome_data = pd.read_csv(malgenome_data_path)\n",
    "\n",
    "Tuandromd_data=pd.read_csv(TUANDROMD_data_path)\n",
    "\n",
    "kronodroid_data=pd.read_csv(kronodroid_data_path)\n",
    "Kronodroid_data = kronodroid_data.iloc[:,range(1,kronodroid_data.shape[1])]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3ab07f",
   "metadata": {},
   "source": [
    "## non iid FedProx FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6356e8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedProx-non-iid-FGSM  2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.165% | global_loss: 0.6760373711585999 | global_f1: 0.0018018018018018016 | global_precision: 1.0 | global_recall: 0.0009017132551848512 | global_auc: 0.7460145841079093| flobal_FPR: 0.9990982867448152 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6453539133071899 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8595969783346653| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.5975015759468079 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9175205402112354| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 65.924% | global_loss: 0.5134667158126831 | global_f1: 0.1408214585079631 | global_precision: 1.0 | global_recall: 0.0757439134355275 | global_auc: 0.9572576520982283| flobal_FPR: 0.9242560865644724 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.5611982345581055 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9376744724929973| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.5699519515037537 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9325790091220713| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.5535220503807068 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9406621395817931| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 90.791% | global_loss: 0.4080829620361328 | global_f1: 0.8594622019279553 | global_precision: 0.9825986078886311 | global_recall: 0.763751127141569 | global_auc: 0.9796537592040991| flobal_FPR: 0.23624887285843102 \n",
      "comm_round: 8 | global_acc: 94.781% | global_loss: 0.2629432678222656 | global_f1: 0.9265325222274216 | global_precision: 0.9630350194552529 | global_recall: 0.8926961226330027 | global_auc: 0.9868399247670099| flobal_FPR: 0.1073038773669973 \n",
      "comm_round: 9 | global_acc: 89.827% | global_loss: 0.3287823796272278 | global_f1: 0.8452982810920122 | global_precision: 0.9620253164556962 | global_recall: 0.7538322813345356 | global_auc: 0.9647306185069167| flobal_FPR: 0.24616771866546439 \n",
      "comm_round: 10 | global_acc: 89.495% | global_loss: 0.34503117203712463 | global_f1: 0.8386108273748722 | global_precision: 0.967020023557126 | global_recall: 0.7403065825067628 | global_auc: 0.9706912327735494| flobal_FPR: 0.25969341749323716 \n",
      "comm_round: 11 | global_acc: 82.713% | global_loss: 0.42416369915008545 | global_f1: 0.7104677060133631 | global_precision: 0.9286754002911208 | global_recall: 0.5752930568079351 | global_auc: 0.9122688558498113| flobal_FPR: 0.4247069431920649 \n",
      "comm_round: 12 | global_acc: 93.019% | global_loss: 0.2549898326396942 | global_f1: 0.9005681818181819 | global_precision: 0.9481555333998006 | global_recall: 0.8575293056807936 | global_auc: 0.9805682930268933| flobal_FPR: 0.1424706943192065 \n",
      "comm_round: 13 | global_acc: 95.745% | global_loss: 0.15052132308483124 | global_f1: 0.9412844036697249 | global_precision: 0.957983193277311 | global_recall: 0.9251577998196574 | global_auc: 0.990826171621816| flobal_FPR: 0.07484220018034266 \n",
      "comm_round: 14 | global_acc: 96.110% | global_loss: 0.11641033738851547 | global_f1: 0.9466970387243736 | global_precision: 0.9567219152854513 | global_recall: 0.9368800721370604 | global_auc: 0.9927003486719553| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 15 | global_acc: 96.609% | global_loss: 0.10184656083583832 | global_f1: 0.9535095715587967 | global_precision: 0.9640552995391705 | global_recall: 0.9431920649233544 | global_auc: 0.9936353004357569| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 16 | global_acc: 96.842% | global_loss: 0.09524668753147125 | global_f1: 0.9567592171142466 | global_precision: 0.9659926470588235 | global_recall: 0.9477006311992786 | global_auc: 0.9941628430510862| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 17 | global_acc: 93.949% | global_loss: 0.15403863787651062 | global_f1: 0.9142318567389255 | global_precision: 0.9575518262586377 | global_recall: 0.8746618575293057 | global_auc: 0.9865835134148246| flobal_FPR: 0.12533814247069433 \n",
      "comm_round: 18 | global_acc: 95.512% | global_loss: 0.11596789956092834 | global_f1: 0.9375866851595007 | global_precision: 0.9620493358633776 | global_recall: 0.9143372407574392 | global_auc: 0.9918484931796954| flobal_FPR: 0.08566275924256087 \n",
      "comm_round: 19 | global_acc: 95.412% | global_loss: 0.11934953927993774 | global_f1: 0.9361702127659575 | global_precision: 0.9610636277302944 | global_recall: 0.9125338142470695 | global_auc: 0.9912715676372785| flobal_FPR: 0.08746618575293057 \n",
      "comm_round: 20 | global_acc: 96.875% | global_loss: 0.08783545345067978 | global_f1: 0.957350272232305 | global_precision: 0.9634703196347032 | global_recall: 0.951307484220018 | global_auc: 0.9945645541695097| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 21 | global_acc: 96.543% | global_loss: 0.09838597476482391 | global_f1: 0.9522935779816515 | global_precision: 0.969187675070028 | global_recall: 0.9359783588818755 | global_auc: 0.9935560028509143| flobal_FPR: 0.06402164111812443 \n",
      "comm_round: 22 | global_acc: 96.077% | global_loss: 0.1130724549293518 | global_f1: 0.9455216989843028 | global_precision: 0.9687795648060549 | global_recall: 0.9233543733092876 | global_auc: 0.9905051825957472| flobal_FPR: 0.07664562669071236 \n",
      "comm_round: 23 | global_acc: 95.346% | global_loss: 0.1291172355413437 | global_f1: 0.9353647276084949 | global_precision: 0.9583727530747398 | global_recall: 0.9134355275022543 | global_auc: 0.9890189464247473| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 24 | global_acc: 95.944% | global_loss: 0.11868218332529068 | global_f1: 0.9436749769159741 | global_precision: 0.9668874172185431 | global_recall: 0.9215509467989179 | global_auc: 0.9891357560407428| flobal_FPR: 0.07844905320108206 \n",
      "comm_round: 25 | global_acc: 96.044% | global_loss: 0.11088679730892181 | global_f1: 0.945387792565397 | global_precision: 0.9626168224299065 | global_recall: 0.9287646528403968 | global_auc: 0.9916091759176558| flobal_FPR: 0.07123534715960325 \n",
      "comm_round: 26 | global_acc: 97.141% | global_loss: 0.07538115978240967 | global_f1: 0.9609800362976407 | global_precision: 0.9671232876712329 | global_recall: 0.9549143372407575 | global_auc: 0.9956224884151926| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 27 | global_acc: 97.374% | global_loss: 0.07059554010629654 | global_f1: 0.964204802899864 | global_precision: 0.9690346083788707 | global_recall: 0.9594229035166817 | global_auc: 0.9960071054434705| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 28 | global_acc: 96.875% | global_loss: 0.08558683842420578 | global_f1: 0.9569202566452796 | global_precision: 0.972972972972973 | global_recall: 0.9413886384129847 | global_auc: 0.994585921782192| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 29 | global_acc: 97.340% | global_loss: 0.068309485912323 | global_f1: 0.9638336347197106 | global_precision: 0.9664551223934723 | global_recall: 0.9612263300270514 | global_auc: 0.9962383504962747| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 30 | global_acc: 97.507% | global_loss: 0.0651705339550972 | global_f1: 0.9660786974219809 | global_precision: 0.969147005444646 | global_recall: 0.9630297565374211 | global_auc: 0.9965873548367491| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 31 | global_acc: 97.207% | global_loss: 0.07109946757555008 | global_f1: 0.9617486338797814 | global_precision: 0.9714811407543699 | global_recall: 0.9522091974752029 | global_auc: 0.9961965649425852| flobal_FPR: 0.047790802524797116 \n",
      "comm_round: 32 | global_acc: 97.540% | global_loss: 0.0661056712269783 | global_f1: 0.9664246823956443 | global_precision: 0.9726027397260274 | global_recall: 0.9603246167718665 | global_auc: 0.99664338546556| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 33 | global_acc: 97.606% | global_loss: 0.06645767390727997 | global_f1: 0.9672727272727273 | global_precision: 0.9752520623281393 | global_recall: 0.9594229035166817 | global_auc: 0.9965826064783753| flobal_FPR: 0.0405770964833183 \n",
      "comm_round: 34 | global_acc: 97.673% | global_loss: 0.06227002665400505 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.9967801381867253| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 35 | global_acc: 97.773% | global_loss: 0.06329505145549774 | global_f1: 0.9696145124716553 | global_precision: 0.9753649635036497 | global_recall: 0.9639314697926059 | global_auc: 0.9966813723325504| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 36 | global_acc: 97.939% | global_loss: 0.06089929863810539 | global_f1: 0.9719710669077758 | global_precision: 0.9746146872166818 | global_recall: 0.9693417493237151 | global_auc: 0.99696959768584| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 37 | global_acc: 97.939% | global_loss: 0.059467244893312454 | global_f1: 0.9720216606498195 | global_precision: 0.9728997289972899 | global_recall: 0.9711451758340848 | global_auc: 0.9970384488822602| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 38 | global_acc: 98.039% | global_loss: 0.05894704535603523 | global_f1: 0.9732668781150884 | global_precision: 0.9781420765027322 | global_recall: 0.9684400360685302 | global_auc: 0.9970436720764714| flobal_FPR: 0.031559963931469794 \n",
      "comm_round: 39 | global_acc: 97.906% | global_loss: 0.06052447482943535 | global_f1: 0.9713506139154162 | global_precision: 0.9798165137614679 | global_recall: 0.9630297565374211 | global_auc: 0.996995713656896| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 40 | global_acc: 98.138% | global_loss: 0.05884220078587532 | global_f1: 0.9746146872166819 | global_precision: 0.9799453053783045 | global_recall: 0.9693417493237151 | global_auc: 0.9970840331226487| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 41 | global_acc: 98.238% | global_loss: 0.0574161559343338 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.9971808996334742| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 42 | global_acc: 98.205% | global_loss: 0.05850818753242493 | global_f1: 0.9756975697569756 | global_precision: 0.9739442946990117 | global_recall: 0.9774571686203787 | global_auc: 0.9971851731560106| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 43 | global_acc: 98.105% | global_loss: 0.06144274026155472 | global_f1: 0.9742431089019431 | global_precision: 0.9764492753623188 | global_recall: 0.9720468890892696 | global_auc: 0.996922114102102| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 44 | global_acc: 98.072% | global_loss: 0.058914914727211 | global_f1: 0.9737556561085974 | global_precision: 0.9772933696639419 | global_recall: 0.9702434625788999 | global_auc: 0.9971714029167267| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 45 | global_acc: 98.205% | global_loss: 0.05722058191895485 | global_f1: 0.9755656108597286 | global_precision: 0.9791099000908265 | global_recall: 0.9720468890892696 | global_auc: 0.9971348405572483| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 46 | global_acc: 98.238% | global_loss: 0.05717035010457039 | global_f1: 0.9760072430964237 | global_precision: 0.98 | global_recall: 0.9720468890892696 | global_auc: 0.997151934647394| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 47 | global_acc: 98.205% | global_loss: 0.058773186057806015 | global_f1: 0.975653742110009 | global_precision: 0.975653742110009 | global_recall: 0.975653742110009 | global_auc: 0.9971557333340931| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 48 | global_acc: 98.205% | global_loss: 0.05830434709787369 | global_f1: 0.9754990925589837 | global_precision: 0.9817351598173516 | global_recall: 0.9693417493237151 | global_auc: 0.9971224948254764| flobal_FPR: 0.030658250676284943 \n",
      "comm_round: 49 | global_acc: 98.238% | global_loss: 0.05774565786123276 | global_f1: 0.9759855006796557 | global_precision: 0.9808743169398907 | global_recall: 0.9711451758340848 | global_auc: 0.9972150878137656| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 50 | global_acc: 98.271% | global_loss: 0.05844935029745102 | global_f1: 0.9764065335753177 | global_precision: 0.982648401826484 | global_recall: 0.9702434625788999 | global_auc: 0.9970707377192021| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 51 | global_acc: 98.305% | global_loss: 0.05800323933362961 | global_f1: 0.976891708201178 | global_precision: 0.9817850637522769 | global_recall: 0.9720468890892696 | global_auc: 0.9971191709746148| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 52 | global_acc: 98.238% | global_loss: 0.05944114178419113 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.9969881162834979| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 53 | global_acc: 98.238% | global_loss: 0.05913037434220314 | global_f1: 0.9760072430964237 | global_precision: 0.98 | global_recall: 0.9720468890892696 | global_auc: 0.9970617158382918| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 54 | global_acc: 98.305% | global_loss: 0.05827997997403145 | global_f1: 0.9769126301493889 | global_precision: 0.980909090909091 | global_recall: 0.9729486023444545 | global_auc: 0.997080709271787| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 55 | global_acc: 98.172% | global_loss: 0.05867737904191017 | global_f1: 0.9751468594667871 | global_precision: 0.9773550724637681 | global_recall: 0.9729486023444545 | global_auc: 0.9970370243747481| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 56 | global_acc: 98.238% | global_loss: 0.058019302785396576 | global_f1: 0.9760072430964237 | global_precision: 0.98 | global_recall: 0.9720468890892696 | global_auc: 0.9970246786429761| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 57 | global_acc: 98.238% | global_loss: 0.0583915188908577 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.9970208799562771| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 58 | global_acc: 98.238% | global_loss: 0.058626797050237656 | global_f1: 0.9759855006796557 | global_precision: 0.9808743169398907 | global_recall: 0.9711451758340848 | global_auc: 0.9969487049089953| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 59 | global_acc: 98.305% | global_loss: 0.059011053293943405 | global_f1: 0.9768707482993196 | global_precision: 0.9826642335766423 | global_recall: 0.9711451758340848 | global_auc: 0.9967668427832787| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 60 | global_acc: 98.305% | global_loss: 0.05764196440577507 | global_f1: 0.9770166741775576 | global_precision: 0.9765765765765766 | global_recall: 0.9774571686203787 | global_auc: 0.9971196458104522| flobal_FPR: 0.02254283137962128 \n",
      "comm_round: 61 | global_acc: 98.172% | global_loss: 0.05756658315658569 | global_f1: 0.9751693002257337 | global_precision: 0.976491862567812 | global_recall: 0.9738503155996393 | global_auc: 0.9969933394777091| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 62 | global_acc: 98.271% | global_loss: 0.05926020070910454 | global_f1: 0.976491862567812 | global_precision: 0.9791477787851315 | global_recall: 0.9738503155996393 | global_auc: 0.9968437661889343| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 63 | global_acc: 98.271% | global_loss: 0.05788750201463699 | global_f1: 0.976491862567812 | global_precision: 0.9791477787851315 | global_recall: 0.9738503155996393 | global_auc: 0.9970341753597238| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 64 | global_acc: 98.305% | global_loss: 0.059004414826631546 | global_f1: 0.9769335142469472 | global_precision: 0.9800362976406534 | global_recall: 0.9738503155996393 | global_auc: 0.9968926742801845| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 65 | global_acc: 98.338% | global_loss: 0.060321271419525146 | global_f1: 0.9773550724637681 | global_precision: 0.9818016378525932 | global_recall: 0.9729486023444545 | global_auc: 0.9966144204794798| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 66 | global_acc: 98.172% | global_loss: 0.05722416192293167 | global_f1: 0.9750792931581332 | global_precision: 0.9799635701275046 | global_recall: 0.9702434625788999 | global_auc: 0.9971139477804036| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 67 | global_acc: 98.205% | global_loss: 0.05928267166018486 | global_f1: 0.9755656108597286 | global_precision: 0.9791099000908265 | global_recall: 0.9720468890892696 | global_auc: 0.9969453810581336| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 68 | global_acc: 98.238% | global_loss: 0.05687788128852844 | global_f1: 0.9760506100316313 | global_precision: 0.9782608695652174 | global_recall: 0.9738503155996393 | global_auc: 0.9971457617815082| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 69 | global_acc: 98.205% | global_loss: 0.05824423208832741 | global_f1: 0.975609756097561 | global_precision: 0.9773755656108597 | global_recall: 0.9738503155996393 | global_auc: 0.996995713656896| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 70 | global_acc: 98.205% | global_loss: 0.0576888807117939 | global_f1: 0.975609756097561 | global_precision: 0.9773755656108597 | global_recall: 0.9738503155996393 | global_auc: 0.9971224948254765| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 71 | global_acc: 98.172% | global_loss: 0.058739423751831055 | global_f1: 0.9751693002257337 | global_precision: 0.976491862567812 | global_recall: 0.9738503155996393 | global_auc: 0.996942057207272| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 72 | global_acc: 98.238% | global_loss: 0.05986487492918968 | global_f1: 0.9760722347629797 | global_precision: 0.9773960216998192 | global_recall: 0.9747520288548241 | global_auc: 0.9967179346920286| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 73 | global_acc: 98.138% | global_loss: 0.05874795466661453 | global_f1: 0.9747292418772563 | global_precision: 0.975609756097561 | global_recall: 0.9738503155996393 | global_auc: 0.9969800440742624| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 74 | global_acc: 98.238% | global_loss: 0.05744938179850578 | global_f1: 0.9760072430964237 | global_precision: 0.98 | global_recall: 0.9720468890892696 | global_auc: 0.9970925801677215| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 75 | global_acc: 98.271% | global_loss: 0.057815808802843094 | global_f1: 0.9764705882352942 | global_precision: 0.9800181653042689 | global_recall: 0.9729486023444545 | global_auc: 0.9970674138683404| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 76 | global_acc: 98.271% | global_loss: 0.05886250361800194 | global_f1: 0.9764492753623188 | global_precision: 0.9808917197452229 | global_recall: 0.9720468890892696 | global_auc: 0.996753547379832| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 77 | global_acc: 98.172% | global_loss: 0.058898113667964935 | global_f1: 0.9750792931581332 | global_precision: 0.9799635701275046 | global_recall: 0.9702434625788999 | global_auc: 0.9969078690269807| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 78 | global_acc: 98.238% | global_loss: 0.05997665971517563 | global_f1: 0.9759637188208617 | global_precision: 0.9817518248175182 | global_recall: 0.9702434625788999 | global_auc: 0.9965607640298557| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 79 | global_acc: 98.205% | global_loss: 0.06096166372299194 | global_f1: 0.9755213055303718 | global_precision: 0.9808568824065633 | global_recall: 0.9702434625788999 | global_auc: 0.9964073920543819| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 80 | global_acc: 98.205% | global_loss: 0.05782270431518555 | global_f1: 0.9755213055303718 | global_precision: 0.9808568824065633 | global_recall: 0.9702434625788999 | global_auc: 0.9970450965839834| flobal_FPR: 0.029756537421100092 \n",
      "comm_round: 81 | global_acc: 98.172% | global_loss: 0.05877280607819557 | global_f1: 0.9751243781094527 | global_precision: 0.9782214156079855 | global_recall: 0.9720468890892696 | global_auc: 0.9970270528221631| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 82 | global_acc: 98.238% | global_loss: 0.05968334153294563 | global_f1: 0.9760072430964237 | global_precision: 0.98 | global_recall: 0.9720468890892696 | global_auc: 0.9966481338239336| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 83 | global_acc: 98.271% | global_loss: 0.059170663356781006 | global_f1: 0.9764705882352942 | global_precision: 0.9800181653042689 | global_recall: 0.9729486023444545 | global_auc: 0.9967364532896864| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 84 | global_acc: 98.238% | global_loss: 0.060353029519319534 | global_f1: 0.9760289461781999 | global_precision: 0.9791288566243194 | global_recall: 0.9729486023444545 | global_auc: 0.996604923762732| flobal_FPR: 0.027051397655545536 \n",
      "comm_round: 85 | global_acc: 98.238% | global_loss: 0.06165534257888794 | global_f1: 0.9759855006796557 | global_precision: 0.9808743169398907 | global_recall: 0.9711451758340848 | global_auc: 0.9963352170071003| flobal_FPR: 0.028854824165915238 \n",
      "comm_round: 86 | global_acc: 98.205% | global_loss: 0.05973700433969498 | global_f1: 0.9755656108597286 | global_precision: 0.9791099000908265 | global_recall: 0.9720468890892696 | global_auc: 0.9967492738572956| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 87 | global_acc: 98.271% | global_loss: 0.05964420735836029 | global_f1: 0.976491862567812 | global_precision: 0.9791477787851315 | global_recall: 0.9738503155996393 | global_auc: 0.9966343635846497| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 88 | global_acc: 98.271% | global_loss: 0.06209535151720047 | global_f1: 0.9764492753623188 | global_precision: 0.9808917197452229 | global_recall: 0.9720468890892696 | global_auc: 0.9963318931562386| flobal_FPR: 0.027953110910730387 \n",
      "comm_round: 89 | global_acc: 98.271% | global_loss: 0.06078849360346794 | global_f1: 0.976491862567812 | global_precision: 0.9791477787851315 | global_recall: 0.9738503155996393 | global_auc: 0.9964581994889816| flobal_FPR: 0.026149684400360685 \n",
      "comm_round: 90 | global_acc: 98.305% | global_loss: 0.0620490238070488 | global_f1: 0.9769751693002258 | global_precision: 0.9783001808318263 | global_recall: 0.975653742110009 | global_auc: 0.9963546852764327| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 91 | global_acc: 98.305% | global_loss: 0.060407206416130066 | global_f1: 0.9769543605964752 | global_precision: 0.9791666666666666 | global_recall: 0.9747520288548241 | global_auc: 0.9964857399675497| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 92 | global_acc: 98.338% | global_loss: 0.0608329214155674 | global_f1: 0.9774368231046933 | global_precision: 0.978319783197832 | global_recall: 0.9765554553651938 | global_auc: 0.9965009347143459| flobal_FPR: 0.023444544634806132 \n",
      "comm_round: 93 | global_acc: 98.305% | global_loss: 0.06030286103487015 | global_f1: 0.9769751693002258 | global_precision: 0.9783001808318263 | global_recall: 0.975653742110009 | global_auc: 0.9965864051650744| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 94 | global_acc: 98.305% | global_loss: 0.05981486290693283 | global_f1: 0.9769543605964752 | global_precision: 0.9791666666666666 | global_recall: 0.9747520288548241 | global_auc: 0.9966058734344069| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 95 | global_acc: 98.305% | global_loss: 0.06120722368359566 | global_f1: 0.9769543605964752 | global_precision: 0.9791666666666666 | global_recall: 0.9747520288548241 | global_auc: 0.9964729193999405| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 96 | global_acc: 98.305% | global_loss: 0.06086983531713486 | global_f1: 0.9769543605964752 | global_precision: 0.9791666666666666 | global_recall: 0.9747520288548241 | global_auc: 0.9965147049536299| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 97 | global_acc: 98.305% | global_loss: 0.06170467659831047 | global_f1: 0.9769543605964752 | global_precision: 0.9791666666666666 | global_recall: 0.9747520288548241 | global_auc: 0.9964311338462508| flobal_FPR: 0.025247971145175834 \n",
      "comm_round: 98 | global_acc: 98.305% | global_loss: 0.06123283505439758 | global_f1: 0.9769751693002258 | global_precision: 0.9783001808318263 | global_recall: 0.975653742110009 | global_auc: 0.9964995102068337| flobal_FPR: 0.024346257889990983 \n",
      "comm_round: 99 | global_acc: 98.371% | global_loss: 0.06369873136281967 | global_f1: 0.9778581111613194 | global_precision: 0.980072463768116 | global_recall: 0.975653742110009 | global_auc: 0.9963508865897338| flobal_FPR: 0.024346257889990983 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedProx 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx-non-iid-FGSM  2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model_prox, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model_prox:\n",
    "                        train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model_prox(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-non-iid-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-non-iid-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-non-iid-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45d522b1",
   "metadata": {},
   "source": [
    "## iid FedProx FGSM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32db8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedProx-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.032% | global_loss: 0.6772813200950623 | global_f1: 0.0 | global_precision: 0.0 | global_recall: 0.0 | global_auc: 0.5713545309547857| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6695433855056763 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6591087046430872| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6644044518470764 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.6998016610707264| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.661167323589325 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7144285042053836| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6585276126861572 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7313552147183915| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6564632654190063 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.745848628982745| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6540607810020447 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7674527099118658| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.652336835861206 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7812960739148458| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.6504459977149963 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7985288161250452| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.6499180197715759 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8031541920169649| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.6482310891151428 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8159260889528968| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.6447635293006897 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.83685732750045| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.644227147102356 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8431586364804029| flobal_FPR: 1.0 \n",
      "comm_round: 13 | global_acc: 63.132% | global_loss: 0.6420020461082458 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8550292949969871| flobal_FPR: 1.0 \n",
      "comm_round: 14 | global_acc: 63.132% | global_loss: 0.6402321457862854 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8644158498303175| flobal_FPR: 1.0 \n",
      "comm_round: 15 | global_acc: 63.132% | global_loss: 0.6390612125396729 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8716832123214202| flobal_FPR: 1.0 \n",
      "comm_round: 16 | global_acc: 63.132% | global_loss: 0.6349180936813354 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8842034462635406| flobal_FPR: 1.0 \n",
      "comm_round: 17 | global_acc: 63.132% | global_loss: 0.6352534890174866 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8885949655055507| flobal_FPR: 1.0 \n",
      "comm_round: 18 | global_acc: 63.132% | global_loss: 0.6353948712348938 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8912032387602796| flobal_FPR: 1.0 \n",
      "comm_round: 19 | global_acc: 63.132% | global_loss: 0.6329004168510437 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8984250170109939| flobal_FPR: 1.0 \n",
      "comm_round: 20 | global_acc: 63.132% | global_loss: 0.6292490363121033 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.905962561093566| flobal_FPR: 1.0 \n",
      "comm_round: 21 | global_acc: 63.132% | global_loss: 0.6271894574165344 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9100091120997194| flobal_FPR: 1.0 \n",
      "comm_round: 22 | global_acc: 63.132% | global_loss: 0.6261592507362366 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9131150133120227| flobal_FPR: 1.0 \n",
      "comm_round: 23 | global_acc: 63.132% | global_loss: 0.6271713376045227 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9138856718760907| flobal_FPR: 1.0 \n",
      "comm_round: 24 | global_acc: 63.132% | global_loss: 0.6222973465919495 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9204061175949945| flobal_FPR: 1.0 \n",
      "comm_round: 25 | global_acc: 63.132% | global_loss: 0.6154184341430664 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9276592350109758| flobal_FPR: 1.0 \n",
      "comm_round: 26 | global_acc: 63.132% | global_loss: 0.6070498824119568 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9328472913701912| flobal_FPR: 1.0 \n",
      "comm_round: 27 | global_acc: 63.132% | global_loss: 0.5951077342033386 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9392772808620741| flobal_FPR: 1.0 \n",
      "comm_round: 28 | global_acc: 63.132% | global_loss: 0.5879027843475342 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9430947235766914| flobal_FPR: 1.0 \n",
      "comm_round: 29 | global_acc: 63.132% | global_loss: 0.5851500034332275 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9441996665702752| flobal_FPR: 1.0 \n",
      "comm_round: 30 | global_acc: 63.132% | global_loss: 0.5790061354637146 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.94747745835571| flobal_FPR: 1.0 \n",
      "comm_round: 31 | global_acc: 63.132% | global_loss: 0.5690019726753235 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9510900094064979| flobal_FPR: 1.0 \n",
      "comm_round: 32 | global_acc: 63.132% | global_loss: 0.5613607168197632 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9530748232067467| flobal_FPR: 1.0 \n",
      "comm_round: 33 | global_acc: 63.132% | global_loss: 0.5599836111068726 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9532927728561044| flobal_FPR: 1.0 \n",
      "comm_round: 34 | global_acc: 63.132% | global_loss: 0.553981363773346 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9546284860666547| flobal_FPR: 1.0 \n",
      "comm_round: 35 | global_acc: 66.157% | global_loss: 0.5313711166381836 | global_f1: 0.15166666666666664 | global_precision: 1.0 | global_recall: 0.08205590622182146 | global_auc: 0.9602282250968783| flobal_FPR: 0.9179440937781785 \n",
      "comm_round: 36 | global_acc: 67.586% | global_loss: 0.5242716073989868 | global_f1: 0.21560740144810944 | global_precision: 1.0 | global_recall: 0.12082957619477007 | global_auc: 0.9610031571834827| flobal_FPR: 0.8791704238052299 \n",
      "comm_round: 37 | global_acc: 72.906% | global_loss: 0.5068151354789734 | global_f1: 0.4199288256227758 | global_precision: 0.9966216216216216 | global_recall: 0.2660054102795311 | global_auc: 0.9650145703376701| flobal_FPR: 0.7339945897204689 \n",
      "comm_round: 38 | global_acc: 82.447% | global_loss: 0.47698166966438293 | global_f1: 0.6883116883116882 | global_precision: 0.9965811965811966 | global_recall: 0.5256988277727682 | global_auc: 0.9706660664741682| flobal_FPR: 0.4743011722272317 \n",
      "comm_round: 39 | global_acc: 84.043% | global_loss: 0.4683992862701416 | global_f1: 0.7257142857142858 | global_precision: 0.9906396255850234 | global_recall: 0.5725879170423805 | global_auc: 0.9710839220110628| flobal_FPR: 0.42741208295761945 \n",
      "comm_round: 40 | global_acc: 86.968% | global_loss: 0.45199644565582275 | global_f1: 0.7871878393051033 | global_precision: 0.9890859481582538 | global_recall: 0.6537421100090172 | global_auc: 0.9723379634575837| flobal_FPR: 0.3462578899909829 \n",
      "comm_round: 41 | global_acc: 89.594% | global_loss: 0.43466857075691223 | global_f1: 0.8375713544369485 | global_precision: 0.9865525672371638 | global_recall: 0.7276825969341749 | global_auc: 0.9738688341972972| flobal_FPR: 0.2723174030658251 \n",
      "comm_round: 42 | global_acc: 91.722% | global_loss: 0.4101698100566864 | global_f1: 0.8758104738154614 | global_precision: 0.9799107142857143 | global_recall: 0.7917042380522994 | global_auc: 0.9771793896555114| flobal_FPR: 0.20829576194770064 \n",
      "comm_round: 43 | global_acc: 92.985% | global_loss: 0.3778223991394043 | global_f1: 0.897323600973236 | global_precision: 0.9746300211416491 | global_recall: 0.8313796212804329 | global_auc: 0.9796105491428976| flobal_FPR: 0.16862037871956717 \n",
      "comm_round: 44 | global_acc: 93.551% | global_loss: 0.3395817279815674 | global_f1: 0.907088122605364 | global_precision: 0.9673135852911133 | global_recall: 0.8539224526600541 | global_auc: 0.9821376254694346| flobal_FPR: 0.1460775473399459 \n",
      "comm_round: 45 | global_acc: 93.551% | global_loss: 0.3307369351387024 | global_f1: 0.907088122605364 | global_precision: 0.9673135852911133 | global_recall: 0.8539224526600541 | global_auc: 0.9814334439225999| flobal_FPR: 0.1460775473399459 \n",
      "comm_round: 46 | global_acc: 94.282% | global_loss: 0.2999463379383087 | global_f1: 0.9189443920829407 | global_precision: 0.9624876604146101 | global_recall: 0.8791704238052299 | global_auc: 0.9830588069939521| flobal_FPR: 0.12082957619477007 \n",
      "comm_round: 47 | global_acc: 94.282% | global_loss: 0.285295307636261 | global_f1: 0.9192488262910798 | global_precision: 0.9588638589618022 | global_recall: 0.8827772768259693 | global_auc: 0.9831029667268284| flobal_FPR: 0.11722272317403065 \n",
      "comm_round: 48 | global_acc: 94.448% | global_loss: 0.2515869438648224 | global_f1: 0.9223616922361692 | global_precision: 0.9520153550863724 | global_recall: 0.8944995491433724 | global_auc: 0.9850051590913731| flobal_FPR: 0.1055004508566276 \n",
      "comm_round: 49 | global_acc: 93.883% | global_loss: 0.2532849907875061 | global_f1: 0.9139382600561271 | global_precision: 0.9494655004859086 | global_recall: 0.8809738503155996 | global_auc: 0.9833266144062345| flobal_FPR: 0.11902614968440037 \n",
      "comm_round: 50 | global_acc: 93.983% | global_loss: 0.24701742827892303 | global_f1: 0.9153810191678354 | global_precision: 0.9504854368932039 | global_recall: 0.8827772768259693 | global_auc: 0.9833332621079578| flobal_FPR: 0.11722272317403065 \n",
      "comm_round: 51 | global_acc: 94.348% | global_loss: 0.2265257090330124 | global_f1: 0.9211502782931353 | global_precision: 0.9484240687679083 | global_recall: 0.8954012623985572 | global_auc: 0.9847591941276103| flobal_FPR: 0.10459873760144274 \n",
      "comm_round: 52 | global_acc: 93.916% | global_loss: 0.22826412320137024 | global_f1: 0.9144460028050491 | global_precision: 0.9495145631067962 | global_recall: 0.8818755635707844 | global_auc: 0.9834932817851549| flobal_FPR: 0.11812443642921551 \n",
      "comm_round: 53 | global_acc: 94.714% | global_loss: 0.20254236459732056 | global_f1: 0.926490984743412 | global_precision: 0.9506641366223909 | global_recall: 0.9035166816952209 | global_auc: 0.9858874040772254| flobal_FPR: 0.09648331830477908 \n",
      "comm_round: 54 | global_acc: 94.614% | global_loss: 0.19423829019069672 | global_f1: 0.9250693802035153 | global_precision: 0.949667616334283 | global_recall: 0.9017132551848512 | global_auc: 0.986079237755527| flobal_FPR: 0.09828674481514878 \n",
      "comm_round: 55 | global_acc: 94.348% | global_loss: 0.1910320222377777 | global_f1: 0.9206349206349206 | global_precision: 0.9545014520813165 | global_recall: 0.8890892696122633 | global_auc: 0.9860132355741311| flobal_FPR: 0.1109107303877367 \n",
      "comm_round: 56 | global_acc: 94.515% | global_loss: 0.18161094188690186 | global_f1: 0.9232914923291492 | global_precision: 0.9529750479846449 | global_recall: 0.8954012623985572 | global_auc: 0.9868090604375802| flobal_FPR: 0.10459873760144274 \n",
      "comm_round: 57 | global_acc: 94.880% | global_loss: 0.1689586043357849 | global_f1: 0.9287037037037037 | global_precision: 0.9543292102759277 | global_recall: 0.9044183949504058 | global_auc: 0.9880141937928509| flobal_FPR: 0.09558160504959423 \n",
      "comm_round: 58 | global_acc: 95.180% | global_loss: 0.1517975628376007 | global_f1: 0.9330254041570438 | global_precision: 0.9564393939393939 | global_recall: 0.9107303877366997 | global_auc: 0.9895175240539965| flobal_FPR: 0.08926961226330027 \n",
      "comm_round: 59 | global_acc: 95.213% | global_loss: 0.1455799639225006 | global_f1: 0.9335180055401663 | global_precision: 0.956480605487228 | global_recall: 0.9116321009918846 | global_auc: 0.9899719419503693| flobal_FPR: 0.08836789900811542 \n",
      "comm_round: 60 | global_acc: 94.880% | global_loss: 0.15261168777942657 | global_f1: 0.9287696577243293 | global_precision: 0.9534662867996201 | global_recall: 0.9053201082055906 | global_auc: 0.9887354694298314| flobal_FPR: 0.09467989179440937 \n",
      "comm_round: 61 | global_acc: 95.146% | global_loss: 0.14209601283073425 | global_f1: 0.932780847145488 | global_precision: 0.9529633113828786 | global_recall: 0.9134355275022543 | global_auc: 0.9899624452336216| flobal_FPR: 0.08656447249774572 \n",
      "comm_round: 62 | global_acc: 94.947% | global_loss: 0.14405986666679382 | global_f1: 0.9295644114921224 | global_precision: 0.9561487130600572 | global_recall: 0.9044183949504058 | global_auc: 0.9894434496633651| flobal_FPR: 0.09558160504959423 \n",
      "comm_round: 63 | global_acc: 95.445% | global_loss: 0.13426971435546875 | global_f1: 0.9367789570835255 | global_precision: 0.9593572778827977 | global_recall: 0.915238954012624 | global_auc: 0.990607747136621| flobal_FPR: 0.08476104598737602 \n",
      "comm_round: 64 | global_acc: 95.612% | global_loss: 0.12566927075386047 | global_f1: 0.9390581717451524 | global_precision: 0.9621570482497634 | global_recall: 0.9170423805229937 | global_auc: 0.9914230402694029| flobal_FPR: 0.0829576194770063 \n",
      "comm_round: 65 | global_acc: 95.645% | global_loss: 0.12139733135700226 | global_f1: 0.9394919168591224 | global_precision: 0.9630681818181818 | global_recall: 0.9170423805229937 | global_auc: 0.9917288345486756| flobal_FPR: 0.0829576194770063 \n",
      "comm_round: 66 | global_acc: 96.210% | global_loss: 0.11204618215560913 | global_f1: 0.9477064220183486 | global_precision: 0.9645191409897292 | global_recall: 0.9314697926059513 | global_auc: 0.9925517250548554| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 67 | global_acc: 96.277% | global_loss: 0.11021383106708527 | global_f1: 0.9485766758494031 | global_precision: 0.9663236669784846 | global_recall: 0.9314697926059513 | global_auc: 0.9926381451772586| flobal_FPR: 0.06853020739404869 \n",
      "comm_round: 68 | global_acc: 96.576% | global_loss: 0.1049778088927269 | global_f1: 0.9529465509365007 | global_precision: 0.9657407407407408 | global_recall: 0.9404869251577999 | global_auc: 0.9930806921776969| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 69 | global_acc: 96.676% | global_loss: 0.10624299943447113 | global_f1: 0.9541704857928506 | global_precision: 0.9701770736253494 | global_recall: 0.9386834986474302 | global_auc: 0.9928888584993952| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 70 | global_acc: 96.775% | global_loss: 0.10094308853149414 | global_f1: 0.9556064073226546 | global_precision: 0.9702602230483272 | global_recall: 0.9413886384129847 | global_auc: 0.993305764364615| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 71 | global_acc: 96.676% | global_loss: 0.10034585744142532 | global_f1: 0.9542124542124543 | global_precision: 0.9693023255813954 | global_recall: 0.939585211902615 | global_auc: 0.993401681203766| flobal_FPR: 0.060414788097385035 \n",
      "comm_round: 72 | global_acc: 96.842% | global_loss: 0.09829378873109818 | global_f1: 0.9565614997713763 | global_precision: 0.9703153988868275 | global_recall: 0.9431920649233544 | global_auc: 0.9935987380762786| flobal_FPR: 0.056807935076645624 \n",
      "comm_round: 73 | global_acc: 96.709% | global_loss: 0.10019790381193161 | global_f1: 0.9546910755148741 | global_precision: 0.9693308550185874 | global_recall: 0.9404869251577999 | global_auc: 0.9933632195009379| flobal_FPR: 0.059513074842200184 \n",
      "comm_round: 74 | global_acc: 96.875% | global_loss: 0.09576769173145294 | global_f1: 0.9570776255707762 | global_precision: 0.969472710453284 | global_recall: 0.9449954914337241 | global_auc: 0.9937473616933786| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 75 | global_acc: 96.875% | global_loss: 0.09497260302305222 | global_f1: 0.9570776255707762 | global_precision: 0.969472710453284 | global_recall: 0.9449954914337241 | global_auc: 0.9938062413372137| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 76 | global_acc: 96.875% | global_loss: 0.09376049786806107 | global_f1: 0.9570776255707762 | global_precision: 0.969472710453284 | global_recall: 0.9449954914337241 | global_auc: 0.9939866789554181| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 77 | global_acc: 96.941% | global_loss: 0.09102718532085419 | global_f1: 0.958029197080292 | global_precision: 0.9695290858725761 | global_recall: 0.9467989179440938 | global_auc: 0.9942212478590838| flobal_FPR: 0.05320108205590622 \n",
      "comm_round: 78 | global_acc: 96.875% | global_loss: 0.08905268460512161 | global_f1: 0.9571167883211679 | global_precision: 0.9686057248384118 | global_recall: 0.9458972046889089 | global_auc: 0.9943831668796306| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 79 | global_acc: 97.141% | global_loss: 0.08713500946760178 | global_f1: 0.9609090909090909 | global_precision: 0.9688359303391384 | global_recall: 0.9531109107303878 | global_auc: 0.9945156460782596| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 80 | global_acc: 97.141% | global_loss: 0.08693064004182816 | global_f1: 0.9608378870673951 | global_precision: 0.9705611775528978 | global_recall: 0.951307484220018 | global_auc: 0.9945693025278834| flobal_FPR: 0.04869251577998197 \n",
      "comm_round: 81 | global_acc: 97.274% | global_loss: 0.08387072384357452 | global_f1: 0.9627611262488647 | global_precision: 0.969807868252516 | global_recall: 0.9558160504959423 | global_auc: 0.9947430924443648| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 82 | global_acc: 97.241% | global_loss: 0.08331100642681122 | global_f1: 0.9622212107419208 | global_precision: 0.9715073529411765 | global_recall: 0.9531109107303878 | global_auc: 0.9948181165066707| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 83 | global_acc: 97.307% | global_loss: 0.08222660422325134 | global_f1: 0.9631315430131998 | global_precision: 0.9724264705882353 | global_recall: 0.9540126239855726 | global_auc: 0.9949567685711856| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 84 | global_acc: 97.374% | global_loss: 0.0801243856549263 | global_f1: 0.964074579354252 | global_precision: 0.9724770642201835 | global_recall: 0.9558160504959423 | global_auc: 0.9950816503964166| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 85 | global_acc: 97.340% | global_loss: 0.08096493035554886 | global_f1: 0.9636363636363636 | global_precision: 0.9715857011915674 | global_recall: 0.9558160504959423 | global_auc: 0.9951148889050333| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 86 | global_acc: 97.307% | global_loss: 0.08094043284654617 | global_f1: 0.9631315430131998 | global_precision: 0.9724264705882353 | global_recall: 0.9540126239855726 | global_auc: 0.9951315081593416| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 87 | global_acc: 97.241% | global_loss: 0.080576092004776 | global_f1: 0.9621177544500228 | global_precision: 0.9741219963031423 | global_recall: 0.9504057709648331 | global_auc: 0.9951338823385284| flobal_FPR: 0.04959422903516682 \n",
      "comm_round: 88 | global_acc: 97.374% | global_loss: 0.07842830568552017 | global_f1: 0.9640418752844789 | global_precision: 0.9733455882352942 | global_recall: 0.9549143372407575 | global_auc: 0.9952953265232377| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 89 | global_acc: 97.307% | global_loss: 0.07898196578025818 | global_f1: 0.9630979498861048 | global_precision: 0.9732965009208103 | global_recall: 0.9531109107303878 | global_auc: 0.995258289327922| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 90 | global_acc: 97.307% | global_loss: 0.0780353769659996 | global_f1: 0.9631315430131998 | global_precision: 0.9724264705882353 | global_recall: 0.9540126239855726 | global_auc: 0.9953361624052525| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 91 | global_acc: 97.374% | global_loss: 0.07602869719266891 | global_f1: 0.9640418752844789 | global_precision: 0.9733455882352942 | global_recall: 0.9549143372407575 | global_auc: 0.9954824118431655| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 92 | global_acc: 97.340% | global_loss: 0.07559648156166077 | global_f1: 0.9635701275045537 | global_precision: 0.9733210671573137 | global_recall: 0.9540126239855726 | global_auc: 0.9955835518765275| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 93 | global_acc: 97.307% | global_loss: 0.07535149902105331 | global_f1: 0.9630979498861048 | global_precision: 0.9732965009208103 | global_recall: 0.9531109107303878 | global_auc: 0.9955702564730808| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 94 | global_acc: 97.374% | global_loss: 0.0758386179804802 | global_f1: 0.9639762881896945 | global_precision: 0.9750922509225092 | global_recall: 0.9531109107303878 | global_auc: 0.9955769041748042| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 95 | global_acc: 97.340% | global_loss: 0.07417735457420349 | global_f1: 0.9635369188696445 | global_precision: 0.9741935483870968 | global_recall: 0.9531109107303878 | global_auc: 0.9956581011029962| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 96 | global_acc: 97.340% | global_loss: 0.07504487782716751 | global_f1: 0.9635369188696445 | global_precision: 0.9741935483870968 | global_recall: 0.9531109107303878 | global_auc: 0.995609193011746| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 97 | global_acc: 97.307% | global_loss: 0.07374448329210281 | global_f1: 0.9631650750341064 | global_precision: 0.9715596330275229 | global_recall: 0.9549143372407575 | global_auc: 0.9956870660890763| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 98 | global_acc: 97.374% | global_loss: 0.07330971211194992 | global_f1: 0.9640091116173121 | global_precision: 0.9742173112338858 | global_recall: 0.9540126239855726 | global_auc: 0.9957070091942464| flobal_FPR: 0.045987376014427414 \n",
      "comm_round: 99 | global_acc: 97.440% | global_loss: 0.07248794287443161 | global_f1: 0.9649522075557578 | global_precision: 0.9742647058823529 | global_recall: 0.9558160504959423 | global_auc: 0.9957207794335303| flobal_FPR: 0.04418394950405771 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def fgsm_attack(model, X, y, epsilon,loss, optimizer):\n",
    "    # Generate adversarial examples\n",
    "    model.eval()\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "    outputs = model(X_adv)\n",
    "    loss = loss(outputs, y)\n",
    "    # print(\"adver\" , loss)\n",
    "    loss.backward()\n",
    "    grad_sign = X_adv.grad.data.sign()\n",
    "    X_adv = X_adv + epsilon * grad_sign\n",
    "    X_adv = torch.clamp(X_adv, 0, 1)\n",
    "    train_loader = DataLoader(TensorDataset(torch.tensor(X_adv, dtype=torch.float32),\n",
    "                                        torch.tensor(y, dtype=torch.float32)),\n",
    "                          batch_size=32, shuffle=True)\n",
    "    return train_loader\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedProx 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "\n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model_prox, fgsm_attack]\n",
    "\n",
    "                    # Randomly choose between normal training and FGSM attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "\n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model_prox:\n",
    "                        train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == fgsm_attack:\n",
    "                        epsilon = 1\n",
    "                        X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, loss, optimizer)\n",
    "                        train_model_prox(local_model, X_adv, loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5238cac1",
   "metadata": {},
   "source": [
    "## iid FedProx PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0127a316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================================================\n",
      "Working with: Drebin\n",
      "===================================================================================================\n",
      "---------------------------------------------\n",
      "No. of Clients: 10\n",
      "No. of Rounds: 100\n",
      "---------------------------------------------\n",
      "|=======================|\n",
      "|Traditional FedProx-FGSM 2017|\n",
      "|=======================|\n",
      "comm_round: 0 | global_acc: 63.132% | global_loss: 0.6690062284469604 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7360748455240311| flobal_FPR: 1.0 \n",
      "comm_round: 1 | global_acc: 63.132% | global_loss: 0.6604980230331421 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7693736582919869| flobal_FPR: 1.0 \n",
      "comm_round: 2 | global_acc: 63.132% | global_loss: 0.6524298787117004 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.7960048262314512| flobal_FPR: 1.0 \n",
      "comm_round: 3 | global_acc: 63.132% | global_loss: 0.6417957544326782 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8281763787214667| flobal_FPR: 1.0 \n",
      "comm_round: 4 | global_acc: 63.132% | global_loss: 0.6307938694953918 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8493820723830253| flobal_FPR: 1.0 \n",
      "comm_round: 5 | global_acc: 63.132% | global_loss: 0.6175049543380737 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8676015234633007| flobal_FPR: 1.0 \n",
      "comm_round: 6 | global_acc: 63.132% | global_loss: 0.6008539199829102 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.8869769623896777| flobal_FPR: 1.0 \n",
      "comm_round: 7 | global_acc: 63.132% | global_loss: 0.5828443765640259 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9015883733596204| flobal_FPR: 1.0 \n",
      "comm_round: 8 | global_acc: 63.132% | global_loss: 0.5630877017974854 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9134692408467082| flobal_FPR: 1.0 \n",
      "comm_round: 9 | global_acc: 63.132% | global_loss: 0.5411218404769897 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9253871455291119| flobal_FPR: 1.0 \n",
      "comm_round: 10 | global_acc: 63.132% | global_loss: 0.5186693072319031 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9369911837230074| flobal_FPR: 1.0 \n",
      "comm_round: 11 | global_acc: 63.132% | global_loss: 0.4947819709777832 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.948901491032013| flobal_FPR: 1.0 \n",
      "comm_round: 12 | global_acc: 63.132% | global_loss: 0.4720335304737091 | global_f1: 0.0 | global_precision: 1.0 | global_recall: 0.0 | global_auc: 0.9584371443182806| flobal_FPR: 1.0 \n",
      "comm_round: 13 | global_acc: 63.830% | global_loss: 0.4473482370376587 | global_f1: 0.03716814159292035 | global_precision: 1.0 | global_recall: 0.018935978358881875 | global_auc: 0.9678787801087468| flobal_FPR: 0.9810640216411182 \n",
      "comm_round: 14 | global_acc: 71.509% | global_loss: 0.42577117681503296 | global_f1: 0.37031594415870683 | global_precision: 1.0 | global_recall: 0.2272317403065825 | global_auc: 0.9749495605631744| flobal_FPR: 0.7727682596934174 \n",
      "comm_round: 15 | global_acc: 85.672% | global_loss: 0.4041038751602173 | global_f1: 0.7590832867523757 | global_precision: 0.9985294117647059 | global_recall: 0.612263300270514 | global_auc: 0.979349864268176| flobal_FPR: 0.38773669972948605 \n",
      "comm_round: 16 | global_acc: 89.927% | global_loss: 0.38506531715393066 | global_f1: 0.8430864836872088 | global_precision: 0.9902676399026764 | global_recall: 0.7339945897204689 | global_auc: 0.9820336364210484| flobal_FPR: 0.2660054102795311 \n",
      "comm_round: 17 | global_acc: 91.755% | global_loss: 0.3657751679420471 | global_f1: 0.8753768844221105 | global_precision: 0.9886492622020431 | global_recall: 0.7853922452660054 | global_auc: 0.9839401023081296| flobal_FPR: 0.2146077547339946 \n",
      "comm_round: 18 | global_acc: 93.019% | global_loss: 0.35023704171180725 | global_f1: 0.896551724137931 | global_precision: 0.988056460369164 | global_recall: 0.8205590622182146 | global_auc: 0.9849899643445771| flobal_FPR: 0.17944093778178538 \n",
      "comm_round: 19 | global_acc: 93.551% | global_loss: 0.3323209881782532 | global_f1: 0.9054580896686159 | global_precision: 0.985153764581124 | global_recall: 0.8376916140667268 | global_auc: 0.9858114303432446| flobal_FPR: 0.1623083859332732 \n",
      "comm_round: 20 | global_acc: 93.983% | global_loss: 0.3163444995880127 | global_f1: 0.9125181246979218 | global_precision: 0.9833333333333333 | global_recall: 0.8512173128944995 | global_auc: 0.986366513437142| flobal_FPR: 0.14878268710550044 \n",
      "comm_round: 21 | global_acc: 94.448% | global_loss: 0.2999122738838196 | global_f1: 0.9199808337326305 | global_precision: 0.9815950920245399 | global_recall: 0.8656447249774571 | global_auc: 0.9867459072712088| flobal_FPR: 0.13435527502254282 \n",
      "comm_round: 22 | global_acc: 94.448% | global_loss: 0.28599950671195984 | global_f1: 0.9205140409328891 | global_precision: 0.9747983870967742 | global_recall: 0.8719567177637512 | global_auc: 0.9870194127135395| flobal_FPR: 0.12804328223624886 \n",
      "comm_round: 23 | global_acc: 94.515% | global_loss: 0.27218493819236755 | global_f1: 0.9216152019002375 | global_precision: 0.9738955823293173 | global_recall: 0.8746618575293057 | global_auc: 0.9872853207824726| flobal_FPR: 0.12533814247069433 \n",
      "comm_round: 24 | global_acc: 94.714% | global_loss: 0.25948667526245117 | global_f1: 0.9246802463287541 | global_precision: 0.9740518962075848 | global_recall: 0.8800721370604148 | global_auc: 0.9875194148503009| flobal_FPR: 0.11992786293958521 \n",
      "comm_round: 25 | global_acc: 94.714% | global_loss: 0.24427157640457153 | global_f1: 0.9247515380974917 | global_precision: 0.9731075697211156 | global_recall: 0.8809738503155996 | global_auc: 0.9879543644773412| flobal_FPR: 0.11902614968440037 \n",
      "comm_round: 26 | global_acc: 94.814% | global_loss: 0.2285778820514679 | global_f1: 0.9264844486333648 | global_precision: 0.9703849950641659 | global_recall: 0.8863841298467088 | global_auc: 0.9886628195467122| flobal_FPR: 0.11361587015329125 \n",
      "comm_round: 27 | global_acc: 94.914% | global_loss: 0.21668803691864014 | global_f1: 0.9279999999999999 | global_precision: 0.9704724409448819 | global_recall: 0.8890892696122633 | global_auc: 0.9889709880051719| flobal_FPR: 0.1109107303877367 \n",
      "comm_round: 28 | global_acc: 94.980% | global_loss: 0.2043846994638443 | global_f1: 0.9290746829497416 | global_precision: 0.9696078431372549 | global_recall: 0.8917944093778178 | global_auc: 0.9891723184002211| flobal_FPR: 0.10820559062218214 \n",
      "comm_round: 29 | global_acc: 95.146% | global_loss: 0.1901772916316986 | global_f1: 0.9316479400749064 | global_precision: 0.9688412852969815 | global_recall: 0.8972046889089269 | global_auc: 0.9896319594908052| flobal_FPR: 0.10279531109107304 \n",
      "comm_round: 30 | global_acc: 95.279% | global_loss: 0.17853081226348877 | global_f1: 0.9337068160597572 | global_precision: 0.968054211035818 | global_recall: 0.9017132551848512 | global_auc: 0.9899524736810366| flobal_FPR: 0.09828674481514878 \n",
      "comm_round: 31 | global_acc: 95.346% | global_loss: 0.16781601309776306 | global_f1: 0.9347623485554519 | global_precision: 0.9672131147540983 | global_recall: 0.9044183949504058 | global_auc: 0.9903161979324697| flobal_FPR: 0.09558160504959423 \n",
      "comm_round: 32 | global_acc: 95.512% | global_loss: 0.15793170034885406 | global_f1: 0.9372384937238494 | global_precision: 0.9673704414587332 | global_recall: 0.90892696122633 | global_auc: 0.9905678609262814| flobal_FPR: 0.09107303877366997 \n",
      "comm_round: 33 | global_acc: 95.479% | global_loss: 0.14906834065914154 | global_f1: 0.93686165273909 | global_precision: 0.9655502392344497 | global_recall: 0.9098286744815148 | global_auc: 0.990873655205554| flobal_FPR: 0.09017132551848513 \n",
      "comm_round: 34 | global_acc: 95.512% | global_loss: 0.14024949073791504 | global_f1: 0.9375866851595007 | global_precision: 0.9620493358633776 | global_recall: 0.9143372407574392 | global_auc: 0.9912174363518174| flobal_FPR: 0.08566275924256087 \n",
      "comm_round: 35 | global_acc: 95.545% | global_loss: 0.13330386579036713 | global_f1: 0.938134810710988 | global_precision: 0.9612109744560076 | global_recall: 0.9161406672678089 | global_auc: 0.9914823947490755| flobal_FPR: 0.08385933273219116 \n",
      "comm_round: 36 | global_acc: 95.911% | global_loss: 0.12708932161331177 | global_f1: 0.943448275862069 | global_precision: 0.9624765478424016 | global_recall: 0.9251577998196574 | global_auc: 0.9917411802804474| flobal_FPR: 0.07484220018034266 \n",
      "comm_round: 37 | global_acc: 95.911% | global_loss: 0.12215457856655121 | global_f1: 0.943448275862069 | global_precision: 0.9624765478424016 | global_recall: 0.9251577998196574 | global_auc: 0.991921617898652| flobal_FPR: 0.07484220018034266 \n",
      "comm_round: 38 | global_acc: 96.376% | global_loss: 0.11709700524806976 | global_f1: 0.9501600365797896 | global_precision: 0.963821892393321 | global_recall: 0.9368800721370604 | global_auc: 0.9921827776092111| flobal_FPR: 0.06311992786293959 \n",
      "comm_round: 39 | global_acc: 96.443% | global_loss: 0.1132771447300911 | global_f1: 0.9511192325262676 | global_precision: 0.9638888888888889 | global_recall: 0.9386834986474302 | global_auc: 0.9923584668690416| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 40 | global_acc: 96.509% | global_loss: 0.1096557229757309 | global_f1: 0.9519890260631002 | global_precision: 0.9656771799628943 | global_recall: 0.9386834986474302 | global_auc: 0.9925218103971005| flobal_FPR: 0.061316501352569885 \n",
      "comm_round: 41 | global_acc: 96.576% | global_loss: 0.10578452050685883 | global_f1: 0.9529895025102693 | global_precision: 0.9648798521256932 | global_recall: 0.9413886384129847 | global_auc: 0.9927791714209606| flobal_FPR: 0.058611361587015326 \n",
      "comm_round: 42 | global_acc: 96.576% | global_loss: 0.10316748172044754 | global_f1: 0.953032375740994 | global_precision: 0.9640221402214022 | global_recall: 0.9422903516681695 | global_auc: 0.9929686309200751| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 43 | global_acc: 96.609% | global_loss: 0.10079450905323029 | global_f1: 0.9534671532846716 | global_precision: 0.9649122807017544 | global_recall: 0.9422903516681695 | global_auc: 0.9931053836412406| flobal_FPR: 0.057709648331830475 \n",
      "comm_round: 44 | global_acc: 96.709% | global_loss: 0.09881928563117981 | global_f1: 0.9548974943052391 | global_precision: 0.9650092081031307 | global_recall: 0.9449954914337241 | global_auc: 0.9932848715877703| flobal_FPR: 0.05500450856627592 \n",
      "comm_round: 45 | global_acc: 96.809% | global_loss: 0.09655997157096863 | global_f1: 0.9562443026435733 | global_precision: 0.9668202764976959 | global_recall: 0.9458972046889089 | global_auc: 0.9934548628175525| flobal_FPR: 0.05410279531109107 \n",
      "comm_round: 46 | global_acc: 96.908% | global_loss: 0.09478992968797684 | global_f1: 0.957630979498861 | global_precision: 0.9677716390423573 | global_recall: 0.9477006311992786 | global_auc: 0.9935930400462301| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 47 | global_acc: 96.875% | global_loss: 0.09337672591209412 | global_f1: 0.9571948998178506 | global_precision: 0.9668813247470102 | global_recall: 0.9477006311992786 | global_auc: 0.9937264689165339| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 48 | global_acc: 96.941% | global_loss: 0.09161519259214401 | global_f1: 0.9580674567000912 | global_precision: 0.9686635944700461 | global_recall: 0.9477006311992786 | global_auc: 0.9939078562064131| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 49 | global_acc: 96.941% | global_loss: 0.09083661437034607 | global_f1: 0.9580674567000912 | global_precision: 0.9686635944700461 | global_recall: 0.9477006311992786 | global_auc: 0.9939349218491438| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 50 | global_acc: 96.975% | global_loss: 0.08968067169189453 | global_f1: 0.9585043319653442 | global_precision: 0.9695571955719557 | global_recall: 0.9477006311992786 | global_auc: 0.9940213419715469| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 51 | global_acc: 96.975% | global_loss: 0.0888468325138092 | global_f1: 0.9585043319653442 | global_precision: 0.9695571955719557 | global_recall: 0.9477006311992786 | global_auc: 0.99410776209395| flobal_FPR: 0.05229936880072137 \n",
      "comm_round: 52 | global_acc: 97.174% | global_loss: 0.08709371834993362 | global_f1: 0.9613460663938155 | global_precision: 0.9697247706422019 | global_recall: 0.9531109107303878 | global_auc: 0.9942245717099455| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 53 | global_acc: 97.207% | global_loss: 0.08694373071193695 | global_f1: 0.9617834394904459 | global_precision: 0.970615243342516 | global_recall: 0.9531109107303878 | global_auc: 0.994208902127312| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 54 | global_acc: 97.174% | global_loss: 0.08632978796958923 | global_f1: 0.9613460663938155 | global_precision: 0.9697247706422019 | global_recall: 0.9531109107303878 | global_auc: 0.9942525870243509| flobal_FPR: 0.046889089269612265 \n",
      "comm_round: 55 | global_acc: 97.207% | global_loss: 0.08474509418010712 | global_f1: 0.9618528610354224 | global_precision: 0.9688929551692589 | global_recall: 0.9549143372407575 | global_auc: 0.9943993112981016| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 56 | global_acc: 97.207% | global_loss: 0.08352770656347275 | global_f1: 0.9618528610354224 | global_precision: 0.9688929551692589 | global_recall: 0.9549143372407575 | global_auc: 0.9945645541695097| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 57 | global_acc: 97.207% | global_loss: 0.08282159268856049 | global_f1: 0.9618528610354224 | global_precision: 0.9688929551692589 | global_recall: 0.9549143372407575 | global_auc: 0.9946115629174104| flobal_FPR: 0.04508566275924256 \n",
      "comm_round: 58 | global_acc: 97.241% | global_loss: 0.08245376497507095 | global_f1: 0.9623241034952338 | global_precision: 0.9689213893967094 | global_recall: 0.9558160504959423 | global_auc: 0.9946495497844007| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 59 | global_acc: 97.241% | global_loss: 0.08178546279668808 | global_f1: 0.9623241034952338 | global_precision: 0.9689213893967094 | global_recall: 0.9558160504959423 | global_auc: 0.9947426176085272| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 60 | global_acc: 97.307% | global_loss: 0.0822935625910759 | global_f1: 0.9632319564230595 | global_precision: 0.9698354661791591 | global_recall: 0.9567177637511272 | global_auc: 0.9946576219936363| flobal_FPR: 0.04328223624887286 \n",
      "comm_round: 61 | global_acc: 97.274% | global_loss: 0.08072860538959503 | global_f1: 0.9627611262488647 | global_precision: 0.969807868252516 | global_recall: 0.9558160504959423 | global_auc: 0.9948703484487826| flobal_FPR: 0.04418394950405771 \n",
      "comm_round: 62 | global_acc: 97.440% | global_loss: 0.08011973649263382 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9949287532567803| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 63 | global_acc: 97.440% | global_loss: 0.07987011224031448 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9949325519434794| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 64 | global_acc: 97.440% | global_loss: 0.0800885409116745 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9949183068683579| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 65 | global_acc: 97.473% | global_loss: 0.07973381131887436 | global_f1: 0.9655485040797824 | global_precision: 0.9708295350957156 | global_recall: 0.9603246167718665 | global_auc: 0.9949667401237707| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 66 | global_acc: 97.473% | global_loss: 0.07933258265256882 | global_f1: 0.9655797101449276 | global_precision: 0.9699727024567789 | global_recall: 0.9612263300270514 | global_auc: 0.9950135114537527| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 67 | global_acc: 97.440% | global_loss: 0.07889044284820557 | global_f1: 0.9651110104213865 | global_precision: 0.9699453551912568 | global_recall: 0.9603246167718665 | global_auc: 0.9950474622161253| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 68 | global_acc: 97.540% | global_loss: 0.07819374650716782 | global_f1: 0.9664246823956443 | global_precision: 0.9726027397260274 | global_recall: 0.9603246167718665 | global_auc: 0.9951267598009677| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 69 | global_acc: 97.540% | global_loss: 0.0782003104686737 | global_f1: 0.9664246823956443 | global_precision: 0.9726027397260274 | global_recall: 0.9603246167718665 | global_auc: 0.9951467029061378| flobal_FPR: 0.03967538322813345 \n",
      "comm_round: 70 | global_acc: 97.573% | global_loss: 0.07852180302143097 | global_f1: 0.9668934240362811 | global_precision: 0.9726277372262774 | global_recall: 0.9612263300270514 | global_auc: 0.995092096784839| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 71 | global_acc: 97.573% | global_loss: 0.07805810123682022 | global_f1: 0.9668934240362811 | global_precision: 0.9726277372262774 | global_recall: 0.9612263300270514 | global_auc: 0.9951846897731282| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 72 | global_acc: 97.606% | global_loss: 0.07752103358507156 | global_f1: 0.9673321234119783 | global_precision: 0.9735159817351599 | global_recall: 0.9612263300270514 | global_auc: 0.995229324341842| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 73 | global_acc: 97.673% | global_loss: 0.07690009474754333 | global_f1: 0.9682107175295187 | global_precision: 0.9752973467520586 | global_recall: 0.9612263300270514 | global_auc: 0.9953228670018058| flobal_FPR: 0.0387736699729486 \n",
      "comm_round: 74 | global_acc: 97.640% | global_loss: 0.07669597119092941 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9953067225833349| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 75 | global_acc: 97.673% | global_loss: 0.07655727863311768 | global_f1: 0.968239564428312 | global_precision: 0.9744292237442922 | global_recall: 0.9621280432822362 | global_auc: 0.9953551558387477| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 76 | global_acc: 97.640% | global_loss: 0.07669387757778168 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9953622783763083| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 77 | global_acc: 97.739% | global_loss: 0.07622499763965607 | global_f1: 0.969118982742961 | global_precision: 0.9762122598353157 | global_recall: 0.9621280432822362 | global_auc: 0.9954159348259323| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 78 | global_acc: 97.640% | global_loss: 0.07662906497716904 | global_f1: 0.9678004535147392 | global_precision: 0.9735401459854015 | global_recall: 0.9621280432822362 | global_auc: 0.9953798473022913| flobal_FPR: 0.03787195671776375 \n",
      "comm_round: 79 | global_acc: 97.673% | global_loss: 0.07681167125701904 | global_f1: 0.9682683590208523 | global_precision: 0.9735642661804923 | global_recall: 0.9630297565374211 | global_auc: 0.9953879195115269| flobal_FPR: 0.0369702434625789 \n",
      "comm_round: 80 | global_acc: 97.872% | global_loss: 0.07588128745555878 | global_f1: 0.970988213961922 | global_precision: 0.9762989972652689 | global_recall: 0.9657348963029756 | global_auc: 0.9954781383206291| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 81 | global_acc: 97.806% | global_loss: 0.07610277831554413 | global_f1: 0.9701086956521738 | global_precision: 0.9745222929936306 | global_recall: 0.9657348963029756 | global_auc: 0.9954676919322067| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 82 | global_acc: 97.839% | global_loss: 0.07614993304014206 | global_f1: 0.970548255550521 | global_precision: 0.9754098360655737 | global_recall: 0.9657348963029756 | global_auc: 0.9954619939021582| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 83 | global_acc: 97.839% | global_loss: 0.07557842880487442 | global_f1: 0.9705749207786328 | global_precision: 0.9745454545454545 | global_recall: 0.9666366095581606 | global_auc: 0.995544140502025| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 84 | global_acc: 97.839% | global_loss: 0.0753733441233635 | global_f1: 0.970548255550521 | global_precision: 0.9754098360655737 | global_recall: 0.9657348963029756 | global_auc: 0.9955707313089184| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 85 | global_acc: 97.872% | global_loss: 0.07495750486850739 | global_f1: 0.9710144927536232 | global_precision: 0.9754322111010009 | global_recall: 0.9666366095581606 | global_auc: 0.9956049194892096| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 86 | global_acc: 97.839% | global_loss: 0.07460326701402664 | global_f1: 0.9704947798456649 | global_precision: 0.9771480804387569 | global_recall: 0.9639314697926059 | global_auc: 0.9956201142360058| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 87 | global_acc: 97.839% | global_loss: 0.0747843012213707 | global_f1: 0.9704947798456649 | global_precision: 0.9771480804387569 | global_recall: 0.9639314697926059 | global_auc: 0.9956001711308358| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 88 | global_acc: 97.906% | global_loss: 0.0747041180729866 | global_f1: 0.9714803078315981 | global_precision: 0.9754545454545455 | global_recall: 0.9675383228133454 | global_auc: 0.9956077685042339| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 89 | global_acc: 97.839% | global_loss: 0.07452578097581863 | global_f1: 0.9705215419501133 | global_precision: 0.9762773722627737 | global_recall: 0.9648331830477908 | global_auc: 0.9956044446533723| flobal_FPR: 0.0351668169522092 \n",
      "comm_round: 90 | global_acc: 97.806% | global_loss: 0.07388810813426971 | global_f1: 0.9700544464609802 | global_precision: 0.9762557077625571 | global_recall: 0.9639314697926059 | global_auc: 0.9956685474914185| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 91 | global_acc: 97.872% | global_loss: 0.07376183569431305 | global_f1: 0.9709355131698456 | global_precision: 0.9780420860018298 | global_recall: 0.9639314697926059 | global_auc: 0.9956699719989307| flobal_FPR: 0.03606853020739405 \n",
      "comm_round: 92 | global_acc: 97.939% | global_loss: 0.07371701300144196 | global_f1: 0.9718693284936479 | global_precision: 0.9780821917808219 | global_recall: 0.9657348963029756 | global_auc: 0.995676619700654| flobal_FPR: 0.034265103697024346 \n",
      "comm_round: 93 | global_acc: 97.906% | global_loss: 0.07314444333314896 | global_f1: 0.9714803078315981 | global_precision: 0.9754545454545455 | global_recall: 0.9675383228133454 | global_auc: 0.9957231536127172| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 94 | global_acc: 97.906% | global_loss: 0.07292204350233078 | global_f1: 0.9714803078315981 | global_precision: 0.9754545454545455 | global_recall: 0.9675383228133454 | global_auc: 0.9957563921213338| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 95 | global_acc: 97.939% | global_loss: 0.0732637569308281 | global_f1: 0.9718948322756119 | global_precision: 0.9772105742935278 | global_recall: 0.9666366095581606 | global_auc: 0.9957074840300837| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 96 | global_acc: 97.872% | global_loss: 0.07392386347055435 | global_f1: 0.9710144927536232 | global_precision: 0.9754322111010009 | global_recall: 0.9666366095581606 | global_auc: 0.995656676595484| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 97 | global_acc: 97.972% | global_loss: 0.07312419265508652 | global_f1: 0.9723356009070295 | global_precision: 0.9781021897810219 | global_recall: 0.9666366095581606 | global_auc: 0.9957749107189916| flobal_FPR: 0.033363390441839495 \n",
      "comm_round: 98 | global_acc: 98.039% | global_loss: 0.07249411940574646 | global_f1: 0.9732426303854875 | global_precision: 0.9790145985401459 | global_recall: 0.9675383228133454 | global_auc: 0.9958456612587613| flobal_FPR: 0.032461677186654644 \n",
      "comm_round: 99 | global_acc: 98.072% | global_loss: 0.07280635833740234 | global_f1: 0.9737080689029918 | global_precision: 0.9790337283500455 | global_recall: 0.9684400360685302 | global_auc: 0.9958219194668922| flobal_FPR: 0.031559963931469794 \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "for d in range(0,1):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedProx 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx-FGSM 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                                  batch_size=32, shuffle=True)\n",
    "    \n",
    "    \n",
    "                    # List of training approaches\n",
    "                    training_approaches = [train_model_prox, pgd_attack]\n",
    "    \n",
    "                    # Randomly choose between normal training and PGD attack\n",
    "                    selected_training_approach = random.choice(training_approaches)\n",
    "    \n",
    "                    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model_prox:\n",
    "                        train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.01\n",
    "                        alpha = 0.01\n",
    "                        num_iter = 5\n",
    "                        X_adv =pgd_attack(local_model,torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32), clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter, loss, optimizer)\n",
    "                        train_model_prox(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-FGSM-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-FGSM-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-FGSM-all-std-results.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bbb1f186",
   "metadata": {},
   "source": [
    "## non-iid FedProx PGD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153b54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def pgd_attack(model, X, y, epsilon, alpha, num_iter, loss_fn, optimizer):\n",
    "    model.eval()\n",
    "    \n",
    "    # Create a copy of the input for adversarial perturbation\n",
    "    X_adv = X.detach().clone()\n",
    "    X_adv.requires_grad = True\n",
    "\n",
    "    for _ in range(num_iter):\n",
    "        outputs = model(X_adv)\n",
    "        loss = loss_fn(outputs, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbation (gradient ascent on the loss)\n",
    "        with torch.no_grad():\n",
    "            grad_sign = X_adv.grad.data.sign()\n",
    "            X_adv = X_adv + alpha * grad_sign\n",
    "            # Clip perturbation to be within epsilon\n",
    "            X_adv = torch.max(torch.min(X_adv, X + epsilon), X - epsilon)\n",
    "            X_adv = torch.clamp(X_adv, 0, 1)\n",
    "\n",
    "        X_adv.requires_grad = True  # Enable gradient tracking for the next iteration\n",
    "\n",
    "    # Create a DataLoader with the adversarial examples\n",
    "    adv_dataset = TensorDataset(X_adv, y)\n",
    "    adv_loader = DataLoader(adv_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "    return adv_loader\n",
    "\n",
    "def train_model_prox(model, train_loader, loss_fn, optimizer, mu=0.01):\n",
    "    model.train()\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        \n",
    "        # Add the proximal term\n",
    "        for param, param_global in zip(model.parameters(), global_model.parameters()):\n",
    "            loss += (mu / 2) * torch.norm(param - param_global, p=2)**2\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "all_avg = []\n",
    "all_std = []\n",
    "\n",
    "n_clients = [10]\n",
    "n_round = [100]\n",
    "\n",
    "dataset = ['Drebin', 'Malgenome', 'Kronodroid', 'Tuandromd']\n",
    "\n",
    "# for d in range(0,1):\n",
    "for d in range(0, 2):\n",
    "    if d == 0:\n",
    "        use_data = Drebin_data\n",
    "    elif d == 1:\n",
    "        use_data = Malgenome_data\n",
    "    elif d == 2:\n",
    "        use_data = Kronodroid_data\n",
    "    elif d == 3:\n",
    "        use_data = Tuandromd_data\n",
    "\n",
    "    print('===================================================================================================')\n",
    "    print('Working with:', dataset[d])\n",
    "    print('===================================================================================================')\n",
    "\n",
    "    for r in n_round:  # number of rounds loop\n",
    "        comms_round = r\n",
    "        for cl in n_clients:  # number of clients loop\n",
    "            number_of_clients = cl\n",
    "\n",
    "            print('---------------------------------------------')\n",
    "            print('No. of Clients:', number_of_clients)\n",
    "            print('No. of Rounds:', comms_round)\n",
    "            print('---------------------------------------------')\n",
    "\n",
    "            features = np.array(use_data.iloc[:, range(0, use_data.shape[1] - 1)])  # feature set\n",
    "            labels = use_data.iloc[:, -1]  # labels --> B : Benign and S\n",
    "\n",
    "            # Do feature scaling\n",
    "            X = preprocessing.StandardScaler().fit(features).transform(features)\n",
    "\n",
    "            # binarize the labels\n",
    "            lb = LabelBinarizer()\n",
    "            y = lb.fit_transform(labels)\n",
    "\n",
    "            # split data into training and test set\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                                y, shuffle=True,\n",
    "                                                                test_size=0.2,\n",
    "                                                                random_state=100)\n",
    "\n",
    "            # create clients -- Horizontal FL\n",
    "            clients = create_non_iid_clients(X_train, y_train, num_clients=number_of_clients, initial='client')\n",
    "\n",
    "            # process and batch the training data for each client\n",
    "            clients_batched = dict()\n",
    "            for (client_name, data) in clients.items():\n",
    "                clients_batched[client_name] = batch_data(data)\n",
    "\n",
    "            # process and batch the test set\n",
    "            test_batched = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                                                     torch.tensor(y_test, dtype=torch.float32)),\n",
    "                                                       batch_size=len(y_test), shuffle=False)\n",
    "\n",
    "            # ==============================================\n",
    "            # Traditional FedProx 2017\n",
    "            # ==============================================\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            all_results = list()\n",
    "\n",
    "            # create optimizer\n",
    "            lr = 0.01\n",
    "            loss = nn.BCELoss()\n",
    "            optimizer = optim.SGD\n",
    "\n",
    "            # initialize global model\n",
    "            smlp_global = SimpleMLP(X.shape[1], 1)\n",
    "            global_model = smlp_global\n",
    "            # -----------------------------------------------\n",
    "\n",
    "            print('|=======================|')\n",
    "            print('|Traditional FedProx non iid pgd 2017|')\n",
    "            print('|=======================|')\n",
    "\n",
    "            # commence global training loop\n",
    "            for comm_round in range(comms_round):\n",
    "                # get the global model's weights - will serve as the initial weights for all local models\n",
    "                global_weights = [param.data.clone() for param in global_model.parameters()]\n",
    " \n",
    "                # initial list to collect local model weights after scaling\n",
    "                scaled_local_weight_list = list()\n",
    "\n",
    "                # randomize client data - using keys\n",
    "                client_names = list(clients_batched.keys())\n",
    "                random.shuffle(client_names)\n",
    "\n",
    "                for client in client_names:\n",
    "                    smlp_local = SimpleMLP(X.shape[1], 1)\n",
    "                    local_model = smlp_local\n",
    "                    # set local model weight to the weight of the global model\n",
    "                    local_model.load_state_dict({name: param.clone() for name, param in zip(local_model.state_dict(), global_weights)})\n",
    "                    optimizer = torch.optim.SGD(local_model.parameters(), lr=0.01)\n",
    "\n",
    "                    # fit local model with client's data\n",
    "                    train_loader = DataLoader(TensorDataset(torch.tensor(clients_batched[client].dataset.tensors[0], dtype=torch.float32),\n",
    "                                                            torch.tensor(clients_batched[client].dataset.tensors[1], dtype=torch.float32)),\n",
    "                                              batch_size=32, shuffle=True)\n",
    "                    selected_training_approach = random.choice([train_model_prox, pgd_attack])\n",
    "\n",
    "                    # List of training approaches    # Apply the selected training approach\n",
    "                    if selected_training_approach == train_model_prox:\n",
    "                        train_model_prox(local_model, train_loader, loss, optimizer)\n",
    "                    elif selected_training_approach == pgd_attack:\n",
    "                        epsilon = 0.1  # Set your desired value for epsilon\n",
    "                        alpha = 0.01   # Set your desired value for alpha\n",
    "                        num_iter = 10   # Set your desired number of iterations\n",
    "                        X_adv = pgd_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon, alpha, num_iter)\n",
    "                        train_model_prox(local_model, DataLoader(TensorDataset(X_adv, clients_batched[client].dataset.tensors[1])),\n",
    "                                    loss, optimizer)\n",
    "                    # train_model(local_model, train_loader, loss, optimizer)\n",
    "                    # # FGSM attack\n",
    "                    # epsilon = 1\n",
    "                    # X_adv = fgsm_attack(local_model, clients_batched[client].dataset.tensors[0], clients_batched[client].dataset.tensors[1], epsilon,loss, optimizer)                  \n",
    "                    # train_model(local_model, X_adv, loss, optimizer)\n",
    "\n",
    "                    # scale the model weights and add to the list\n",
    "                    scaling_factor = weight_scalling_factor(clients_batched, client)\n",
    "                    scaled_weights = scale_model_weights(local_model.state_dict().values(), scaling_factor)\n",
    "                    scaled_local_weight_list.append(scaled_weights)\n",
    "\n",
    "                    # clear session to free memory after each communication round\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                # to get the average over all the local model, we simply take the sum of the scaled weights\n",
    "                average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
    "\n",
    "                # update global model\n",
    "                for param, avg_param in zip(global_model.parameters(), average_weights):\n",
    "                    param.data.copy_(avg_param)\n",
    "\n",
    "                # test global model and print out metrics after each communications round\n",
    "                for X_test_batch, Y_test_batch in test_batched:\n",
    "                    global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr = test_model(X_test_batch, Y_test_batch, global_model, comm_round)\n",
    "                    all_results.append([global_acc, global_loss, global_f1, global_precision, global_recall, global_auc, global_fpr])\n",
    "\n",
    "            all_R = pd.DataFrame(all_results, columns=['global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "            flname = f'results/round-{r}/{cl}-clients/FedProx-non-iid-pgd-{dataset[d]}-results.csv'\n",
    "            all_R.to_csv(flname, index=None)\n",
    "\n",
    "            all_avg.append(np.concatenate(([dataset[d], r, cl], np.mean(all_results, axis=0))))  # Storing avg values for each dataset\n",
    "            all_std.append(np.concatenate(([dataset[d], r, cl], np.std(all_results, axis=0))))  # Storing std values for each dataset\n",
    "\n",
    "ALL_AVG = pd.DataFrame(all_avg, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_AVG.to_csv('FedProx-non-iid-pgd-results.csv')\n",
    "\n",
    "ALL_STD = pd.DataFrame(all_std, columns=['Dataset', 'num of round', 'num of clients', 'global_acc', 'global_loss', 'global_f1', 'global_precision', 'global_recall', 'global_auc', 'global_fpr'])\n",
    "ALL_STD.to_csv('FedProx-non-iid-pgd-all-std-results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
